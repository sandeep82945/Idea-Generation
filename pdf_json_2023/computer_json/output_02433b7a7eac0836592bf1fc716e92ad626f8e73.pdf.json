{
    "abstractText": "Video stabilization refers to the problem of transforming a shaky video into a visually pleasing one. The question of how to strike a good trade-off between visual quality and computational speed has remained one of the open challenges in video stabilization. Inspired by the analogy between wobbly frames and jigsaw puzzles, we propose an iterative optimization-based learning approach using synthetic datasets for video stabilization, which consists of two interacting submodules: motion trajectory smoothing and full-frame outpainting. First, we develop a two-level (coarse-to-fine) stabilizing algorithm based on the probabilistic flow field. The confidence map associated with the estimated optical flow is exploited to guide the search for shared regions through backpropagation. Second, we take a divide-and-conquer approach and propose a novel multiframe fusion strategy to render full-frame stabilized views. An important new insight brought about by our iterative optimization approach is that the target video can be interpreted as the fixed point of nonlinear mapping for video stabilization. We formulate video stabilization as a problem of minimizing the amount of jerkiness in motion trajectories, which guarantees convergence with the help of fixedpoint theory. Extensive experimental results are reported to demonstrate the superiority of the proposed approach in terms of computational speed and visual quality. The code will be available on GitHub.",
    "authors": [
        {
            "affiliations": [],
            "name": "Weiyue Zhao"
        },
        {
            "affiliations": [],
            "name": "Xin Li"
        },
        {
            "affiliations": [],
            "name": "Zhan Peng"
        },
        {
            "affiliations": [],
            "name": "Xianrui Luo"
        },
        {
            "affiliations": [],
            "name": "Xinyi Ye"
        },
        {
            "affiliations": [],
            "name": "Hao Lu"
        },
        {
            "affiliations": [],
            "name": "Zhiguo Cao"
        }
    ],
    "id": "SP:65944c05397c97f79cba049c357cb56abb6b3ccd",
    "references": [
        {
            "authors": [
                "Ravi P Agarwal",
                "Maria Meehan",
                "Donal O\u2019regan"
            ],
            "title": "Fixed point theory and applications, volume 141",
            "venue": "Cambridge university press,",
            "year": 2001
        },
        {
            "authors": [
                "Relja Arandjelovi\u0107",
                "Andrew Zisserman"
            ],
            "title": "Three things everyone should know to improve object retrieval",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2012
        },
        {
            "authors": [
                "Adam Baumberg"
            ],
            "title": "Reliable feature matching across widely separated views",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2000
        },
        {
            "authors": [
                "Ronald Newbold Bracewell",
                "Ronald N Bracewell"
            ],
            "title": "The Fourier transform and its applications, volume 31999",
            "venue": "McGraw-hill New York,",
            "year": 1986
        },
        {
            "authors": [
                "Stanley H Chan",
                "Xiran Wang",
                "Omar A Elgendy"
            ],
            "title": "Plugand-play admm for image restoration: Fixed-point convergence and applications",
            "venue": "IEEE Transactions on Computational Imaging,",
            "year": 2016
        },
        {
            "authors": [
                "Jinsoo Choi",
                "In So Kweon"
            ],
            "title": "Deep iterative frame interpolation for full-frame video stabilization",
            "venue": "ACM Trans. Graph.,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick L Combettes",
                "Jean-Christophe Pesquet"
            ],
            "title": "Fixed point strategies in data science",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2021
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2016
        },
        {
            "authors": [
                "Martin A Fischler",
                "Robert C Bolles"
            ],
            "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Commun. ACM,",
            "year": 1981
        },
        {
            "authors": [
                "Amit Goldstein",
                "Raanan Fattal"
            ],
            "title": "Video stabilization using epipolar geometry",
            "venue": "ACM Trans. Graph.,",
            "year": 2012
        },
        {
            "authors": [
                "Matthias Grundmann",
                "Vivek Kwatra",
                "Irfan Essa"
            ],
            "title": "Autodirected video stabilization with robust l1 optimal camera paths",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2011
        },
        {
            "authors": [
                "Wilko Guilluy",
                "Laurent Oudre",
                "Azeddine Beghdadi"
            ],
            "title": "Video stabilization: Overview, challenges and perspectives",
            "venue": "Signal Processing: Image Communication,",
            "year": 2021
        },
        {
            "authors": [
                "Tae Hyun Kim",
                "Kyoung Mu Lee"
            ],
            "title": "Generalized video deblurring for dynamic scenes",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2015
        },
        {
            "authors": [
                "Andrey Ignatov",
                "Nikolay Kobyshev",
                "Radu Timofte",
                "Kenneth Vanhoey",
                "Luc Van Gool"
            ],
            "title": "Dslr-quality photos on mobile devices with deep convolutional networks",
            "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Proc. Int. Conf. Learn. Repr.,",
            "year": 2014
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Proc. Eur. Conf. Comput. Vis.,",
            "year": 2014
        },
        {
            "authors": [
                "Qiang Ling",
                "Minda Zhao"
            ],
            "title": "Stabilization of traffic videos based on both foreground and background feature trajectories",
            "venue": "IEEE Trans. Circuits Syst. Video Technol.,",
            "year": 2018
        },
        {
            "authors": [
                "Feng Liu",
                "Michael Gleicher",
                "Hailin Jin",
                "Aseem Agarwala"
            ],
            "title": "Content-preserving warps for 3d video stabilization",
            "venue": "In ACM SIGGRAPH 2009 Papers, SIGGRAPH \u201909,",
            "year": 2009
        },
        {
            "authors": [
                "Feng Liu",
                "Michael Gleicher",
                "Jue Wang",
                "Hailin Jin",
                "Aseem Agarwala"
            ],
            "title": "Subspace video stabilization",
            "venue": "ACM Trans. Graph.,",
            "year": 2011
        },
        {
            "authors": [
                "Feng Liu",
                "Michael Gleicher",
                "Jue Wang",
                "Hailin Jin",
                "Aseem Agarwala"
            ],
            "title": "Subspace video stabilization",
            "venue": "ACM Trans. Graph.,",
            "year": 2011
        },
        {
            "authors": [
                "Shuaicheng Liu",
                "Yinting Wang",
                "Lu Yuan",
                "Jiajun Bu",
                "Ping Tan",
                "Jian Sun"
            ],
            "title": "Video stabilization with a depth camera",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2012
        },
        {
            "authors": [
                "Shuaicheng Liu",
                "Lu Yuan",
                "Ping Tan",
                "Jian Sun"
            ],
            "title": "Bundled camera paths for video stabilization",
            "venue": "ACM Trans. Graph.,",
            "year": 2013
        },
        {
            "authors": [
                "Shuaicheng Liu",
                "Lu Yuan",
                "Ping Tan",
                "Jian Sun"
            ],
            "title": "Steadyflow: Spatially smooth optical flow for video stabilization",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2014
        },
        {
            "authors": [
                "Yu-Lun Liu",
                "Wei-Sheng Lai",
                "Ming-Hsuan Yang",
                "Yung-Yu Chuang",
                "Jia-Bin Huang"
            ],
            "title": "Hybrid neural fusion for fullframe video stabilization",
            "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
            "year": 2021
        },
        {
            "authors": [
                "David G Lowe"
            ],
            "title": "Distinctive image features from scaleinvariant keypoints",
            "venue": "Int. J. Comput. Vis.,",
            "year": 2004
        },
        {
            "authors": [
                "Xiaojiao Mao",
                "Chunhua Shen",
                "Yu-Bin Yang"
            ],
            "title": "Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections",
            "venue": "Proc. Adv. Neural Inf. Process. Syst.,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Matsushita",
                "E. Ofek",
                "Weina Ge",
                "Xiaoou Tang",
                "Heung- Yeung Shum"
            ],
            "title": "Full-frame video stabilization with motion inpainting",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2006
        },
        {
            "authors": [
                "Carlos Morimoto",
                "Rama Chellappa"
            ],
            "title": "Evaluation of image stabilization algorithms",
            "venue": "In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP\u201998 (Cat. No. 98CH36181),",
            "year": 1998
        },
        {
            "authors": [
                "Th\u00e9odore Papadopoulo",
                "Manolis IA Lourakis"
            ],
            "title": "Estimating the jacobian of the singular value decomposition: Theory and applications",
            "venue": "In Proc. Eur. Conf. Comput. Vis.,",
            "year": 2000
        },
        {
            "authors": [
                "Giovanni Puglisi",
                "Sebastiano Battiato"
            ],
            "title": "A robust image alignment algorithm for video stabilization purposes",
            "venue": "IEEE Trans. Circuits Syst. Video Technol.,",
            "year": 2011
        },
        {
            "authors": [
                "Marcos e Roberto",
                "Helena de Almeida Maia",
                "Helio Pedrini"
            ],
            "title": "Survey on digital video stabilization: Concepts, methods, and challenges",
            "venue": "ACM Comput. Surv.,",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Rublee",
                "Vincent Rabaud",
                "Kurt Konolige",
                "Gary Bradski"
            ],
            "title": "Orb: An efficient alternative to sift or surf",
            "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
            "year": 2011
        },
        {
            "authors": [
                "Mark Sabini",
                "Gili Rusak"
            ],
            "title": "Painting outside the box: Image outpainting with gans",
            "venue": "Comput. Res. Repository,",
            "year": 2018
        },
        {
            "authors": [
                "Zhenmei Shi",
                "Fuhao Shi",
                "Wei-Sheng Lai",
                "Chia-Kai Liang",
                "Yingyu Liang"
            ],
            "title": "Deep online fused video stabilization",
            "venue": "In Proc. Winter Conf. Appl. Comput. Vis.,",
            "year": 2022
        },
        {
            "authors": [
                "Brandon M. Smith",
                "Li Zhang",
                "Hailin Jin",
                "Aseem Agarwala"
            ],
            "title": "Light field video stabilization",
            "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
            "year": 2009
        },
        {
            "authors": [
                "Shuochen Su",
                "Mauricio Delbracio",
                "Jue Wang",
                "Guillermo Sapiro",
                "Wolfgang Heidrich",
                "Oliver Wang"
            ],
            "title": "Deep video deblurring for hand-held cameras",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2017
        },
        {
            "authors": [
                "Roman Suvorov",
                "Elizaveta Logacheva",
                "Anton Mashikhin",
                "Anastasia Remizova",
                "Arsenii Ashukha",
                "Aleksei Silvestrov",
                "Naejin Kong",
                "Harshith Goka",
                "Kiwoong Park",
                "Victor Lempitsky"
            ],
            "title": "Resolution-robust large mask inpainting with fourier convolutions",
            "venue": "In Proc. Winter Conf. Appl. Comput. Vis.,",
            "year": 2022
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Pdc-net+: Enhanced probabilistic dense correspondence network",
            "venue": "arXiv preprint arXiv:2109.13912,",
            "year": 2021
        },
        {
            "authors": [
                "Prune Truong",
                "Martin Danelljan",
                "Fisher Yu",
                "Luc Van Gool"
            ],
            "title": "Probabilistic warp consistency for weaklysupervised semantic correspondences",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2022
        },
        {
            "authors": [
                "Miao Wang",
                "Guo-Ye Yang",
                "Jin-Kun Lin",
                "Song-Hai Zhang",
                "Ariel Shamir",
                "Shao-Ping Lu",
                "Shi-Min Hu"
            ],
            "title": "Deep online video stabilization with multi-grid warping transformation learning",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2019
        },
        {
            "authors": [
                "Yu-Shuen Wang",
                "Feng Liu",
                "Pu-Sheng Hsu",
                "Tong-Yee Lee"
            ],
            "title": "Spatially and temporally optimized video stabilization",
            "venue": "IEEE Trans. Vis. Comput. Graphics,",
            "year": 2013
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2004
        },
        {
            "authors": [
                "Jonas Wulff",
                "Michael J. Black"
            ],
            "title": "Efficient sparse-to-dense optical flow estimation using a learned basis and layers",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2015
        },
        {
            "authors": [
                "Sen-Zhe Xu",
                "Jun Hu",
                "Miao Wang",
                "Tai-Jiang Mu",
                "Shi- Min Hu"
            ],
            "title": "Deep video stabilization using adversarial networks",
            "venue": "Computer Graphics Forum,",
            "year": 2018
        },
        {
            "authors": [
                "Yufei Xu",
                "Jing Zhang",
                "Stephen J Maybank",
                "Dacheng Tao"
            ],
            "title": "Dut: learning video stabilization by simply watching unstable videos",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2022
        },
        {
            "authors": [
                "Yufei Xu",
                "Jing Zhang",
                "Dacheng Tao"
            ],
            "title": "Out-of-boundary view synthesis towards full-frame video stabilization",
            "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
            "year": 2021
        },
        {
            "authors": [
                "Junlan Yang",
                "Dan Schonfeld",
                "Magdi Mohamed"
            ],
            "title": "Robust video stabilization based on particle filter tracking of projected camera motion",
            "venue": "IEEE Trans. Circuits Syst. Video Technol.,",
            "year": 2009
        },
        {
            "authors": [
                "Xinyi Ye",
                "Weiyue Zhao",
                "Hao Lu",
                "Zhiguo Cao"
            ],
            "title": "Learning second-order attentive context for efficient correspondence pruning",
            "venue": "In Proc. AAAI Conf. Artificial Intell.,",
            "year": 2023
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zhe Lin",
                "Jimei Yang",
                "Xiaohui Shen",
                "Xin Lu",
                "Thomas S Huang"
            ],
            "title": "Free-form image inpainting with gated convolution",
            "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
            "year": 2019
        },
        {
            "authors": [
                "Jiyang Yu",
                "Ravi Ramamoorthi"
            ],
            "title": "Selfie video stabilization",
            "venue": "In Proc. Eur. Conf. Comput. Vis.,",
            "year": 2018
        },
        {
            "authors": [
                "Jiyang Yu",
                "Ravi Ramamoorthi"
            ],
            "title": "Robust video stabilization by optimization in cnn weight space",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2019
        },
        {
            "authors": [
                "Jiyang Yu",
                "Ravi Ramamoorthi"
            ],
            "title": "Learning video stabilization using optical flow",
            "venue": "In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2020
        },
        {
            "authors": [
                "Jiahui Zhang",
                "Dawei Sun",
                "Zixin Luo",
                "Anbang Yao",
                "Lei Zhou",
                "Tianwei Shen",
                "Yurong Chen",
                "Long Quan",
                "Hongen Liao"
            ],
            "title": "Learning two-view correspondences and geometry using order-aware network",
            "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
            "year": 2019
        },
        {
            "authors": [
                "Lei Zhang",
                "Qian-Kun Xu",
                "Hua Huang"
            ],
            "title": "A global approach to fast video stabilization",
            "venue": "IEEE Trans. Circuits Syst. Video Technol.,",
            "year": 2015
        },
        {
            "authors": [
                "Weiyue Zhao",
                "Hao Lu",
                "Zhiguo Cao",
                "Xin Li"
            ],
            "title": "A2b: Anchor to barycentric coordinate for robust correspondence",
            "venue": "Int. J. Comput. Vis.,",
            "year": 2023
        },
        {
            "authors": [
                "Weiyue Zhao",
                "Hao Lu",
                "Xinyi Ye",
                "Zhiguo Cao",
                "Xin Li"
            ],
            "title": "Learning probabilistic coordinate fields for robust correspondences",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2023
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Tete Xiao",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Semantic understanding of scenes through the ade20k dataset",
            "venue": "Int. J. Comput. Vis.,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "With the growing popularity of short videos on social media platforms (e.g., TikTok, Instagram), video has played an increasingly important role in our daily life. However, casually captured videos are often shaky and wobbly due to amateur shooting. Although it is possible to alleviate those problems by resorting to professional equipment (e.g., dollies and steadicams), the cost of hardware-based solutions\n*Corresponding author\nis often expensive, making it impractical for real-world applications. By contrast, software-based or computational solutions such as video stabilization algorithms [12] have become an attractive alternative to improve the visual quality of shaky video by eliminating undesirable jitter.\nExisting video stabilization techniques can be classified into two categories: optimization-based and learning-based. Traditional optimization-based algorithms [11,19,22,28,40] have been widely studied due to their speed and robustness. The challenges of them are the occlusion caused by changes in depth of field and the interference caused by foreground objects on camera pose regression. Furthermore, their results often contain large missing regions at frame borders, particularly when videos with a large camera motion. In recent years, learning-based video stabilization algorithms [6, 24, 46, 52] have shown their superiority by achieving higher visual quality compared to traditional methods. However, their stabilization model is too complex for rapid computation, and its generalization property is unknown due to the scarcity of training datasets.\nTo overcome those limitations, we present an iterative optimization-based learning approach that is efficient and robust, capable of achieving high-quality stabilization results with full-frame rendering, as shown in Fig. 1. The probabilistic stabilized network addresses the issues of occlusion and interfering objects, and achieves fast pose estimation. Then the full-frame outpainting module retains the original field of view (FoV) without aggressive cropping. An important new insight brought by our approach is that the objective of video stabilization is to suppress the implicitly embedded noise in the video frames rather than the explicit noise in the pixel intensity values. This inspired us to adopt an expectation-maximization (EM)-like approach for video stabilization. Importantly, considering the strong redundancy of video in the temporal domain, we ingeniously consider stable video (the target of video stabilization) as the fixed point of nonlinear mapping. Such a fixed-point perspective allows us to formulate an optimization problem of the optical flow field in commonly shared regions. Unlike ar X iv :2\n30 7.\n12 77\n4v 2\n[ cs\n.C V\n] 3\n1 Ju\nl 2 02\nmost methods that resort to the ad hoc video dataset [58] or the deblurred dataset [36] as stabilized videos, we propose to construct a synthetic training dataset to facilitate joint optimization of model parameters in different network modules.\nTo solve the formulated iterative optimization problem, we take a divide-and-conquer approach by designing two modules: probabilistic stabilization network (for motion trajectory smoothing) and video outpainting network (for full-frame video rendering). For the former, we propose to build on the previous work of PDCNet [38, 39] and extend it using a coarse-to-fine strategy to improve robustness. For a more robust estimate of the uncertainty of the optical flow, we infer masks from the optical flow by bidirectional propagation with a low computational cost (around 1/5 of the time Yu et al. [52]). Accordingly, we have developed a two-level (coarse-to-fine) flow smoothing strategy that first aligns adjacent frames by global affine transformation and then refines the result by warping the fields of intermediate frames. For the latter, we propose a two-stage approach (flow and image outpainting) to render full-frame video. Our experimental results have shown highly competent performance against others on three public benchmark datasets. The main contribution of this work is threefold:\n\u2022 We propose a formulation of video stabilization as a fixed-point problem of the optical flow field and propose a novel procedure to generate a model-based synthetic dataset.\n\u2022 We construct a probabilistic stabilization network based on PDCNet and propose an effective coarse-tofine strategy for robust and efficient smoothing of optical flow fields.\n\u2022 We propose a novel video outpainting network to render stabilized full-frame video by exploiting the spatial coherence in the flow field."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Video Stabilization",
            "text": "Most video stabilization methods can typically be summarized as a three-step procedure: motion estimation, smoothing the trajectory, and generating stable frames. Traditional methods focus primarily on 2D features or image alignment [30] when it comes to motion estimation. These methods are different in modeling approaches to motion, including the trajectory matrix [20], epipolar geometry [10, 22, 48, 55], and the optical flow field [23, 56]. Regarding the smoothing trajectory, particle filter tracking [47], space-time path smoothing [20, 41], and L1 optimization [11] have been proposed. Existing methods for generating stable frames rely mainly on 2D transformations [27], grid warping [20, 22], and dense flow field warping [23].\nCompared to the 2D method, some approaches turn to 3D reconstruction [18]. However, specialized hardware such as depth camera [21] and light field camera [35] are necessary for these methods based on 3D reconstruction. Some methods [31, 44, 45, 51] tackle video stabilization from the perspective of deep learning. In [51], the optimization of video stabilization was formulated in the CNN weight space. Recent work [52] represents motion by flow field and attempts to learn a stable optical flow to warp frames. Another method [6] aims to learn stable frames by interpolation. These deep-learning methods generate stable videos with less distortion."
        },
        {
            "heading": "2.2. Large FOV Video",
            "text": "Unlike the early work (e.g., traffic video stabilization [17]), large field of view (Field Of View) video stabilization has been attracting more researchers\u2019 attention. For most video stabilization methods, cropping is inevitable, which is why the FOV is reduced. Several approaches have been proposed to maintain a high FOV ratio. OVS [46] proposed to improve FOV by extrapolation. DIFRINT [6] choose iterative interpolation to generate high-FOV frames. FuSta [24] used neural rendering to synthesize high-FOV\nframes from feature space. To a great extent, the performance of interpolation-based video stabilization [6, 46] depends on the selected frames. If selected frames have little correspondence with each other, performance will deteriorate disastrously. Neural rendering [24] synthesizes the image by weighted summing, causing blur. Most recently, a deep neural network [34] (DNN) has jointly exploited sensor data and optical flow to stabilize videos."
        },
        {
            "heading": "3. Stabilization via Iterative Optimization",
            "text": "Motivation. Despite rapid advances in video stabilization [31], existing methods still suffer from several notable limitations [12]. First, a systematic treatment of various uncertainty factors (e.g., low-texture regions in the background and moving objects in the foreground) in the problem formulation is still lacking. These uncertainty factors often cause occlusion-related problems and interfere with the motion estimation process. Second, the high computational cost has remained a technical barrier to real-time video processing. The motivation behind our approach is two-fold. On the one hand, we advocate for finding commonly shared regions among successive frames to address various uncertainty factors in handheld video. On the other hand, in contrast to these prestabilization algorithms [22, 24, 51, 52] based on traditional approaches, we proposed a novel highefficiency prestabilization algorithm based on probabilistic optical flow. Flow-based methods are generally more accurate in motion estimation and deserve a high time cost. Accuracy and efficiency, we have both. Approach. We propose to formulate video stabilization as a problem of minimizing the amount of jerkiness in motion trajectories. It is enlightening to think of video stabilization as a special kind of \u201cvideo denoising\u201d where noise contamination is not associated with pixel intensity values, but embedded into the motion trajectories of foreground and background objects. Conceptually, for video restoration in which unknown motion is the hidden variable, we can treat video stabilization as a chicken-and-egg problem [13] - i.e., the objective of smoothing motion trajectories is intertwined with that of video frame interpolation. Note that an improved estimation of motion information can facilitate the task of frame interpolation and vice versa. Such an observation naturally inspires us to tackle the problem of video stabilization by iterative optimization.\nThrough divide-and-conquer, we propose to formulate video stabilization as the following optimization problem. Given a sequence of n frames along with the set of optical flows Y , we first define the confidence map, which originally indicates the reliability and accuracy of the optical flow prediction at each pixel. Here, we have thresholded the confidence map as a binary image, which represents accurate matches (as shown in the 4-th column of Fig. 2). Then we can denote n frames and the corresponding set of confi-\ndence maps M by:\nY = {Y1, Y2, \u00b7 \u00b7 \u00b7 , Yq} ,M = {M1,M2, \u00b7 \u00b7 \u00b7 ,Mq} . (1)"
        },
        {
            "heading": "4. Probabilistic Stabilization Network",
            "text": "The problem of video stabilization can be formulated as finding a nonlinear mapping f : Y \u2192 Y\u0302 where Y\u0302 denotes the optical flow set of the stabilized video. We hypothesize that a desirable objective to pursue f is the well-known fixed-point property, i.e., Y\u0302 = f(Y\u0302). To achieve this objective, we aim to minimize an objective function F characterized by the magnitude of optical flows between commonly shared regions, as represented by M. Note that M is the hidden variable in our problem formulation (that is, we need to estimate M from Y). A popular strategy to address this chicken-and-egg problem is to alternatively solve the two subproblems of unknown M and Y\u0302 . More specifically, the optimization of F is decomposed into the following two subproblems:\n(M\u03021, \u00b7 \u00b7 \u00b7 , M\u0302q) = argmin M F(Y\u03021 \u2299M1, \u00b7 \u00b7 \u00b7 , Y\u0302q \u2299Mq) ,\n(Y\u03021, \u00b7 \u00b7 \u00b7 , Y\u0302q) = argmin Y\nF(Y1 \u2299 M\u03021, \u00b7 \u00b7 \u00b7 , Yq \u2299 M\u0302q) , (2)\nwhere Y/Y\u0302 denotes the magnitude of the optical flow values before and after stabilization and \u2299 is the Hadamard product. Instead of analyzing the estimated motion in the frequency domain, we hypothesize that stabilized videos are the fixed points of video stabilization algorithms, which minimize the above objective function. A fixed point [1] is a mathematical object that does not change under a given transformation. Numerically, fixed-point iteration is a method of computing fixed points of a function. It has been widely applied in data science [7], and image restoration [5, 26]. Here, we denote F in Eq. (2) as the function to be optimized, and the fixed point of F is defined as the stabilized video. Next, we solve these two subproblems by constructing the module of stabilization."
        },
        {
            "heading": "4.1. Probabilistic Flow Field",
            "text": "First, we start with an interesting observation. When playing an unstable video at a slower speed (e.g., from 50fps to 30fps), the video tends to appear less wobbly. It follows from the observation that the fundamental cause of video instability is the fast frequency and the large amplitude of the still objects\u2019 motion speed. Therefore, the core task of motion smoothing is to identify the region that needs to be stabilized. As Yu et al. [52] pointed out, mismatches, moving objects, and inconsistent motion areas lead to variation of the estimated motion from the true motion trajectories and should be masked. An important new insight brought about by this work is that these regions of inconsistency in the optical flow fields tend to show greater variability as the frame\ninterval increases. Therefore, if these inconsistent regions are excluded, the video stabilization task can be simplified to the first sub-problem in Eq. (2) - i.e., minimizing F over M for a fixed Y .\nTo detect unreliable matches and inconsistent regions, we have adopted the probabilistic flow network \u2013 PDCNet [38, 39], that jointly tackles the problem of dense correspondence and uncertainty estimation, as our building block. Suppose that PDCNet estimates the optical flow Yk from frame k + d to frame k with the resulting confidence map Ck. Although Ck denotes the inaccuracy of the optical flow estimate in Yk, it is often sensitive to the frame interval d. For example, it is difficult to identify the inconsistent region when d is small, while the common area is less controllable when d is large. Therefore, simply using an optical flow field to estimate inconsistent regions is not sufficient.\nWe have designed a more robust solution to the joint estimation of dense flow and confidence map based on a coarseto-fine strategy. The basic idea is to first obtain the probabilistic flow field at a coarse scale (e.g., with the downsampled video sequence by a factor of d along the temporal axis) and then fill in the rest (i.e., the frames between the adjacent frames in the down-sampled video) at the fine scale. Such a two-level estimation strategy effectively overcomes the limitations of PDCNet by propagating the estimation results of probabilistic flow fields bidirectionally, as we will elaborate next.\nCoarse-scale strategy As shown by the second-to-last column in Fig. 2, the confidence map estimated by PDCNet can identify the mismatched region, but fails to locate the objects with small motions (e.g., people and the sky). To overcome this difficulty, we introduce the binary mask as a warped and aggregated confidence map (refer to the last column of Fig. 2). Specifically, we first propose to obtain the confidence map C\u0302k+(n\u22121)d with down-sampled video\n(the last row of Fig. 2). In the forward direction, we estimate dense flow and confidence map using PDCNet; then C\u0302k+(n\u22121)d is backpropagated to update the binary mask M\u0302 by thresholding and setting intersection operators. Through bidirectional propagation, the region covered by M is the shared content from frame k to frame k + (n \u2212 1)d. The complete procedure can be found in Supp. S1.\nFine-scale strategy Based on the coarse-scale estimation result for downsampled video (i.e., M = {M\u0302k, M\u0302k+d, \u00b7 \u00b7 \u00b7 , M\u0302k+(n\u22121)d}), we fill in the missing d \u2212 1 frames at the fine scale. Specifically, considering the sequential frames from k to k + d, we can obtain two sets similar to Eq. (1), which are Y = {Yk, Yk+1, \u00b7 \u00b7 \u00b7 , Yk+d\u22121, Y\u0302k+d} and their corresponding confidence map set C. Note that M\u0302k+d \u2208 M has been calculated in coarse stage. Setting d = 1, we can call the algorithm again to obtain the set of output masks M for the rest of d\u2212 1 frames."
        },
        {
            "heading": "4.2. Coarse-scale Stabilizer",
            "text": "To coarsely stabilize the video, we first propose aligning the adjacent frames with a global affine transformation [54]. The optimization function F in Eq. (2) is represented as\nT\u2217 = argmin T T(Y \u2299 M\u0302) , (3)\nwhere T(\u00b7) denotes the image transformation applied to the shared region M\u0302 (the result of Sec. 4.1) of the optical flow field Y . Most conventional methods [24,45,52] adopt image matching to obtain T(\u00b7)- e.g., keypoint detection [2,25,32], feature matching [2, 3, 53]; and camera pose estimation is implemented by OpenCV-python. However, these methods are often time-consuming and computationally expensive. For example, two adjacent frames of unstable videos usually share a large area and are free from perspective transformation. Thus, an affine transformation, including translation, rotation, and scaling, is sufficient. More importantly, within the optimization-based learning framework, we can regress these linear parameters of T(\u00b7) from the optical flow field, which characterize the relative coordinate transformation of the matched features.\nWe propose a novel camera pose regression network, as shown in Fig. 3. Given an optical flow field Y and the cor-\nresponding mask field M\u0302 , our network \u03a6(\u00b7) can directly estimate the unknown parameters T(\u00b7) \u221d {\u03b8, s, dx, dy} = \u03a6(Y, M\u0302). To solve the optimization problem of Eq. (3), we use the estimated parameters to iteratively compute the corresponding residual optical flow fields such that\nY\u0304 = Y \u2212 (S \u00b7R \u00b7 V + T ) , (4)\nwhere T , S, and R, respectively, denote the translation, scaling, and rotation matrix, and V \u2208 R2\u00d7H\u00d7W represents an image coordination grid. Then {\u2206\u03b8,\u2206s,\u2206x, \u03b4y} = \u03a6(Y\u0304 , M\u0302) is calculated iteratively to produce the updated parameters {\u03b8+\u2206\u03b8, s \u00b7\u2206s, dx+\u2206x, dy+\u2206y}. The finally estimated affine transformation is smoothed by a moving Gaussian filter with a window size of 20 pixels.\nLoss functions Our loss functions include robust loss \u21131 and grid loss commonly applied in consistency filtering [53]. We directly calculate the \u21131 loss between the predictions and their ground truth {\u03b8\u0302, s\u0302, d\u0302x, d\u0302y},\nLgt =\u03bb\u03b8 \u2225| \u03b8 \u2212 \u03b8\u0302 |\u22251 +\u03bbs \u2225| 1\u2212 s\ns\u0302 |\u22251\n+ \u03bbt \u2225| dx \u2212 d\u0302x | + | dy \u2212 d\u0302y |\u22251 . (5)\nFor better supervision of the estimated pose, we calculate the loss Lgrid with the grid V \u2208 R2\u00d7h\u00d7h in Eq. (4), i.e.,\nLgrid =\u2225 (S\u0302 \u00b7 R\u0302 \u00b7 V + T\u0302 )\u2212 (S \u00b7R \u00b7 V + T ) + \u03f5 \u22251 (6)\nwhere \u00b7\u0302 denotes the ground truth and \u03f5 is a small value for stability. The final loss Lstab consists of Lgt and Lgrid,\nLstab = Lgt + \u03bbgridLgrid . (7)"
        },
        {
            "heading": "4.3. Fine-scale Stabilizer",
            "text": "The assumption with an affine transformation of the coarse-scale stabilizer could cause structural discontinuity and local distortion. Therefore, our objective is to refine the coarsely stabilized video by optical flow smoothing. Unlike Eq. 3 which applies an image transformation matrix to optimize the optical flow field, we optimize it at the pixel level by a flow warping field W. Thus, the function F in Eq. (2) is given by\nW\u2217 = argmin W N\u22121\u2211 i=0 Wi(Yi \u2299 M\u0302i) . (8)\nThe flow smoothing network follows the U-Net architecture in [58]. We useN frames of optical flow fields F and mask fields M\u0302 as input and obtain (N\u22121) frame warp fields W of intermediate frames. Specifically, for the optical flow field Yk (from frame k + 1 to frame k), we denote the aligned matrices of each frame in Sec. 4.2 as Hk \u2208 R2\u00d73 and\nHk+1 \u2208 R2\u00d73, respectively. Then the input optical flow Fk \u2208 R2\u00d7HW can be represented by\nFk = Hk+1 \u00b7 [V + Yk | 1]\u2212Hk \u00b7 [V | 1] , (9)\nwhere V \u2208 R2\u00d7HW and [\u00b7|1] denote the normalized coordinate representation. Furthermore, to better adapt the flow smoothing network to the mask field M\u0302 , we fine-tune it using our synthetic dataset (as we will elaborate in Sec. 6.1). The loss function follows the motion loss [51]\nLsmooth = N\u22121\u2211 k=0 (Fk +Wk \u2212 Fk(Wk+1))\u2299 M\u0302 , (10)\nwhere W0 = WN = 0 ."
        },
        {
            "heading": "5. Video Outpainting Network",
            "text": "Most video stabilization methods crop an input video with a small field of view (FOV), excluding missing margin pixels due to frame warping. In contrast, full-frame video stabilization [6, 24, 27] proposes to generate a video that maintains the same FOV as the input video without cropping. They directly generate stabilized video frames with large FOV by fusing the information from neighboring frames. An important limitation of existing fusion strategies is the unequal importance of different frames (i.e., the current and neighboring frames are weighted equally), which would lead to unpleasant distortions in fast-moving situations. To overcome this weakness, we propose a two-stage framework to combine flow and image outpainting strategies [33]. In the first stage, we used flow-outpainting to iteratively align the neighboring frames with the target frame. In the second stage, we fill the target frame with adjacent aligned frame pixels by image outpainting."
        },
        {
            "heading": "5.1. Flow Outpainting Network",
            "text": "Let It denote the outpainted target image and Is the neighboring source image. We aim to fill the missing pixel region M t\u2205 of I\nt with the pixels of Is. As shown in Fig. 4, we take two different FOVs of It as input and obtain the corresponding optical flow fields and warped results Iswarp. The small FOV It cannot guide Is well to fill the regions in M t\u2205. Since the predicted optical flow field in M t \u2205 is unreliable due to the lack of pixel guidance of It, the out-of-view\nregion of M t\u2205 has artifacts (marked in the last column of Fig. 4). We observe that the optical flow field of the large FOV It is continuous in M t\u2205, which inspired us to extrapolate the flow of M t\u2205 using the reliable flow region (the 3rd column of Fig. 4).\nWe propose a novel flow outpainting network (Fig. 5), which extrapolates the large FOV flow field using a small FOV flow field and the corresponding valid mask. Specifically, we adopt a U-Net architecture and apply a sequence of gated convolution layers with downsampling / upsampling to obtain the large flow field of FOV Ylarge. Note that the input flow field Ysmall and the valid maskMvalid have been estimated by PDCNet (see Sec. 4.1).\nLoss functions Our loss functions include robust loss \u21131 and loss in the frequency domain [4]. We directly calculate the \u21131 loss between Ylarge and its ground truth Y\u0302large,\nLY =\u03bbin\u00b7 \u2225| Ylarge \u2212 Ysmall | \u2299Mvalid \u22251 + \u03bbout\u00b7 \u2225| Ylarge \u2212 Y\u0302large | \u2299(\u223c Mvalid) \u22251 . (11)\nTo encourage low frequency and smoothing Ylarge, we add the loss in the frequency domain LF =\u2225 G\u0302 \u00b7 FYlarge \u22252 , where the normalized Gaussian map G\u0302 with \u00b5 = 0 and \u03c3 = 3 is inverted by its maximum value and FYlarge denotes the Fourier spectrum of Ylarge. The final loss consists of LY and LF ,\nLoutpaint = LY + \u03bbFLF . (12)"
        },
        {
            "heading": "5.2. Image Margin Outpainting",
            "text": "Based on our proposed flow-outpainting network, we design a margin-outpainting method by iteratively aligning frame pairs (see Fig. 6). As discussed in Sec. 5.1, we can obtain a large FOV flow field Ylarge. The neighboring reference frame Is is warped as Iwarp = Ylarge(Is) as shown in Fig. 6. In theory, the outpainted frame Iresult = It \u00b7Mvalid + Iwarp \u00b7 (\u223cMvalid). However, we notice that there are obvious distortions at the image border. To further align the margins, we take a margin fusion approach (the detailed algorithm can be found in Supp. S1) We crop Iwarp and It to Isc and I t c. Then, we can obtain a new warped frame Iwarpc by flow outpainting. In particular, we did not choose to add Iwarpc and I t directly. To identify the\nmisaligned region, we propose to outpaint the mask MIt by extending the watershed outward from the center. Instead of a preset threshold, we adaptively choose between the target image It and the warped image Iwarpc . Then, the final frame Iresult consists of It and Iwarpc : I\nresult = It \u00b7 MIt + Iwarpc \u00b7 (\u223c MIt) . Compared to the two results Iresult, our strategy successfully mitigates misalignment and distortions at the boundary of video frames.\nMulti-frame fusion During the final stage of rendering, we use a sequence of neighboring frames to outpaint the target frame, while they may have filled duplicate regions. It is important to find out which frame and which region should be selected. We proposed the selection strategy for multiframe fusion (the details can be found in Supp. S1). By weighing the metric parameters of each frame, we finally obtain the target frame with large FOV. Note that each frame has an added margin in the stabilization process, so we need to crop them to the original resolution. Although we have outpainted the target frame, some missing pixel holes may still exit at boundaries. Here, we apply the state-of-the-art LaMa image inpainting method [37] to fill these holes using nearest-neighbor interpolation."
        },
        {
            "heading": "6. Experiments",
            "text": ""
        },
        {
            "heading": "6.1. Synthetic Datasets for Supervised Learning",
            "text": "Due to the limited amount of paired training data, we propose a novel model-based data generation method by carefully designing synthetic datasets for video stabilization. For our base synthetic dataset, we used a collection of images from the DPED [14], CityScapes [8] and ADE20K [57] datasets. To generate a stable video, we randomly generate the homography parameters for each image, including rotation angle \u03b8, scaling s, translations (dx, dy) and perspective factors (px, py). Then we divide these transformation parameters into N bins equally and obtain a video of N frames by homography transformations. To simulate\nthe presence of moving objects in real scenarios, the stable video is further augmented with additional independently moving random objects. To do so, the objects are sampled from the COCO dataset [16] and inserted on top of the synthetic video frames using their segmentation masks. Specifically, we randomly choose m objects (no more than 5), and generate randomly affine transformation parameters for each independent of the background transformation. Finally, we cropped each frame to 720 \u00d7 480 around its center. For different training requirements, we apply various combinations of synthetic dataset. The implementation and training details can be found in Supp. S2 , S3."
        },
        {
            "heading": "6.2. Quantitative Evaluation",
            "text": "We compare the results of our method with various video stabilization methods, including Grundmann et al. [11], Liu et al. [22], Wang et al. [40], Yu and Ramamoorthi [51, 52], DUT [45], OVS [46], DIFRINT [6], and FuSta [24]. We obtain the results of the compared methods from the videos released by the authors or generated from the publicly available official implementation with default parameters or pre-trained models. Note that OVS [46] does not honor their promise to provide code, thus we only report the results from their paper.\nDatasets. We evaluate all approaches on the NUS dataset [22], DeepStab dataset [40], and Selfie dataset [50]. The NUS dataset consists of 144 videos and the corresponding ground truths in 6 scenes. The DeepStab dataset contains 61 videos and the Selfie dataset consistsof 33 videos.\nMetrics. We introduce three metrics widely used in many methods [6, 24, 52] to evaluate our model:1) Cropping ratio measures the remaining frame area after cropping off the invalid boundaries. 2) Distortion value evaluates the anisotropic scaling of the homography between the input and output frames. 3) Stability score measures the stability of the output video. We calculate the metrics using the evaluation code provided by DIFRINT.\nQuantitative comparison. The results of the NUS dataset [22] are summarized in Table 1 (Per-category result can be found in Supp. S5). Overall, our method achieves the best distortion value compared to the state-of-the-art method, FuSta [24]. Especially in the quick-rotation and zoom categories, our method outperforms pure image generation methods [6,24]. We suspect that the reduction of the shared information between frames causes the image generation methods to prefer artifacts. However, our method can ensure local structural integrity when outpainting the margin region. Furthermore, our method achieves an average cropping ratio of 1.0 and stability scores comparable to recent approaches [6, 24, 51, 52]. Since FuSta [24] uses Yu et al. [52] to obtain stabilized input videos, they have the same stability scores. It is important to note that although the stability scores are competitive, our method runs 5 times faster than [52] in the video stabilization stage. Moreover, the comparison results on DeepStab dataset [40] and Selfie dataset [50] are also reported in Table 1. Our method still shows effectiveness in different datasets, proving the generalizability of the proposed method. Note that, due to that Wang et al. [40] is trained on the DeepStab dataset, we do not report its results on the DeepStab dataset for a fair comparison. Qualitative comparisons can be found in Supp. S4 and supplementary video."
        },
        {
            "heading": "6.3. Ablation Study",
            "text": "Importance of mask generation. We investigate the influence of the mask on video stabilization at different stages. To better demonstrate the necessity of mask in complex sce-\nnarios, we choose the Crowd category of NUS dataset [22] which includes a display of moving pedestrians and occlusions. Stability and distortion at different settings are shown in Table 2. It can be seen that the performance with mask increases significantly in both stabilizers. Specifically, the mask can both improve stability globally and alleviate image warping distortion locally. This result demonstrates the importance of mask generation for video stabilization.\nFlow outpainting. We compare our flow outpainting method with the traditional flow inpainting method PCAFlow [43]. Following [52] we fit the first 5 principal components proposed by PCAFlow to the Mvalid = 1 regions of the optical flow field, and then outpaint the flow vectors in the Mvalid = 0 regions with reasonable values of the PCA Flow fitted. The result is obtained by warping the source image with the outpainted optical flow field. We perform this comparison on our synthetic validation set and evaluate it with the corresponding ground truth. Additionally, we use PSNR and SSIM [42] to evaluate the quality of the results. As shown in Table 3, ours dramatically outperforms PCAFlow [43] in all objective metrics.\nImportance of image filling strategies. We explore the following proposed strategies for image filling: margin outpainting, mask generation MIt , and multiframe selection. We isolate them from our method to compare their results with the complete version. The results are shown in Table 3. The proposed strategies are generally helpful in improving image quality. Especially, margin outpainting and mask MIt are crucial to the results."
        },
        {
            "heading": "6.4. Runtime Comparison",
            "text": "Our network and pipeline are implemented with PyTorch and Python. Table 4 is a summary of the runtime per frame for various competing methods. All timing results are obtained on a desktop with an RTX3090Ti GPU and an Intel(R) Xeon(R) Gold 6226R CPU. First, we compare the run-time of pose regression. Traditional pose regression is time consuming in feature matching [2, 9], homography estimation, and SVD decomposition [29]. Although our learning-based pose regression network runs 10 times faster than the traditional framework. Then, we report the average run time of different methods, including optimization-\nbased [22, 51] and learning-based [6, 11, 24, 40, 52]. Our method takes 97ms which gives \u223c 5x speed-up. This is because our method computes the optical flow field only once and without the help of other upstream task methods and manual optimization."
        },
        {
            "heading": "6.5. Fixed-point Experiment",
            "text": "To demonstrate the stability of our fixed-point optimization solution, we performed an interesting toy experiment. We input a sequence of shaky frames into our coarse-to-fine stabilizers, and the stabilized result will be iteratively restabilized by our stabilizers. For each iteration, we calculate the average magnitude of optical flow filed with global transformation and flow warping for each frame. Specifically, the regions where we calculate are marked by M\u0302 . As shown in Fig. 7(a), the deviation of the shaky frames decreases rapidly with each iteration. Furthermore, we have pointed out that plugging a stabilized video into the stabilization system should not have an impact on the input. Thus, we plug stable frames into the coarse-to-fine stabilizers and iteratively stabilize them. The result is shown in Fig. 7(b). The deviation of each frame is perturbed around the value of zero. Obviously, our method has no effect on stable frames, which shows that stabilized video is indeed the fixed point of our developed stabilization system."
        },
        {
            "heading": "7. Conclusion",
            "text": "In this paper, we have presented a fast full-frame video stabilization technique based on the iterative optimization\nstrategy. Our approach can be interpreted as the combination of probabilistic stabilization network (coarse-tofine extension of PDC-Net) and video outpainting outwork (flow-based image outpainting). When trained on synthetic data constructed within the optimization-based learning framework, our method achieves state-of-the-art performance at a fraction of the computational cost of other competing techniques. It is also empirically verified that stabilized video is the fixed point of the stabilization network."
        },
        {
            "heading": "Appendix A. Algorithm Details",
            "text": "We introduce 3 algorithms in the main paper. In this section, we supplement the algorithm details of the confidence map back-propagation, margin fusion approach and multiframe fusion strategy in the main paper.\nConfidence map back-propagation. Algorithm 1 summaries the strategy of confidence map back-propagation in the main paper Section 4.1. The parameters in Algorithm 1 are set to k = 5, d = 10, and \u03b4C = 0.5.\nAlgorithm 1 Back-propagation for aggregated confidence map Input: Y: optical flow; C: confidence map; \u03b4C : threshold\nfor confidence map; d: sampling interval; k: index of the first frame; Output: M: updated mask containing aggregated confidence map;\n1: setmpre = 1(C\u0302k+(n\u22121)d\u2212\u03b4C) \u2208 C, putmpre into M; 2: for i = n\u2212 1; i >= 0; i\u2212\u2212 do 3: the optical flow field Ywarp = Yk+id \u2208 Y; 4: using Ywarp to warpmpre to m\u0302pre = Ywarp(mpre); 5: the binarized confidence map mnew = 1(Ck+id \u2212 \u03b4C), where Mk+id \u2208 C; 6: the final mask field M\u0302k+id = m\u0302pre &mnew 7: put M\u0302k+id into M; 8: mpre = M\u0302k+id 9: end for\nMargin fusion. The complete pipeline of the margin fusion approach is shown in Fig. 8. At first, we coarsely align the reference frame Is and the target frame It. We then crop Iwarp and It to Isc and I t c , and re-align them by the optical flow outpainting. Per Algorithm 2, we further calculate the mask MIt which indicates the chosen regions of It. The final result Iresult is obtained by combining It, MIt , and Iwarpc . The parameters in the Algorithm 2 are set to \u03b4D = 0.2, \u03b7t = 20, and ktin = 11.\nMulti-frame fusion. To adaptively determine which frame and which region should be selected, the multi-frame fusion strategy is illustrated in Algorithm 3. The parameters in the Algorithm 3 are set to \u03b7u = 25k, \u03b7r = 1.2, and \u03b7s = 2k."
        },
        {
            "heading": "Appendix B. Synthetic Dataset for Training",
            "text": "We proposed a model-based synthetic dataset in this paper. The settings of the homography parameters are as follows: The maximum rotation angle \u03b8 is set to 10\u25e6. The range of scaling s is set to 0.7 \u223c 1.3. The maximum translations (dx, dy) in the x and y directions are 100 and 70,\nAlgorithm 2 Outpainting mask Algorithm Input: It: target frame; Itc: cropped target frame; Isc :\ncropped source frame; Iwarpc : warped frame of I s c ; M t: valid mask of It; M tc : valid mask of I t c; Output: MIt : unchanged mask of It; 1: extract feature maps with VGG-16 network f tc = V GG(Itc) , f warp c = V GG(I warp c );\n2: calculate the Euclidean distance in feature space D =\u2225 f tc \u2212 fwarpc \u22252; 3: MD = D < \u03b4D; 4: labeled region Mlabel; 5: for i, j = 0; i < h, j < w; i++, j ++ do 6: if \u223cM t[i, j] then Mlabel[i, j] = 1; 7: else if M t[i, j]&(\u223c M tc [i, j]) then Mlabel[i, j] =\n2; 8: else if M t[i, j]&M tc [i, j]&MD[i, j] then Mlabel[i, j] = 0;\n9: elseMlabel[i, j] = \u22121; 10: end if 11: end for 12: tin =Mlabel, tout = 0, f lag = True; 13: while Sum(tin \u2212 tout) > \u03b7t do 14: if flag then 15: tin = tout, f lag = False; 16: end if 17: inflate tin with kernel size ktin , obtain tout =\ninflate(tin); 18: tout[Mlabel == 1] = 1; 19: tout[Mlabel == \u22121] = \u22121; 20: end while 21: MIt = (tout == 2);\nrespectively. The maximum perspective factors in the x direction and in the y direction are 0.1 and 0.15.\nFor different training requirements, we apply various combinations of synthetic dataset, as shown in Fig. 9 (more visualizations can be found in the Supplementary Video). For camera pose regression, we use the large FOV video pair of stable and unstable. For training the flow smoothing network, we alternatively adopt small FOV video pairs, which simulate coarsely stabilized video. Aiming at the flow outpainting network, we take small-FOV stable videos for training and large-FOV for ground-truth supervising.\nData for Camera Pose Regression. For training the camera pose regression network, we need to generate unstable videos. For every frame, a random homography matrix produces an unstable frame. In practice, the perspective effects in the x direction and the y direction are restricted to 1e\u22125 \u223c 5e\u22125. The pose between two unstable frames is parameterized by rotation, scaling, and translation. Data for Flow smoothing. For training the flow smoothing\nnetwork, we need to generate unstable videos with small FOV. Specifically, for the stable video, we randomly generate a series of cropping mask. The cropped stable video will be jittered by random homography transformations. Then, we obtain a cropped unstable video for training and the cropped stable video for supervision.\nData for Flow Outpainting. To supervise the learning of large-FOV optical flow fields, we mask the boundaries of stable videos. Specifically, we set up a sliding window 640 \u00d7 360, which moves randomly with the video timeline. Then, we obtain a cropped video for training and the corresponding full-frame video for supervision.\nAlgorithm 3 Multi-frame Fusion Algorithm Input: It: target frame; Iwarpck : warped of cropping source\nframe Isk; I result k : margin outpainting result of I warp ck ; Mwarpk : valid mask of I warp ck\n; Output: Ifuse: output fusion frame;\n1: calculate the filling areaAsk, misaligned region areaA u k ,\nand corresponding IoU ration Sk = Auk/(A s k + 1) of\nIwarpck ; 2: sorted by Ask to obtain index list IDs; 3: Ifuse = It,Mfuse =Mwarpk ; 4: for k in IDs do 5: if (Auk < \u03b7u)&(Sk > \u03b7r)&(Ask > \u03b7s) then 6: compute overlapped area Aok between I warp ck\nand Ifuse; 7: if (Aok/Ask < \u03b4r) then 8: Ifuse = Ifuse \u00b7 (\u223c Mwarpk ) + Iresultk \u00b7 Mwarpk\n9: end if 10: else 11: continue; 12: end if 13: end for"
        },
        {
            "heading": "Appendix C. Implementation Details",
            "text": "We will illustrate the training details of different networks, including the camera pose regression network, the optical flow smoothing network, and the flow outpainting network. All networks are implemented using Pytorch. Camera pose regression network. We first describe the architecture of the camera pose regression network. The network processes each input concatenated tensor fin \u2208 Rb\u00d73\u00d7h\u00d7w with several 2D convolutional layers, where b indicates the batch dimension and h\u00d7w indicates the spatial dimensions. The final predicted parameters are obtained by a series of 1D convolutional layers. We use a batch size of 40 and train for 10k iterations. we use Adam optimizer [15] with a constant leaning rate of 10\u22124 for the first 4k iterations, followed by an exponential decay of 0.99995 until iteration 10k. The input resolution is set to 256\u00d7 512. The weights in training loss Eq. (5) and Eq. (7) in the main paper are set to \u03bb\u03b8 = 1.0, \u03bbs = 1.0, \u03bbt = 1.5, \u03bbgrid = 2.0 for the first 6k iterations and \u03bb\u03b8 = 2.0, \u03bbs = 8.0, \u03bbt = 1.0, \u03bbgrid = 2.0 for the remaining 4k iterations.\nOptical flow smoothing network. We use a batch size of 6 and train for 20k iterations. we use Adam optimizer [15] with a constant leaning rate of 10\u22124 for the first 10k iterations, followed by an exponential decay of 0.99995 until iteration 20k. The input resolution is set to 488\u00d7 768.\nFlow outpainting network. We apply an Unet architecture with gated convolution layers [49] as a flow-outpainting net-\nwork. We use a batch size of 12 and train for 20k iterations. we use the Adam optimizer [15] with a constant leaning rate of 10\u22124. The input resolution is set to 488 \u00d7 768. The weights in training loss Eq. (14) in the main article are set to \u03bbin = 2.0, \u03bbout = 1.0, \u03bbF = 10.0 for the first 10k iterations and \u03bbin = 0.6, \u03bbout = 1.0, \u03bbF = 0.0 for the remaining 10k iterations."
        },
        {
            "heading": "Appendix D. Qualitative Evaluation",
            "text": "We show the results of the comparison of our method and the latest approaches in Fig. 10. Most methods [11, 22, 40, 52] suffer from a large amount of cropping, as indicated by the green checkerboard regions. Compared to full frame rendering approaches for interpolation [6] / generation [24], our method shows fewer visual artifacts. In particular, FuSta [24] would discard most of the input frame content for stabilization and deblurring, while we argue that video stabilization is based on destroying as little of the input frame content as possible. Thus, our method preserves the original content of the input frame as much as possible. We strongly recommend that the reviewers see our additional supplementary video, especially the comparison with other full-frame approaches (FuSta [24], DIFRINT [6])."
        },
        {
            "heading": "Appendix E. More Experimental Results",
            "text": "Per-category Evaluation. We present the the average scores for the 6 categories in the NUS dataset [22]. Two-stage Stabilization. To illustrate our two-stage stabilization method, we conduct an interesting experiment. We tracked the position (x, y) of a fixed keypoint in 10 frames, where every two frames were spaced 5 frames apart. As shown in Fig. 12, the trajectory of the shaky keypoint converges to a fixed/stable position through two-stage stabilization. Analysis of Runtime. We attribute the faster runtime of our approach against FuSta to the following three reasons: i) The traditional pose regression algorithm used in FuSta is 10 times slower than our proposed pose regression network (see Section 6.4); ii) Our method only requires computing optical flow once per frame, while FuSta requires computing it three times and relies on additional task-specific optimization and manual adjustments (see Section 6.4); iii) In the rendering stage, FuSta takes input from 11 RGB frames and their corresponding optical flow, whereas our approach only requires 7 frames. We will highlight these reasons in the final version of the manuscript."
        },
        {
            "heading": "Appendix F. Network Architectures",
            "text": "Camera pose regression network. We first describe the architecture of the camera pose regression network. Given\nGrundman et.al. Liu et.al. Wang et.al.\nInput\nYu et.al.\nFuStaDIFIRINT Ours\nGrundman et.al. Liu et.al.\nInput\nYu et.al.\nFuStaDIFIRINT Ours\nWang et.al.\nGrundman et.al. Liu et.al.\nYu et.al.\nWang et.al.\na concatenated input tensor fin \u2208 R3\u00d7H\u00d7W , we process it with multiple down-sampled convolution layers and flatten the output feature map to fout \u2208 Rd\u00d7 HW D\u00d7D , where d,D denotes the dimension of the feature channel and the spa-\ntial down-sampling ratio, respectively. The feature vector fsum, obtained by weighting the sum of fout along the feature channel, regresses all parameters of the affine transfor-\nmation. given by\nw = \u03c8(fout), fsum = HW D\u00d7D\u2211 i=0 wifout(i, \u00b7), {\u03b8, s, dx, dy} = \u2127(fsum) . (13) Specifically, The network processes each input concatenated tensor fin \u2208 Rb\u00d73\u00d7h\u00d7w with several 2D convolutional layers, as shown in Table 5, where b indicates the batch dimension and h\u00d7w indicate the spatial dimensions. The final predicted parameters are obtained by a series of 1D convolutional layers.\nFlow outpainting network. We apply a Unet architecture with gated convolution layers [49] as a flow outpainting network, as shown in Table 6.\nTable 5. Modular architecture of camera pose regression network modules. Each convolution operator is followed by batch normalization and LeakyReLU (negative slope=0.1), except for the last one. K refers to the kernel size, s denotes the stride, and p indicates the padding. We apply the Max-pooling layer to downsample each feature map.\nInput Size Convolution Layer Output Size\n(K \u00d7K, s, p)\nFeature map extraction\ninput: b\u00d7 3\u00d7 h\u00d7 w conv0: (3\u00d7 3, 1, 1) b\u00d7 8\u00d7 h\u00d7 w\nconv0: b\u00d7 8\u00d7 h\u00d7 w conv1: (3\u00d7 3, 1, 1) b\u00d7 32\u00d7 h\u00d7 w\nconv1: b\u00d7 32\u00d7 h\u00d7 w pool1: (5\u00d7 5, 2, 4) b\u00d7 32\u00d7 h 4 \u00d7 w 4 pool1: b\u00d7 32\u00d7 h 4 \u00d7 w 4 conv2: (3\u00d7 3, 1, 1) b\u00d7 64\u00d7 h 4 \u00d7 w 4 conv2: b\u00d7 64\u00d7 h 4 \u00d7 w 4 pool2: (5\u00d7 5, 2, 4) b\u00d7 64\u00d7 h 16 \u00d7 w 16 pool2: b\u00d7 64\u00d7 h 16 \u00d7 w 16 conv3: (3\u00d7 3, 1, 1) b\u00d7 64\u00d7 h 16 \u00d7 w 16\nCamera pose regression\ninput: b\u00d7 64\u00d7 1 conv1: (1, 1, 0) b\u00d7 32\u00d7 1\nconv1: b\u00d7 32\u00d7 1 conv2: (1, 1, 0) b\u00d7 16\u00d7 1\nconv2: b\u00d7 16\u00d7 1 conv3: (1, 1, 0) b\u00d7 4\u00d7 1\nTable 6. Architecture of the flow-outpainting network. Each 2D gated-convolution [49] (\u2018G conv\u2019) is followed by batch normalization and Sigmoid. The final \u2018conv\u2019 denotes the 2D convolution layer without batch normalization and Sigmoid. K refers to the kernel size, s denotes the stride, and p indicates the padding. We apply the Maxpooling Layer for downsampling (\u2018down\u2019) and bilinear interpolation for upsampling (\u2018up\u2019).\nInput Size Convolution Layer Output Size\n(K \u00d7 K, s, p)\ninput: b \u00d7 3 \u00d7 h \u00d7 w down 0 b \u00d7 3 \u00d7 h4 \u00d7 w 4 down 0: b \u00d7 3 \u00d7 h4 \u00d7 w 4 G conv0: (3 \u00d7 3, 1, 1) b \u00d7 16 \u00d7 h 4 \u00d7 w 4 G conv0: b \u00d7 16 \u00d7 h4 \u00d7 w 4 down 1 b \u00d7 16 \u00d7 h 8 \u00d7 w 8 down 1: b \u00d7 16 \u00d7 h8 \u00d7 w 8 G conv1: (3 \u00d7 3, 1, 1) b \u00d7 64 \u00d7 h 8 \u00d7 w 8 G conv1: b \u00d7 64 \u00d7 h4 \u00d7 w 4 down 2 b \u00d7 64 \u00d7 h 16 \u00d7 w 16 down 2: b \u00d7 64 \u00d7 h16 \u00d7 w 16 G conv2: (3 \u00d7 3, 1, 1) b \u00d7 64 \u00d7 h 16 \u00d7 w 16 G conv2: b \u00d7 64 \u00d7 h16 \u00d7 w 16 conv0: (3 \u00d7 3, 1, 1) b \u00d7 64 \u00d7 h 16 \u00d7 w 16 conv0: b \u00d7 64 \u00d7 h16 \u00d7 w 16 G conv3: (3 \u00d7 3, 1, 1) b \u00d7 32 \u00d7 h 16 \u00d7 w 16 G conv3: b \u00d7 32 \u00d7 h16 \u00d7 w 16 up 0 b \u00d7 32 \u00d7 h 8 \u00d7 w 8 up 0+G conv1: b \u00d7 96 \u00d7 h8 \u00d7 w 8 G conv4: (3 \u00d7 3, 1, 1) b \u00d7 16 \u00d7 h 8 \u00d7 w 8 G conv4: b \u00d7 16 \u00d7 h8 \u00d7 w 8 up 1 b \u00d7 16 \u00d7 h 4 \u00d7 w 4 up 1+G conv0: b \u00d7 32 \u00d7 h4 \u00d7 w 4 conv0: (3 \u00d7 3, 1, 1) b \u00d7 2 \u00d7 h 4 \u00d7 w 4 conv0: b \u00d7 2 \u00d7 h4 \u00d7 w 4 up 2 b \u00d7 2 \u00d7 h \u00d7 w"
        },
        {
            "heading": "Appendix G. Limitations",
            "text": "Although our method achieves a comparable stability score, we use only a simple Gaussian sliding window filter to smooth the camera trajectory in the coarse stage, leaving room for further improvement. In addition, our rendering strategy could generate artifacts in human-dense scenar-\nios due to the nonrigid transformation of the human body, breaking our assumption of local spatial coherence."
        }
    ],
    "title": "Fast Full-frame Video Stabilization with Iterative Optimization",
    "year": 2023
}