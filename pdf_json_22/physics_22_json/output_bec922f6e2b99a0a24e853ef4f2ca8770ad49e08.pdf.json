{
    "abstractText": "We empirically investigate the effect of class manifold entanglement and the intrinsic and extrinsic dimensionality of the data distribution on the sample complexity of supervised classification with deep ReLU networks. We separate the effect of entanglement and intrinsic dimensionality and show statistically for artificial and real-world image datasets that the intrinsic dimensionality and the entanglement have an interdependent effect on the sample complexity. Low levels of entanglement lead to low increases of the sample complexity when the intrinsic dimensionality is increased, while for high levels of entanglement the impact of the intrinsic dimensionality increases as well. Further, we show that in general the sample complexity is primarily due to the entanglement and only secondarily due to the intrinsic dimensionality of the data distribution.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniel Kienitz"
        },
        {
            "affiliations": [],
            "name": "Ekaterina Komendantskaya"
        },
        {
            "affiliations": [],
            "name": "Michael Lones"
        }
    ],
    "id": "SP:498d9b991d0dc364792158c6e3804a50fb8c86f0",
    "references": [
        {
            "authors": [
                "Z. Allen-Zhu",
                "Y. Li",
                "Y. Liang"
            ],
            "title": "Learning and generalization in overparameterized neural networks, going beyond two layers",
            "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems, 6158\u20136169.",
            "year": 2019
        },
        {
            "authors": [
                "A. Ansuini",
                "A. Laio",
                "J.H. Macke",
                "D. Zoccolan"
            ],
            "title": "Intrinsic dimension of data representations in deep neural networks",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "P.L. Bartlett"
            ],
            "title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
            "venue": "IEEE transactions on Information Theory, 44(2): 525\u2013536.",
            "year": 1998
        },
        {
            "authors": [
                "P.L. Bartlett",
                "D.J. Foster",
                "M.J. Telgarsky"
            ],
            "title": "Spectrally-normalized margin bounds for neural networks",
            "venue": "Advances in Neural Information Processing Systems, 30: 6240\u20136249.",
            "year": 2017
        },
        {
            "authors": [
                "B.E. Boser",
                "I.M. Guyon",
                "V.N. Vapnik"
            ],
            "title": "A training algorithm for optimal margin classifiers",
            "venue": "Proceedings of the fifth annual workshop on Computational learning theory, 144\u2013152.",
            "year": 1992
        },
        {
            "authors": [
                "P.P. Brahma",
                "D. Wu",
                "Y. She"
            ],
            "title": "Why deep learning works: A manifold disentanglement perspective",
            "venue": "IEEE transactions on neural networks and learning systems, 27(10): 1997\u20132008.",
            "year": 2015
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. FeiFei"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "CVPR09.",
            "year": 2009
        },
        {
            "authors": [
                "K. Fukushima"
            ],
            "title": "Visual feature extraction by a multilayered network of analog threshold elements",
            "venue": "IEEE Transactions on Systems Science and Cybernetics, 5(4): 322\u2013333.",
            "year": 1969
        },
        {
            "authors": [
                "K. Fukushima",
                "S. Miyake"
            ],
            "title": "Neocognitron: A selforganizing neural network model for a mechanism of visual pattern recognition",
            "venue": "Competition and cooperation in neural nets, 267\u2013285. Springer.",
            "year": 1982
        },
        {
            "authors": [
                "X. Glorot",
                "A. Bordes",
                "Y. Bengio"
            ],
            "title": "Deep sparse rectifier neural networks",
            "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics, 315\u2013323. JMLR Workshop and Conference Proceedings.",
            "year": 2011
        },
        {
            "authors": [
                "N. Golowich",
                "A. Rakhlin",
                "O. Shamir"
            ],
            "title": "Sizeindependent sample complexity of neural networks",
            "venue": "Conference On Learning Theory, 297\u2013299. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "I. Goodfellow",
                "Y. Bengio",
                "A. Courville"
            ],
            "title": "Deep learning",
            "venue": "MIT press.",
            "year": 2016
        },
        {
            "authors": [
                "N. Harvey",
                "C. Liaw",
                "A. Mehrabian"
            ],
            "title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks",
            "venue": "Conference on learning theory, 1064\u20131068. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "International conference on machine learning, 448\u2013 456. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "ICLR (Poster).",
            "year": 2015
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Y. LeCun",
                "B.E. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R.E. Howard",
                "W.E. Hubbard",
                "L.D. Jackel"
            ],
            "title": "Handwritten digit recognition with a back-propagation network",
            "venue": "Advances in neural information processing systems, 396\u2013404.",
            "year": 1990
        },
        {
            "authors": [
                "V. Nagarajan",
                "J.Z. Kolter"
            ],
            "title": "Uniform convergence may be unable to explain generalization in deep learning",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "H. Narayanan",
                "S. Mitter"
            ],
            "title": "Sample complexity of testing the manifold hypothesis",
            "venue": "Advances in neural information processing systems, 1786\u20131794.",
            "year": 2010
        },
        {
            "authors": [
                "H. Narayanan",
                "P. Niyogi"
            ],
            "title": "On the Sample Complexity of Learning Smooth Cuts on a Manifold",
            "venue": "COLT.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Netzer",
                "T. Wang",
                "A. Coates",
                "A. Bissacco",
                "B. Wu",
                "A.Y. Ng"
            ],
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning",
            "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011.",
            "year": 2011
        },
        {
            "authors": [
                "B. Neyshabur",
                "S. Bhojanapalli",
                "N. Srebro"
            ],
            "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "P. Pope",
                "C. Zhu",
                "A. Abdelkader",
                "M. Goldblum",
                "T. Goldstein"
            ],
            "title": "The Intrinsic Dimension of Images and Its Impact on Learning",
            "venue": "arXiv preprint arXiv:2104.08894.",
            "year": 2021
        },
        {
            "authors": [
                "D. Stutz",
                "M. Hein",
                "B. Schiele"
            ],
            "title": "Disentangling adversarial robustness and generalization",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6976\u20136987.",
            "year": 2019
        },
        {
            "authors": [
                "V. Vapnik"
            ],
            "title": "Principles of risk minimization for learning theory",
            "venue": "Advances in neural information processing systems, 831\u2013838.",
            "year": 1992
        },
        {
            "authors": [
                "H. Xiao",
                "K. Rasul",
                "R. Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747.",
            "year": 2017
        },
        {
            "authors": [
                "C. Zhang",
                "S. Bengio",
                "M. Hardt",
                "B. Recht",
                "O. Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "arXiv preprint arXiv:1611.03530.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "It is a common assumption that distributions of natural data, such as images, concentrate near or lie on low-dimensional manifolds embedded in high-dimensional ambient spaces (Goodfellow, Bengio, and Courville 2016). The dimension of this manifold is the intrinsic dimensionality of the distribution and the dimension of the ambient space is the extrinsic dimensionality. It has been shown theoretically that the sample complexity of empirical risk minimization depends on the curvature of the data manifold and the decision boundary, and on the number of intrinsic dimensions, but not on the number of extrinsic dimensions (Narayanan and Niyogi 2009; Narayanan and Mitter 2010). Recently, Pope et al. (Pope et al. 2021) provided empirical evidence that real-world image distributions indeed have low intrinsic dimensionality and that the sample complexity for deep classifiers is positively correlated with the intrinsic and almost independent of the extrinsic dimensionality.\nThe goal of this work is to further study the effects on the sample complexity of deep classifiers, however, this time under consideration of the entanglement of the class manifolds (i.e. the curvature of the decision boundary). Intuitively, the entanglement can be defined as the number of connected hyperplanes that are necessary to perfectly separate the classes.\nCopyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nTo the best of the authors\u2019 knowledge this is the first empirical study that factors in the entanglement when studying the sample complexity and systematically compares the effects of intrinsic dimensionality and entanglement. We define the sample complexity \u03c2 := \u03a3 + I + E as a result of the entanglement \u03a3, the intrinsic dimensionality I and the extrinsic dimensionality E and estimate via regression analysis which of these properties is statistically the most influential one for supervised classification with deep ReLU (Fukushima 1969; Fukushima and Miyake 1982; Glorot, Bordes, and Bengio 2011) networks. We show that the entanglement has a significantly larger effect on the sample complexity than the intrinsic dimensionality and, more importantly, observe an interdependence between these two factors. For low levels of entanglement increasing the intrinsic dimensionality results in equally low increases of the sample complexity while for high levels of entanglement increases in the intrinsic dimensionality lead to larger increases of the sample complexity. In other words, the effect of the intrinsic dimensionality on the sample complexity depends on the given distribution\u2019s level of entanglement. Thus, intrinsic dimensionality and entanglement cannot be considered independently when studying the sample complexity but have to be considered jointly.\nOur results do not contradict but complement the findings of Pope et al. (Pope et al. 2021). Pope et al.\u2019s investigation of the intrinsic dimensionality\u2019s impact on the sample complexity is limited to several complex datasets: ImageNet (Deng et al. 2009), CIFAR-10 (Krizhevsky, Hinton et al. 2009) and FONTS (Stutz, Hein, and Schiele 2019). Thus, their analysis only considers distributions with constant and high levels of entanglement. In our study, on the other hand, we regard the entanglement as another variable that influences the sample complexity and include it in our analysis as well.\nThe work is structured as follows. In Sections and we describe the notation and related work. Section introduces two simple measures for the entanglement. In Section we demonstrate the aforementioned results first for artificial datasets and then in Section for real-world image benchmarks."
        },
        {
            "heading": "Notation",
            "text": "Throughout this work we consider l \u2208 N+ samples x \u2208 R1\u00d7E arranged in the matrix X \u2208 Rl\u00d7E with labels y \u2208\n{0, 1}. We assume that those samples concentrate near manifolds M (y=0)samples \u2282 M (y=0) data and M (y=1) samples \u2282 M (y=1) data , where M (y=0) data and M (y=1) data support the entire data distribution p(xdata). A manifold M is a topological space that is locally homeomorphic to a Euclidean space of dimension I, so for every x \u2208 M there exits an open set U , x \u2208 U \u2282 M , that is homeomorphic to an open set V \u2282 RI with homeomorphism \u03d5x : U \u2192 V . As such, the intrinsic dimensionality can also be described as the dimensionality of the basis that spans the tangent spaces TxM at points x \u2208 M . The dimensionality I of the aforementioned Euclidean space is the intrinsic dimensionality of the manifold while E is the dimension of the manifold\u2019s ambient space, i.e. the extrinsic dimensionality. For natural images, for example, the extrinsic dimensionality is the number of pixels and colour channels, while the intrinsic dimensions denote the distribution\u2019s factors of variation, i.e. those changes that do not alter the semantics of a particular sample. These changes depend on the considered distribution and can for example include rigid transformations, changes in illumination or other changes in the appearance of the objects.\nThroughout this work we consider a binary classifier f : RE \u2192 R2+ which is either a support vector machine with a linear kernel (Boser, Guyon, and Vapnik 1992), a fullyconnected or a convolutional neural network. The neural networks have ReLU activations and are trained with the Adam optimizer (Kingma and Ba 2015)."
        },
        {
            "heading": "Related Work",
            "text": "In the setting of statistical learning theory (Vapnik 1992) the goal is to find a classifier f that minimizes the risk R(f) := Ex\u2208p(xdata)[L(y, f(x))], where L is a suitable loss function. The Bayes-classifier fBayes is defined as the classifier with the minimum possible risk which parametrizes the conditional distribution p(y|xdata). Since p(y|xdata) is generally unknown, the goal of learning is to find f that approximates fBayes. The sample complexity of a hypothesis class containing f is the number of train samples necessary to ensure a probably approximately correct (PAC) solution so a solution such that |R(f) \u2212 R(fBayes)| < \u03f5 with probability 1\u2212 \u03b4 for \u03f5, \u03b4 \u2208 R.\nNarayanan et al. (Narayanan and Niyogi 2009) studied the sample complexity of empirical risk minimization for binary classification from a theoretical point of view. They prove bounds on the sample complexity that depend on the curvature of the data manifold Mdata on which the data distribution pdata(x) is supported, the curvature of the decision boundary separating M (y=0)data and M (y=1) data and the intrinsic dimensionality of Mdata. Additionally, they show that the extrinsic dimensionality does not have an influence on the sample complexity. Recently, Pope et al. (Pope et al. 2021) confirmed some of these findings for deep classifiers by showing empirically that the sample complexity is well correlated with the intrinsic dimensionality of modern image benchmarks and almost independent of the extrinsic dimensionality. Ansuini et al. (Ansuini et al. 2019) on the other hand studied the intrinsic dimensionality of the data manifold as it is propagated through the network\u2019s layers. They find a characteris-\ntic increase followed by a progressive decrease of the intrinsic dimensionality and that the intrinsic dimensionality in the last layer is negatively correlated with the generalization error. Brahma et al. (Brahma, Wu, and She 2015) studied the ability of deep belief networks to disentangle and linearise manifolds. They showed that deep architectures progressively linearise and disentangle manifolds and that the presence of extrinsic dimensions that are not predictive of the label can hinder their ability to do so.\nZhang et al. (Zhang et al. 2016), show empirically that neural networks, despite having perfect sample expressivity, generalize well which complicates their analysis by tools from learning theory like the VC-dimension (Harvey, Liaw, and Mehrabian 2017). From a theoretical perspective the generalization capabilities have been studied by several authors (Bartlett 1998; Allen-Zhu, Li, and Liang 2019). Neyshabur et al. (Neyshabur, Bhojanapalli, and Srebro 2018) and Bartlett et al. (Bartlett, Foster, and Telgarsky 2017) provide bounds based on the spectral norms and Lipschitz constant of the networks. Golowich et al. (Golowich, Rakhlin, and Shamir 2018) bound the Rademacher complexity of networks independently of architectural parameters.\nOur Work Our work is orthogonal to the aforementioned works as we study the sample complexity not from a modelperspective but from a data-perspective. Since deep classifiers do not always behave like the predictions made by classical statistical learning theory (e.g., (Zhang et al. 2016; Nagarajan and Kolter 2019)) we are interested, whether classical bounds on the sample complexity of empirical risk minimization based on the distribution\u2019s geometry hold for deep classifiers. We are especially interested what influence the entanglement of class manifolds has on classifiers since this problem has not been independently studied despite its obvious importance for learning"
        },
        {
            "heading": "Entanglement of Class Manifolds",
            "text": ""
        },
        {
            "heading": "Entanglement Measures",
            "text": "The entanglement between two manifolds can be defined as the number of connected (E \u2212 1)-dimensional hyperplanes needed to perfectly separate the classes. In a twodimensional ambient space, for example, this corresponds to the number of connected line segments. If two classes are linearly separable, only a single hyperplane is required. Perfect separation is, by definition, given by the Bayes classifier fBayes. Thus, its decision boundary provides the measure of the entanglement between the two classes. Since fBayes is unknown, we approximate it with the classifier f . This approximation is a lower bound of the true entanglement between classes. Since the available samples X l\u00d7E are in reality only a small subset of the data distribution pdata(x), they might not be an accurate representation of the topology of the data distribution. If p(xdata) is not uniform over Mdata then, in the worst case, there could be two easily separable modes while the low-density regions are highly entangled. Then, our samples are dominated by the ones coming from the high-density regions and our estimation of the entanglement via investigation of the decision boundary fd of f will underestimate the true entanglement.\nKnowing the actual number of connected line segments necessary to separate the classes implies the availability of a perfect classifier. Thus, computing the absolute level of entanglement for real-world distributions is, from a learning perspective, just as difficult as finding this perfect classifier. In this study, however, we do not require the absolute values of entanglement but only the relative levels. In other words, an ordinal measure that allows to rank different distributions and their subsets according to their entanglement is sufficient for our study. We use the two methods described below.\nLinear Support Vector Classifier (LSVC) If f is a support vector classifier with a linear kernel, its accuracy can be used as a measure for the entanglement between two manifolds when compared for different distributions. The poorer the approximation of a decision boundary by a single hyperplane gets, the worse the LSVC\u2019s accuracy is if the classes have equal number of samples. Using the LSVC\u2019s accuracy this way, we can interpret it as a simple global measure for the entanglement of two manifolds.\nSpectrum of the Decision Function\u2019s Hessian For the second measure, we consider a neural network classifier f with decision function fd, where fd(x\u0304) = 0 for all x\u0304. Assuming a square approximation of the decision function, the second-order Taylor approximation of fd around x\u0304 yields\nTfd(x) = fd(x\u0304) + (x\u2212 x\u0304)TJfd(x\u0304)+ 1\n2! (x\u2212 x\u0304)THfd(x\u0304)(x\u2212 x\u0304)\n(1)\nwhere Jfd(x\u0304) is the Jacobian and Hfd(x\u0304) is the Hessian of fd evaluated at x\u0304. Determining the curvature of fd at x\u0304 where fd(x\u0304) = 0 comes down to investigating the spectrum of the Hessian Hfd(x\u0304). In contrast to the LSVC\u2019s accuracy this measure of entanglement is local. It quantifies how much the decision boundary around an x\u0304 differs from a linear one.\nTo compute those x\u0304 for which fd(x\u0304) = 0 we sample two points of different classes, x(y=0) and x(y=1), and solve\nx\u0304 = wx(y=0) + (1\u2212 w)x(y=1) (2)\nfor w \u2208 [0, 1]. This procedure ensures that all points sampled from the decision boundary are from within the convex hull of the data distribution and therefore separate the two supports, M (y=0)samples and M (y=1) samples, where they are closest."
        },
        {
            "heading": "Entanglement of Common Image Benchmarks",
            "text": "In this section we test the two entanglement measures introduced in the previous section on the real-world image benchmarks MNIST (LeCun et al. 1990), FASHION (Xiao, Rasul, and Vollgraf 2017), SVHN (Netzer et al. 2011) and CIFAR-10. It is common knowledge that these image benchmarks vary significantly in their entanglement. MNIST, for example, can be solved with high accuracy by a linear classifier while SVHN and CIFAR-10 cannot. In this section we measure the entanglement of the aforementioned datasets by choosing a representative binary classification problem consisting of two semantically similar classes. The intuition behind this is that those classes lie closer in pixel space (and\npossibly also in an arbitrary representation space) and so might also exhibit greater entanglement (in the presence of nuisance perturbations) than classes that are visually very different. Thus, choosing a representative pair of similar classes for each dataset provides an estimate of the entanglement for the entire dataset. For MNIST and SVHN the similar classes are the digits eight and nine, for FASHION the classes ankle boot and sneaker and for CIFAR-10 the classes cat and dog. To measure the entanglement we balance the classes for the LSVC to make comparisons between datasets possible. We randomly sample a certain fraction of the original binary datasets and compute the LSVC\u2019s accuracy on those smaller ones as well as on the complete dataset (Fraction = 1). In Figure 1 we display the results for the similar class pair. Unsurprisingly, we observe that the perceived difficulty of these image benchmarks is aligned with this entanglement measure. It is, however, noteworthy that we have to remove a significant fraction of samples of the complex benchmarks SVHN and CIFAR-10 before the LSVC\u2019s accuracy improves to levels of that for MNIST and FASHION. This means that a significant amount of samples lie near the decision boundary for those chosen classes.\nThe Hessian entanglement measure gives the same result. We train the neural network classifier f on the class pairs mentioned above and sample 500 points {x\u0304i}500i=1 on its decision boundary fd for which we compute the Hessian Hfd(x\u0304i). In Figure 2 we display the mean of the ordered singular values of those Hessians. We observe that more complex image datasets, like CIFAR-10 and SVHN, have a higher spectrum and therefore exhibit larger entanglement. Since these results confirm common knowledge and the global LSVC and the local Hessian measure give the same results, we provide only the LSVC\u2019s accuracy in our further study."
        },
        {
            "heading": "Intrinsic Dimensionality and Entanglement",
            "text": "When sorted increasingly according to their entanglement the previously used benchmarks exhibit the following order: MNIST < FASHION < SVHN < CIFAR-10 (see Figures 1 and 2). Pope et al. (Pope et al. 2021) report the same order when sorting these benchmarks according to their intrinsic dimensionality. Thus, image datasets with higher intrinsic dimensionality also exhibit higher entanglement.\nThis observation is noteworthy because in Sections and\nwe demonstrate through extensive experimentation on artificial and real-world datasets that the entanglement is the leading contributor to the sample complexity and that the effect of the intrinsic dimensionality depends on the given level of entanglement. Thus, we hypothesize that the reported complexity of these datasets might by primarily due to their entanglement and not their intrinsic dimensionality."
        },
        {
            "heading": "Sample Complexity of Artificial Datasets",
            "text": "In this section we investigate the effect that the entanglement, the intrinsic and the extrinsic dimensionality have on the sample complexity for datasets for which we can control all these parameters independently of each other."
        },
        {
            "heading": "Datasets",
            "text": "Archimedean Spiral Dataset The first artificial dataset consists of one-dimensional Archimedean spirals embedded in a two-dimensional ambient space. In Cartesian coordinates these spirals can be described as\nA(\u03a3Arch) = (\u03a3Arch cos \u03a3Arch,\u03a3Arch sin \u03a3Arch) (3)\nwhere \u03a3Arch \u2208 R\u22651 is the rotation angle (see Figure 3a for illustration).\nStep-function Dataset The second artificial dataset is directly described by its decision boundary which has the form of a step-function. It is defined as\nS(x) = \u2308x\u2309, x \u2208 [1,\u03a3Step] (4)\nwhere \u2308\u00b7\u2309 denotes the floor function and \u03a3Step \u2208 N+ is the maximum value of x (see Figure 3b for illustration).\nChanging the Entanglement When we consider two intertwined Archimedean spirals, each generated according to Equation 3 for a common \u03a3Arch, approximating the decision boundary requires increasingly more linear segments. Therefore, we use the rotation angle \u03a3Arch as a proxy for the entanglement of the two spirals. For the step-function the maximum value \u03a3Step of x describes the (2\u03a3Step \u2212 1) connected line segments that make up the decision boundary, therefore, \u03a3Step is the proxy for the entanglement.\nIncreasing the Intrinsic and Extrinsic Dimensionality The original Archimedean spiral dataset is a one-manifold embedded in a two-dimensional ambient space, so it has intrinsic dimensionality Iorg = 1 and extrinsic dimensionality Eorg = 2. The data separated by the step-functions is a twomanifold embedded in a two-dimensional ambient space as well, so Iorg = 2 and Eorg = 2. We scale all datasets so that they lie within the unit cube [0, 1]E .\nTo increase the intrinsic and extrinsic dimensionality of the spiral and the step-function dataset, the original data matrix X \u2208 Rl\u00d72 generated for some \u03a3Arch or \u03a3Step is augmented by a random matrix I \u2208 U l\u00d7Iadd[0,1] , with entries distributed according to a uniform distribution U over [0, 1], and a zero-matrix E \u2208 0l\u00d7Eadd . Iadd and Eadd are the additional intrinsic and extrinsic dimensions that are added to the base distribution. The augmented data matrix\nXa = [X|I|E] \u2208 Rl\u00d7(2+Iadd+Eadd) (5) is matrix-multiplied by a random orthogonal matrix\nO \u2208 R(2+Iadd+Eadd)\u00d7(2+Iadd+Eadd) (6) to remove the previously introduced zeros in the augmented columns. Then, we obtain the projected data matrix\nXp = XaO \u2208 Rl\u00d7(2+Iadd+Eadd) (7) with intrinsic dimensionality I = Iorg + Iadd, extrinsic dimensionality E = Eorg + Eadd and entanglement \u03a3Arch or \u03a3Step, respectively."
        },
        {
            "heading": "Results",
            "text": "Since the spiral and step-function datasets provide an easy way to change the entanglement, intrinsic and extrinsic dimensionality independently of each other, we can estimate the effect that those parameters have on the sample complexity \u03c2 . We measure the sample complexity as the number of samples from the train set needed to achieve a certain accuracy on the test set. In other words, we measure the number of samples needed so that the generalization error is below a certain threshold.\nArchimedean Spirals We train a fully-connected neural network on spiral datasets with independently changed \u03a3Arch \u2208 [1.0, 1.25, 1.5, 1.75, 2.0], I \u2208 [1, 2, ..., 11] and E \u2208 [2, 3, ..., 12] and measure the sample complexity \u03c2 . Then, we estimate the following three regression models,\n\u03c2 = \u03b1\u03a3Arch + \u03b2I + \u03b3E (8)\n\u03c2 =\u03b1\u03a3Arch + \u03b2I + \u03b3E+ \u03b4(\u03a3Arch \u00b7 I) + \u03f5(\u03a3Arch \u00b7 E) + \u03b6(I \u00b7 E)\n(9)\n\u03c2 =\u03b2I + \u03b3E + \u2211\n\u03c3\u2208\u03a3Arch\n\u03b1\u0302(\u03c3)[\u03a3 (\u03c3) Arch] + \u03b1 (\u03c3)(I \u00b7 [\u03a3(\u03c3)Arch])\n(10) where \u03b1\u0302(\u00b7), \u03b1(\u00b7), \u03b2, \u03b3, \u03b4, \u03f5, \u03b6 \u2208 R are the regression coefficients. [\u03a3(\u00b7)Arch] denotes dummy variables for different entanglement values. The dummy shows the level of entanglement when it is given or zero otherwise. The base case is\n[\u03a3 (1.0) Arch ] and it is therefore omitted from the regression equation. The first two regression models displayed in Equations 8 and 9 measure the effect of the entanglement, the intrinsic and extrinsic dimensionality independently of each other and with a potential interaction between them. The introduction of the dummy variables in Equation 10 allows us to estimate the intrinsic dimensionality\u2019s effect on the sample complexity given a certain level of entanglement. These regression models offer the best trade-off between interpretability and goodness-of-fit. Choosing higher-order polynomials to model the interactions between independent variables might result in a better fit; however, we would sacrifice model interpretability, as well as risk overfitting noise.\nThe results of these regressions are displayed in Table 1. We observe that in all three cases the entanglement is by a significant margin the most impactful factor on the sample complexity while the extrinsic dimensionality is not statistically relevant. In addition, we can state that the effect of the intrinsic dimension on the sample complexity depends on the distribution\u2019s entanglement. While for easily separable datasets (\u03a3Arch = [1, 1.25, 1.5]) increases in the intrinsic dimensionality do not influence the sample complexity significantly, we can observe that for highly entangled datasets (\u03a3Arch = [1.75, 2.0]) the sample complexity positively increases with an increase of the intrinsic dimensionality. In other words, the combination of intrinsic dimensionality and entanglement is empirically the most important one for the difficulty of the learning problem and when judging the sample complexity for a certain classification problem both of these parameters cannot be investigated independently but need to be considered in conjunction.\nStep-function Datasets For the step-function datasets we estimate the same regression models as for the Archimedean spirals, so Equations 8 and 9 but with \u03a3Step instead of \u03a3Arch. The regression in Equation 10 is estimated for the levels \u03a3Step = [1, 2, 3, 4, 5] where [\u03a3Step = 1] is the base case. Again, we train a fully-connected neural network and measure the sample complexity.\nIn Table 2 we display the findings and can observe that the results are aligned with the ones for the Archimedean spirals. Again, the entanglement is the significantly more important factor for the sample complexity. The previously made observation that the intrinsic dimension\u2019s influence depends on the given entanglement is similar for the stepfunction datasets. In Table 2 we can see that the intrinsic dimensionality positively influences the sample complexity for all levels of entanglement. However, this increase is larger for higher levels of entanglement, so the earlier made\nobservation of an interdependent effect of intrinsic dimensionality and entanglement on the sample complexity remains true. In contrast to the results of the Archimedean spiral datasets, we observe for Equation 10 in Table 2 a statistically significant negative effect of the extrinsic dimensionality on the sample complexity. This results is not theoretically\npredicted and since the other findings are aligned with the previous experimental results, we hypothesise that it might be due to the topology of the step-function dataset which is the union of disjoint linear subspaces. An investigation into what causes this effect is left for future work."
        },
        {
            "heading": "Summary",
            "text": "For both artificial datasets we observe that the regressions that take interactions between the entanglement and the intrinsic dimensionality into account fit the observed sample complexities significantly better than the regressions that assume their independence. These results show a statistically significant interaction between these two factors and demonstrate that the effect of the intrinsic dimensionality on the\nsample complexity is dependent on the given level of entanglement. For datasets that exhibit low levels of entanglement (so those that are (almost) linearly separable), increases in their intrinsic dimensionality have either no or small effects on the sample complexity relative to complex datasets where classes are highly entangled."
        },
        {
            "heading": "Sample complexity of Real-world Datasets",
            "text": "We now expand the analysis from the previous section to real-world image benchmarks."
        },
        {
            "heading": "Datasets",
            "text": "We use the binary classification problems introduced in Section again. Since FASHION is (almost) linearly separable even for large sample sizes, we defer its analysis to the extended on-line version where we show that increases in its intrinsic dimensionality do not appear to cause an increase in the sample complexity. In this Section we only present the results for SVHN and CIFAR-10.\nChanging the Entanglement As discussed in Section the number of samples drawn from the data distribution p(xdata) can influence the estimation of the entanglement when the density is not uniform over the data manifold Mdata. Therefore, estimation of the entanglement via a well-trained classifier f only gives a lower bound on it. As a result, without access to p(xdata) from which we could sample, we cannot increase but only decrease the entanglement of a given distribution. To decrease the entanglement between two manifolds, the class-boundary points, those samples close to the decision boundary, need to be removed. One way to identify these points is by computing the magnitude of a neural network\u2019s gradient gi = ||\u2202Lf\u2202xi ||F for all train samples xi \u2208 X l\u00d7E , where Lf is the network\u2019s loss function and || \u00b7 ||F denotes the Frobenius-norm. Then, gi can be used as an estimate of the proximity of point xi to the boundary. We remove those points with values gi above the \u03a3Realpercentile of all gradient norms and replace them with random samples from the class interior, perturbed by Gaussian noise. For \u03a3Real = 0.4 for example, those 60% of points which have the highest gradient norms gi are removed. \u03a3Real = 1 is the original set. We test this heuristic and can confirm that this approach indeed reduces the entanglement up to some negligible stochastic effects (Figure 4). We note that a significant number of samples need to be removed to observe a meaningful reduction in the entanglement. This is in line with the findings described in Section in which we show that the samples of complex image benchmarks appear to concentrate near the decision boundary.\nIncreasing the Intrinsic Dimensionality To increase the intrinsic dimensionality of the image datasets we use a similar procedure as in Section . We add uniform noise over [0, 1]Iadd to all available samples. This procedure has been shown to increase the intrinsic dimensionality of real-world image datasets (Pope et al. 2021)."
        },
        {
            "heading": "Results",
            "text": "We train a convolutional neural network with batchnormalization (Ioffe and Szegedy 2015) on the binary classification tasks for \u03a3Real \u2208 [0.1, 0.5, 1.0] and Iadd \u2208 [0, 5, 10, 15, 30, 60, 90, 120, 150] where Iadd = 0 is the dataset with the original intrinsic dimensionality. Pope et al. (Pope et al. 2021) report an original intrinsic dimensionality for SVHN between 9 and 19 and for CIFAR-10 between 13 and 25, depending on the method. Thus, the ratios between intrinsic and extrinsic dimensionality for the artificial and the real-world datasets are comparable in our work.\nWe estimate the same regressions as for the artificial datasets (Equations 8, 9 and 10) with the exceptions that we have omitted changing the extrinsic dimensionality as we have have not found it to be statistically significant and that we normalize the sample complexity by dividing it by the number of available samples; \u03c2norm = \u03c2l to make comparisons between different class pairs and datasets possible.\nThe results are displayed in Tables 3 and 4. As for the artificial datasets the entanglement is in general the most significant factor for the (normalized) sample complexity. Importantly, we note again a dependence of the intrinsic dimensionality\u2019s impact on the level of entanglement. When enough samples from the class boundaries of both classification tasks are removed, increasing the intrinsic dimensionality does not have a statistically significant effect on the sample complexity any more."
        },
        {
            "heading": "Conclusions",
            "text": "The sample complexity of empirical risk minimization has been studied theoretically and recent empirical work has confirmed that effect of the intrinsic dimensionality on the sample complexity of deep classifiers. In addition, theoretical bounds on the sample complexity of deep classifiers have been proposed (see Section ). In this work we take an orthogonal approach to the model-dependent bounds on the sample complexity and provide an extension for the data-dependent study. This is achieved by investigating the effect of the entanglement of class manifolds on the sample complexity. We show for deep ReLU networks that the entanglement is the most important factor for the difficulty of a learning problem and that it has an interdependent effect with the intrinsic dimensionality. Fully-connected and convolutional classifiers exhibit much stronger increases of their sample complexity\nfor higher levels of entanglement, while for low levels the intrinsic dimensionality\u2019s effect is smaller."
        }
    ],
    "title": "The Effect of Manifold Entanglement and Intrinsic Dimensionality on Learning",
    "year": 2022
}