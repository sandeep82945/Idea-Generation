{
    "abstractText": "Knowledge distillation has proven to be an effective technique in improving the performance a student model using predictions from a teacher model. However, recent work has shown that gains in average efficiency are not uniform across subgroups in the data, and in particular can often come at the cost of accuracy on rare subgroups and classes. To preserve strong performance across classes that may follow a long-tailed distribution, we develop distillation techniques that are tailored to improve the student\u2019s worst-class performance. Specifically, we introduce robust optimization objectives in different combinations for the teacher and student, and further allow for training with any tradeoff between the overall accuracy and the robust worst-class objective. We show empirically that our robust distillation techniques not only achieve better worst-class performance, but also lead to Pareto improvement in the tradeoff between overall performance and worstclass performance compared to other baseline methods. Theoretically, we provide insights into what makes a good teacher when the goal is to train a robust student.",
    "authors": [
        {
            "affiliations": [],
            "name": "Serena Wang"
        }
    ],
    "id": "SP:15b89b06ba3c26d8cfefa6c80467de4a482b1d96",
    "references": [
        {
            "authors": [
                "Alekh Agarwal",
                "Alina Beygelzimer",
                "Miroslav Dud\u00edk",
                "John Langford",
                "Hanna Wallach"
            ],
            "title": "A reductions approach to fair classification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Rohan Anil",
                "Gabriel Pereyra",
                "Alexandre Tachard Passos",
                "Robert Ormandi",
                "George Dahl",
                "Geoffrey Hinton"
            ],
            "title": "Large scale distributed neural network training through online distillation",
            "year": 2018
        },
        {
            "authors": [
                "Kaidi Cao",
                "Colin Wei",
                "Adrien Gaidon",
                "Nikos Arechiga",
                "Tengyu Ma"
            ],
            "title": "Learning imbalanced datasets with label-distribution-aware margin loss",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Robert S Chen",
                "Brendan Lucier",
                "Yaron Singer",
                "Vasilis Syrgkanis"
            ],
            "title": "Robust optimization for non-convex objectives",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Cotter",
                "Heinrich Jiang",
                "Serena Wang",
                "Taman Narayan",
                "Seungil You",
                "Karthik Sridharan",
                "Maya R. Gupta"
            ],
            "title": "Optimization with non-differentiable constraints with applications to fairness, recall, churn, and other goals",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2019
        },
        {
            "authors": [
                "Yin Cui",
                "Menglin Jia",
                "Tsung-Yi Lin",
                "Yang Song",
                "Serge Belongie"
            ],
            "title": "Class-balanced loss based on effective number of samples",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel D\u2019souza",
                "Zach Nussbaum",
                "Chirag Agarwal",
                "Sara Hooker"
            ],
            "title": "A tale of two long tails",
            "venue": "arXiv preprint arXiv:2107.13098,",
            "year": 2021
        },
        {
            "authors": [
                "Mengnan Du",
                "Subhabrata Mukherjee",
                "Yu Cheng",
                "Milad Shokouhi",
                "Xia Hu",
                "Ahmed Hassan Awadallah"
            ],
            "title": "What do compressed large language models forget? robustness challenges in model compression, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Vitaly Feldman"
            ],
            "title": "Does learning require memorization? a short tale about a long tail, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Tommaso Furlanello",
                "Zachary Chase Lipton",
                "Michael Tschannen",
                "Laurent Itti",
                "Anima Anandkumar"
            ],
            "title": "Born again neural networks",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nathan Srebro"
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "In NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "G. Hinton",
                "O. Vinyals",
                "J. Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "ArXiv, abs/1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Sara Hooker",
                "Nyalleng Moorosi",
                "Gregory Clark",
                "Samy Bengio",
                "Emily Denton"
            ],
            "title": "Characterising bias in compressed models",
            "venue": "arXiv preprint arXiv:2010.03058,",
            "year": 2020
        },
        {
            "authors": [
                "Ganesh Ramachandra Kini",
                "Orestis Paraskevas",
                "Samet Oymak",
                "Christos Thrampoulidis"
            ],
            "title": "Label-imbalanced and group-sensitive classification under overparameterization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report, University of Toronto,",
            "year": 2009
        },
        {
            "authors": [
                "Ya Le",
                "Xuan Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "CS 231N,",
            "year": 2015
        },
        {
            "authors": [
                "Weiyang Liu",
                "Yandong Wen",
                "Zhiding Yu",
                "Ming Li",
                "Bhiksha Raj",
                "Le Song"
            ],
            "title": "Sphereface: Deep hypersphere embedding for face recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Michal Lukasik",
                "Srinadh Bhojanapalli",
                "Aditya Krishna Menon",
                "Sanjiv Kumar"
            ],
            "title": "Teacher\u2019s pet: understanding and mitigating biases in distillation",
            "venue": "arXiv preprint arXiv:2106.10494,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya K Menon",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Seungyeon Kim",
                "Sanjiv Kumar"
            ],
            "title": "A statistical perspective on distillation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Krishna Menon",
                "Ankit Singh Rawat",
                "Sanjiv Kumar"
            ],
            "title": "Overparameterisation and worstcase generalisation: friend or foe",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Krishna Menon",
                "Sadeep Jayasumana",
                "Ankit Singh Rawat",
                "Himanshu Jain",
                "Andreas Veit",
                "Sanjiv Kumar"
            ],
            "title": "Long-tail learning via logit adjustment",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Harikrishna Narasimhan",
                "Aditya K Menon"
            ],
            "title": "Training over-parameterized models with non-decomposable objectives",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hieu Pham",
                "Zihang Dai",
                "Qizhe Xie",
                "Quoc V Le"
            ],
            "title": "Meta pseudo labels",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Vihari Piratla",
                "Praneeth Netrapalli",
                "Sunita Sarawagi"
            ],
            "title": "Focus on the common good: Group distributional robustness follows",
            "venue": "arXiv preprint arXiv:2110.02619,",
            "year": 2021
        },
        {
            "authors": [
                "I. Radosavovic",
                "P. Doll\u00e1r",
                "R. Girshick",
                "G. Gkioxari",
                "K. He"
            ],
            "title": "Data distillation: Towards omni-supervised learning",
            "year": 2018
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein",
                "Alexander C. Berg",
                "Li Fei-Fei"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2015
        },
        {
            "authors": [
                "Andrei A Rusu",
                "Sergio Gomez Colmenarejo",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Guillaume Desjardins",
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "Raia Hadsell"
            ],
            "title": "Policy distillation",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Tatsunori B. Hashimoto",
                "Percy Liang"
            ],
            "title": "Distributionally robust neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Aditi Raghunathan",
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "An investigation of why overparameterization exacerbates spurious correlations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Shai Shalev-Shwartz"
            ],
            "title": "Online learning and online convex optimization",
            "venue": "Foundations and trends in Machine Learning,",
            "year": 2011
        },
        {
            "authors": [
                "Maurice Sion"
            ],
            "title": "On general minimax theorems",
            "venue": "Pacific Journal of mathematics,",
            "year": 1958
        },
        {
            "authors": [
                "Nimit Sohoni",
                "Jared Dunnmon",
                "Geoffrey Angus",
                "Albert Gu",
                "Christopher R\u00e9"
            ],
            "title": "No subclass left behind: Fine-grained robustness in coarse-grained classification problems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Grant Van Horn",
                "Pietro Perona"
            ],
            "title": "The devil is in the tails: Fine-grained classification in the wild",
            "venue": "arXiv preprint arXiv:1709.01450,",
            "year": 2017
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou"
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Robert C Williamson",
                "Elodie Vernet",
                "Mark D Reid"
            ],
            "title": "Composite multiclass losses",
            "venue": "Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Canwen Xu",
                "Wangchunshu Zhou",
                "Tao Ge",
                "Ke Xu",
                "Julian McAuley",
                "Furu Wei"
            ],
            "title": "Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Ziyu Xu",
                "Chen Dan",
                "Justin Khim",
                "Pradeep Ravikumar"
            ],
            "title": "Class-weighted classification: Trade-offs and robust approaches",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chenglin Yang",
                "Lingxi Xie",
                "Siyuan Qiao",
                "Alan L Yuille"
            ],
            "title": "Training deep neural networks in generations: A more tolerant teacher educates better students",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge distillation, wherein one trains a teacher model and uses its predictions to train a student model of similar or smaller capacity, has proven to be a powerful tool that improves efficiency while achieving state-of-the-art classification accuracies on a variety of problems [13, 27, 25, 2, 36]. While originally devised for model compression, distillation has also been widely applied to improve the performance of fixed capacity models and in semi-supervised learning settings [29, 10, 41, 38].\nEvaluating the trade-offs incurred by distillation techniques has largely focused on canonical measures such as average error over a given data distribution. Along this dimension, distillation has proven to be a remarkably effective technique, with the student achieving even better test performance than the teacher (e.g. Xie et al. [38]). However, average error may differ significantly from the performance on individual subpopulations in the data, and thus may be an inadequate metric to optimize in real-world settings that also require good performance on different subgroups. For instance, subgroups may be defined by attributes such as country, language, or racial attributes [11, 42, 1], in which case performance on the individual subgroups becomes a policy and fairness concern.\nThe potential for mismatch between average error and worst-group error is further exacerbated by the fact that many real-world datasets exhibit imbalance in the number of training examples belonging to different subgroups. In a multi-class classification setting where each target label delineates a\nPreprint. Under review.\nar X\niv :2\n20 6.\n06 47\n9v 1\n[ cs\n.L G\n] 1\nsubgroup, many real-world datasets exhibit a long-tailed label distribution [35, 23, 9, 7]. Recent work has shown that improved average error of the student model often comes at the expense of poorer performance on the tail classes [20, 8] or under-represented groups [31, 22], and model compression can amplify performance disparities across groups [15, 39]. Further hurting test performance is the possibility that the training data may have particularly poor representation of important subgroups that occur more frequently at deployment or test time.\nTo mitigate the disparity between average error and subgroup performance, a common approach is to train a model to achieve low worst-group test error. Modified objectives for robust optimization techniques have successfully achieved state-of-the-art worst-class performance with manageable computational overhead [30, 34, 24]. However, an evaluation of these techniques has disproportionately focused on a standard single model training setting. In this work, we focus on applying robust optimization to a multi-step training setting typical of distillation. We take a wider view of the optimization process, and ask what combination of student and teacher objectives achieves low worst-group test error and minimizes difference between average and worst case error.\nTo integrate robust optimization objectives into the distillation procedure, we introduce a robust objective for either the teacher, the student, or both. We also quantify the trade-off between average error across all subgroups and the worst-group error by mapping out Pareto fronts for each distillation procedure. Theoretically, we ask the question, what makes for a \u201cgood\u201d teacher when training a robust student? That is, what properties are important in a teacher so that a student achieves good worst-class performance?\nIn summary, we make the following contributions:\n(i) We present distillation techniques integrating distributionally robust optimization (DRO) algorithms with the goal of achieving a robust student, and explore different combinations of modifications to the teacher and student objectives.\n(ii) In both self-distillation and compression settings, we empirically demonstrate that the proposed robust distillation algorithms can train student models that yield better worst-class accuracies than the teacher and other recent baselines. We also demonstrate gains in the trade-off between overall and worst-class performance.\n(iii) We derive robustness guarantees for the student under different algorithmic design choices, and provide insights into when the student yields better worst-class robustness than the teacher.\nRelated Work. Fairness in distillation: Lukasik et al. [20] study how distillation impacts worst-group performance and observe that the errors that the teacher makes on smaller subgroups get amplified and transferred to the student. They also propose simple modifications to the student\u2019s objective to control the strength of the teacher\u2019s labels for different groups. In contrast, in this work we propose a more direct and theoretically-grounded procedure that seeks to explicitly optimize for the student\u2019s worst-case error, and explore modifications to both the teacher and student objectives.\nWorst-group robustness: The goal of achieving good worst-case performance across subgroups can be framed as a (group) distributionally robust optimization (DRO) problem, and can be solved by iteratively updating costs on the individual groups and minimizing the resulting cost-weighted loss [4]. Recent variants of this approach have sought to avoid over-fitting through group-specific regularization [30] or margin-based losses [24, 16], to handle unknown subgroups [34], and to balance between average and worst-case performance [26]. Other approaches include the use of Conditional Value at Risk for worst-class robustness [40].\nAmong these, the work that most closely relates to our paper is the margin-based DRO algorithm [24], which includes preliminary distillation experiments in support of training the teacher with standard ERM and the student with a robust objective. However, this and other prior work have only explored modifications to the student loss [20, 24], while training the teacher using a standard procedure. Our robust distillation proposals build on this method, but carry out a more extensive analysis, exploring different combinations of teacher-student objectives and different trade-offs between average and worst-class performance. Additionally, we provide robustness guarantees for the student, equip the DRO algorithms to achieve different trade-offs between overall and worst-case error, and provide a rigorous analysis of different design choices."
        },
        {
            "heading": "2 Problem Setup",
            "text": "We are interested in a multi-class classification problem with instance space X and output space [m] = {1, . . . ,m}. Let D denote the underlying data distribution over X \u00d7 [m], and DX denote the marginal distribution over X . Let \u2206m denote the (m\u2212 1)-dimensional probability simplex over m classes. We define the conditional-class probability as \u03b7y(x) = P(Y = y|X = x) and the class priors \u03c0y = P(Y = y). Note that \u03c0y = EX\u223cDX [\u03b7y(X)]. Learning objectives. Our goal is to learn a multiclass classifier h : X \u2192 [m] that maps an instance x \u2208 X to one of m classes. We will do so by first learning a scoring function f : X \u2192 Rm that assigns scores [f1(x), . . . , fm(x)] \u2208 Rm to a given instance x, and construct the classifier by predicting the class with the highest score: h(x) = argmaxj\u2208[m] fj(x). We will denote a softmax transformation of f by softmaxy(f(x)) = exp(fy(x))\u2211 j exp(fj(x))\n, and use the notation softmaxy(f(x)) \u221d zy to indicate that softmaxy(f(x)) =\nzy\u2211m j=1 zj .\nWe measure the efficacy of the scoring function f using a loss function ` : [m] \u00d7 Rm \u2192 R+ that assigns a penalty `(y, z) for predicting score vector z \u2208 Rm for true label y. Examples of loss functions include the 0-1 loss: `0-1(y, z) = 1 ( z 6= argmaxj fj(x) ) , and the softmax cross-entropy\nloss: `xent(y, z) = \u2212fy(x) + log (\u2211 j\u2208[m] exp (fj(x)) ) .\nStandard objective: A standard machine learning goal entails minimizing the overall expected risk:\nLstd(f) = E [`(Y, f(X))] . (1)\nBalanced objective: In applications where the classes are severely imbalanced, i.e., the class priors \u03c0y are non-uniform and significantly skewed, one may wish to instead optimize a balanced version of the above objective, where we average over the conditional loss for each class. Notice that the conditional loss for class y is weighted by the inverse of its prior:\nLbal(f) = 1\nm \u2211 y\u2208[m] E [`(y, f(X)) |Y = y] = 1 m \u2211 y\u2208[m] 1 \u03c0y EX [\u03b7y(X) `(y, f(X))] . (2)\nRobust objective: A more stringent objective would be to focus on the worst-performing class, and minimize a robust version of (1) that computes the worst among the m conditional losses:\nLrob(f) = max y\u2208[m]\n1\n\u03c0y E [\u03b7y(X) `(y, f(X))] . (3)\nIn practice, focusing solely on either the average or the worst-case performance may not be an acceptable solution, and therefore, in this paper, we will additionally seek to characterize the trade-off between the balanced and robust objectives. One way to achieve this trade-off is to minimize the robust objective, while constraining the balanced objective to be within an acceptable range. This constrained optimization can be equivalently formulated as optimizing a convex combination of the balanced and robust objectives, for trade-off \u03b1 \u2208 [0, 1]:\nLtdf(f) = (1\u2212 \u03b1)Lbal(f) + \u03b1Lrob(f). (4)\nA similar trade-off can also be specified between the standard and robust objectives. To better understand the differences between the standard, balanced and robust objectives in (1)\u2013(4), we look at the optimal scoring function for each given a cross-entropy loss:\nTheorem 1 (Bayes-optimal scorers). When ` is the cross-entropy loss `xent, the minimizers of (1)\u2013(3) over all measurable functions f : X \u2192 Rm are given by: (i) Lstd(f): softmaxy(f\u2217(x)) = \u03b7y(x) (ii) Lbal(f): softmaxy(f\u2217(x)) \u221d 1\u03c0y \u03b7y(x)\n(iii) Lrob(f): softmaxy(f\u2217(x)) \u221d \u03bby\u03c0y \u03b7y(x) (iv) L tdf(f): softmaxy(f\u2217(x)) \u221d\n(1\u2212\u03b1) 1m +\u03b1\u03bb \u2032 y\n\u03c0y \u03b7y(x),\nfor class-specific constants \u03bb, \u03bb\u2032 \u2208 Rm+ that depend on distribution D. All proofs are provided in Appendix A. Interestingly, the optimal scorers for all four objectives involve a simple scaling of the conditional-class probabilities \u03b7y(x)."
        },
        {
            "heading": "3 Distillation for Worst-class Performance",
            "text": "We adopt the common practice of training both the teacher and student on the same dataset. Specifically, given a training sample S = {(x1, y1), . . . , (xn, yn)} drawn from D, we first train a teacher model pt : X \u2192 \u2206m, and use it to generate a student dataset S\u2032 = {(x1, pt(x1)), . . . , (xn, pt(xn))} by replacing the original labels with the teacher\u2019s predictions. We then train a student scorer fs : X \u2192 [m] using the re-labeled dataset, and use it to construct the final classifier. In a typical setting, both the teacher and student are trained to optimize a version of the standard objective in (1), i.e., the teacher is trained to minimize the average loss against the original training labels, and the student is trained to minimize an average loss against the teacher\u2019s predictions:\nTeacher: L\u0302std(f t) = 1\nn n\u2211 i=1 ` ( yi, f t(xi) ) ; Student: L\u0302std-d(fs) = 1 n n\u2211 i=1 m\u2211 y=1 pty(xi) ` (y, f(xi)) ,\n(5) where pt(x) = softmax(f t(x)). It is also common to have the student use a mixture of the teacher and one-hot labels. For concreteness, we consider a simpler distillation setup without this mixture, though extensions with this mixture would be straightforward to add. This work takes a wider view and explores what combinations of student and teacher objectives facilitate better worst-group performance for the student. Our experiments evaluate all nine combinations of standard, balanced, and robust teacher objectives, paired with standard, balanced, and robust student objectives.\nGiven the choice of teacher objective, the student will either optimize a distilled version of the balanced objective in (2):\nL\u0302bal-d(fs) = 1\nm \u2211 y\u2208[m] 1 \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) ` (y, f s(xi)) , (6)\nor a distilled version of the robust objective in (3):\nL\u0302rob-d(fs) = max y\u2208[m]\n1\n\u03c0\u0302ty\n1\nn n\u2211 i=1 pty(xi) ` (y, f s(xi)) , (7)\nIn practice, the teacher\u2019s predictions may have a different marginal distribution from the underlying class priors, particularly when temperature scaling is applied to the teacher\u2019s logits to soften the predicted probabilities [24]. To address this, in both (6) and (7) we have replaced the class priors \u03c0y with the marginal distribution \u03c0\u0302ty = 1 n \u2211n i=1 p t y(xi) from the teacher\u2019s predictions.\nIn addition to exploring the combination of objectives that facilitates better worst-group performance for the student, we evaluate a more flexible approach \u2013 have both the teachers and the students trade-off between the balanced and robust objectives:\nTeacher: L\u0302tdf(f t) = (1\u2212 \u03b1t)L\u0302bal(f t) + \u03b1tL\u0302rob(f t) (8) Student: L\u0302tdf-d(fs) = (1\u2212 \u03b1s)L\u0302bal-d(fs) + \u03b1sL\u0302rob-d(fs),\nwhere L\u0302bal(f t) and L\u0302rob(f t) are the respective empirical estimates of (2) and (3) from the training sample, and \u03b1t, \u03b1s \u2208 [0, 1] are the respective tradeoff parameters for the teacher and student. We are thus able to evaluate the Pareto-frontier of balanced and worst-case accuracies, obtained from different combinations of the teachers and students, and trained with different trade-off parameters."
        },
        {
            "heading": "4 Robust Distillation Algorithms",
            "text": "The different objectives we consider \u2013 standard, balanced and robust \u2013 entail different loss objectives to ensure efficient optimization during training. For example, while training the standard teacher and student in (5), we take ` to be the softmax cross-entropy loss, and optimize it using SGD. For the balanced and robust models, we employ the margin-based surrogates that we detail below, which have shown to be more effective in training over-parameterized networks [3, 23, 16]. Across all objectives, at evaluation we take the loss ` in the student and teacher objectives to be the 0-1 loss.\nMargin-based surrogate for balanced objective. When the teacher or student model being trained is over-parameterized, i.e., has sufficient capacity to correctly classify all examples in the training set,\nAlgorithm 1 Distilled Margin-based DRO Inputs: Teacher pt, Student hypothesis class F , Training set S, Validation set Sval, Step-size \u03b3 \u2208 R+, Number of iterations K, Loss `, Initial student f0 \u2208 F , Initial multipliers \u03bb0 \u2208 \u2206m Compute \u03c0\u0302tj = 1 n \u2211 (x,y)\u2208S p t j(x), \u2200j \u2208 [m]\nCompute \u03c0\u0302t,valj = 1 nval \u2211 (x,y)\u2208Sval p t j(x), \u2200j \u2208 [m] For k = 0 to K \u2212 1 \u03bb\u0303k+1j = \u03bb k j exp ( \u03b3R\u0302j ) ,\u2200j \u2208 [m] where R\u0302j = 1nval 1\n\u03c0\u0302t,valj \u2211 (x,y)\u2208Sval ptj(x) `(j, f k(x))\n\u03bbk+1y = \u03bb\u0303k+1y\u2211m\nj=1 \u03bb\u0303 k+1 j\n,\u2200y\nfk+1 \u2208 argmin f\u2208F 1 n\n\u2211n i=1 Lmar ( pt(xi), f(xi); \u03bbk+1 \u03c0\u0302t ) // Replaced with a few steps of SGD\nEnd For Output: f\u0304s : x 7\u2192 1K \u2211K k=1 f k(x)\nthe use of an outer weighting term in the objective (such as the inverse class marginals in (6)) can be ineffective. In other words, a model that yields zero training objective would do so irrespective of what outer weights we choose. To remedy this problem, we make use of the margin-based surrogate of Menon et al. [23], and incorporate the outer weights as margin terms within the loss. For the balanced student objective in (6), this would look like:\nL\u0303bal-d(fs) = 1\nn n\u2211 i=1 Lmar ( pt(xi), f s(xi);1/\u03c0\u0302 t ) , (9)\nwhere Lmar (p, f ; c) = 1 m \u2211 y\u2208[m] py log ( 1 + \u2211 j 6=y exp (log(cy/cj) \u2212 (fy \u2212 fj)) ) ,\nfor teacher probabilities p \u2208 \u2206m, student scores f \u2208 Rm, and per-class costs c \u2208 Rm+ . For the balanced teacher, the margin-based objective would take a similar form, but with one-hot labels.\nWe include a proof in Appendix A.3 showing that a scoring function that minimizes this surrogate objective also minimizes the the balanced objective in (6) (when ` is the cross-entropy loss, and the student is chosen from a sufficiently flexible function class). In practice, the margin term log(cy/cj) encourages a larger margin of separation for classes y for which the cost cy is relatively higher.\nMargin-based DRO for robust objective. Minimizing the robust objective with plain SGD can be difficult due to the presence of the outer \u201cmax\u201d over m classes. The key difficulty is in computing reliable stochastic gradients for the max objective, especially given a small batch size. The standard approach is to instead use a (group) distributionally-robust optimization (DRO) procedure, which comes in multiple flavors [4, 30, 16]. We employ the margin-based variant of group DRO [24] as it naturally extends the margin-based objective used in the balanced setting.\nWe illustrate below how this applies to the robust student objective in (7). The procedure for the robust teacher is similar, but involves one-hot labels. For a student hypothesis class F , we first re-write the minimization in (7) over f \u2208 F into an equivalent min-max optimization using per-class multipliers \u03bb \u2208 \u2206m:\nmin f\u2208F max \u03bb\u2208\u2206m \u2211 y\u2208[m] \u03bby \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) ` (y, f(xi)) ,\nand then maximize over \u03bb for fixed f , and minimize over f for fixed \u03bb: \u03bbk+1y \u221d \u03bbky exp ( \u03b3 1\nn\u03c0\u0302ty n\u2211 i=1 pty(xi) ` ( y, fk(xi) )) ,\u2200y\nfk+1 \u2208 argmin f\u2208F \u2211 y\u2208[m] \u03bbk+1y n\u03c0\u0302ty n\u2211 i=1 pty(xi) ` (y, f(xi)) ,\nwhere \u03b3 > 0 is a step-size parameter. The updates on \u03bb implement exponentiated gradient (EG) ascent to maximize over the simplex [32].\nFollowing Narasimhan and Menon [24], we make two modifications to the above updates when used to train over-parameterized networks that can fit the training set perfectly. First, we perform the updates on \u03bb using a small held-out validation set Sval = {(x1, y1), . . . , (xnval , ynval)}, instead of the training set, so that the \u03bbs reflect how well the model generalizes out-of-sample. Second, in keeping with the balanced objective, we modify the weighted objective in the f -minimization step to include a margin-based surrogate. Algorithm 1 provides a summary of these steps and returns a scorer that averages over the K iterates: f\u0304s(x) = 1K \u2211K k=1 f\nk(x). While the averaging is needed for our theoretical analysis, in practice, we find it sufficient to return the last scorer fK . In Appendix D, we describe how Algorithm 3 can be easily modified to trade-off between the balanced and robust objectives, as shown in (8).\nTo distill the validation set or not? The updates on \u03bb in Algorithm 1 use a validation set labeled by the teacher. One could instead perform these updates with a curated validation set containing the original one-hot labels. Each of these choices presents different merits. The use of a teacher-labeled validation set is useful in many real world scenarios where labeled data is hard to obtain, while unlabeled data abounds. In contrast, the use of one-hot validation labels, although more expensive to obtain, may make the student more immune to errors in the teacher\u2019s predictions, as the coefficients \u03bbs are now based on an unbiased estimate of the student\u2019s performance on each class. With a one-hot validation set, we update \u03bbs as follows:\n\u03bbk+1j = \u03bb k j exp ( \u03b7R\u0302j ) ,\u2200j \u2208 [m], where R\u0302j = 1\nnval 1\n\u03c0\u0302j \u2211 (x,y)\u2208Sval 1(y = j) `(j, fk(x)), (10)\nfor estimates \u03c0\u0302y \u2248 \u03c0y of the original class priors. We analyze both the variants in our experiments, and in the next section, discuss robustness guarantees for each."
        },
        {
            "heading": "5 Theoretical Analysis",
            "text": "To simplify our exposition, we will present our theoretical analysis for a student trained using Algorithm 1 to yield good worst-class performance. Our results easily extend to the case where the student seeks to trade-off between overall and worst-case performance.\nWhat constitutes a good teacher? We would first like to understand what makes a good teacher when the student\u2019s goal is to minimize the robust population objectiveLrob(fs) in (3). We also analyze whether the student\u2019s ability to perform well on this worst-case objective depends on the teacher also performing well on the same objective. As a proxy for Lrob(fs), the student minimizes the distilled objective L\u0302rob-d(fs) in (7) with predictions from teacher pt. We argue that an ideal teacher in this case would be one that ensures that the difference between the two objectives |L\u0302rob-d(fs)\u2212Lrob(fs)| is as small as possible. Below, we provide a simple bound on this difference:\nTheorem 2. Suppose `(y, z) \u2264 B, \u2200x \u2208 X for some B > 0. Let \u03c0ty = Ex [ pty(x) ] , and let the following denote the per-class expected and empirical student losses respectively:\n\u03c6y(f s) = 1\u03c0ty\nEx [ pty(x) ` (y, f s(x)) ]\n; \u03c6\u0302y(f s) = 1\u03c0\u0302ty 1 n \u2211n i=1 p t y(xi) ` (y, f s(xi)) .\nThen for teacher pt and student fs:\n|L\u0302rob-d(fs)\u2212 Lrob(fs)| \u2264 B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223]\ufe38 \ufe37\ufe37 \ufe38 Approximation error + max y\u2208[m] \u2223\u2223\u03c6y(fs)\u2212 \u03c6\u0302y(fs)\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38 Estimation error .\nThe approximation error captures how well the teacher\u2019s predictions mimic the conditional-class distribution \u03b7(x) \u2208 \u2206m, up to per-class normalizations. This suggests that even if pt does not achieve good worst-class performance, as long as it is well calibrated within each class (as measured by the approximation error above), it will serve as a good teacher. Indeed when the teacher outputs the conditional-class probabilities, i.e. pt(x) = \u03b7(x), the approximation error is trivially zero (recall that the normalization term \u03c0ty = \u03c0y in this case). We know from Theorem 1 that, this would be the case with a teacher trained to optimize the standard cross-entropy objective (provided we use an unrestricted model class). In practice, however, we do not expect the teacher to approximate \u03b7(x)\nvery well, and this opens the door for training the teacher with the other objectives described in Section 2, each of which encourage the teacher to approximate a scaled (normalized) version of \u03b7(x).\nThe estimation error captures how well the teacher aids in the student\u2019s out-of-sample generalization. The prior work by Menon et al. [21] studies this question in detail for the standard student objective, and provide a bound that depends on the variance induced by the teacher\u2019s predictions on the student\u2019s objective: the lower the variance, the better the student\u2019s generalization. A similar analysis can be carried out with the per-class loss terms in Theorem 2 (more details in Appendix B).\nRobustness guarantee for the student. We next seek to understand if the student can match or outperform the teacher\u2019s worst-class performance. For a fixed teacher pt, we consider a self-distillation setup where the student is chosen from the same function class F as the teacher, and can thus exactly mimic the teacher\u2019s predictions. Under this setup, we provide robustness gurarantees for the student output by Algorithm 1 in terms of the approximation and estimation errors described above. Proposition 3. Suppose pt \u2208 F and F is closed under linear transformations. Let \u03bb\u0304y = ( \u220fK k=1 \u03bb k y/\u03c0 t y) 1/K ,\u2200y. Then the scoring function f\u0304s(x) = 1K \u2211K k=1 f\nk(x) output by Alg. 1 is of the form: softmaxj(f\u0304s(x)) \u221d \u03bb\u0304jptj(x), \u2200j \u2208 [m], \u2200(x, y) \u2208 S. Theorem 4. Suppose pt \u2208 F and F is closed under linear transformations. Suppose ` is the cross-entropy loss `xent, `(y, z) \u2264 B and maxy\u2208[m] 1\u03c0ty \u2264 Z, for some B,Z > 0. Furthermore, suppose for any \u03b4 \u2208 (0, 1), the following bound holds on the estimation error in Theorem 2: with probability at least 1\u2212 \u03b4 (over draw of S \u223c Dn), \u2200f \u2208 F , maxy\u2208[m]\n\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223 \u2264 \u2206(n, \u03b4), for some \u2206(n, \u03b4) \u2208 R+ that is increasing in 1/\u03b4, and goes to 0 as n\u2192\u221e. Then when the step size \u03b3 = 12BZ \u221a log(m) K and n\nval \u2265 8Z log(2m/\u03b4), we have that with probability at least 1\u2212 \u03b4 (over draw of S \u223c Dn and Sval \u223c Dnval ), Lrob(f\u0304s) \u2264 min\nf\u2208F Lrob(f)\n+ 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\ufe38 \ufe37\ufe37 \ufe38 Estimation error + 2B max y\u2208[m]\nEx [\u2223\u2223\u2223\u2223pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223]\ufe38 \ufe37\ufe37 \ufe38 Approximation error + 4BZ \u221a log(m) K\ufe38 \ufe37\ufe37 \ufe38 EG convergence .\nProposition 3 shows the student not only learns to mimic the teacher on the training set, but makes per-class adjustments to its predictions, and Theorem 4 shows that these adjustments are chosen to close-in on the gap to the optimal robust scorer in F . The form of the student suggests that it can not only match the teacher\u2019s performance, but can potentially improve upon it by making adjustments to its scores. However, the student\u2019s convergence to the optimal scorer in F would still be limited by the teacher\u2019s approximation error: even when the sample sizes and number of iterations n, nval,K \u2192\u221e, the student\u2019s optimality gap may still be non-zero as long as the teacher is sub-optimal.\nConnection to post-hoc adjustment. The form of the student in Proposition 3 raises an interesting question. Instead of training an explicit student model, why not directly construct a new scoring model by making post-hoc adjustments to the teacher\u2019s predictions? Specifically, one could optimize over functions of the form fsy (x) = log(\u03b3yp t y(x)), where the teacher p\nt is fixed, and pick the coefficients \u03b3 \u2208 Rm so that resulting scoring function yields the best worst-class accuracy on a held-out dataset. This simple post-hoc adjustment strategy may not be feasible if the goal is to distill to a student that is considerably smaller than the teacher. Often, this is the case in settings where distillation is used as a compression technique. Yet, this post-hoc method serves as good baseline to compare with.\nOne-hot validation labels. If Algorithm 1 used one-hot labels in the validation set instead of the teacher generated labels (as prescribed in (10)), the form of the student learned remains the same as in Theorem 4. However, the coefficients \u03bb\u0304 used to make adjustments to the teacher\u2019s predictions enjoy a slightly different guarantee. As shown in Appendix C, the approximation error bound now has a weaker dependence on the teacher\u2019s predictions (and hence is more immune to the teacher\u2019s errors), while the estimation error bound incurs slower convergence with increase in sample size."
        },
        {
            "heading": "6 Experiments",
            "text": "Datasets. We evaluate each robust distillation objective across different image dataset benchmarks: (i) CIFAR-10, (ii) CIFAR-100 [17], and (iii) TinyImageNet (a subset of ImageNet with 200 classes;\nLe and Yang [18]). We also include long tailed versions of each dataset [6]. Details on sampling the long tailed versions of the datasets and additional results on the full ImageNet dataset [28] are given in Appendix E. For all datasets, we randomly split the original default test set in half to create our validation set and test set. We use the same validation and test sets for the long-tailed training sets as we do for the original versions, following the convention in prior work [23, 24].\nArchitectures. We evaluate our distillation protocols in both a self-distillation and compression setting. On all CIFAR datasets, all teachers were trained with the ResNet-56 architecture and students were trained with either ResNet-56 or ResNet-32. On TinyImageNet, teachers and students were trained with ResNet-18. On ImageNet, teachers and students were trained with ResNet-34 and ResNet-18. This is as done by Lukasik et al. [20] and He et al. [12], where more details on these architectures can be found (see, e.g., Table 7 in Lukasik et al. [20]).\nHyperparameters. We apply temperature scaling to the teacher score distributions, i.e., compute pt(x) = softmax(f t(x)/\u03b3), and vary the temperature parameter \u03b3 over a range of {1, 3, 5}. Unless otherwise specified, the temperature hyperparameters were chosen to achieve the best worst-class accuracy on the validation set. A higher temperature produces a softer probability distribution over classes [14]. When teacher labels are applied to the validation set (see Section 4), we additionally include a temperature of 0.1 to approximate a hard thresholding of the teacher probabilities. We\nclosely mimic the learning rate and regularization settings from prior work [23, 24]. In keeping with the theory, the regularization ensures that the losses are bounded. See Appendix E for further details.\nBaselines. We compare the robust distillation objectives with the following: (i) No distillation: Models trained without distillation using each objective: Lstd, Lbal, and Lrob. We also include a comparison to group DRO [30] without distillation in Appendix E which differs from our robust objective Lrob in that we apply a margin-based loss with a validation set. (ii) Standard distillation without robustness: Standard distillation protocol where both the teacher and student are trained with Lstd, Lstd-d, respectively. (iii) Post-shifting: Following Section 5, we evaluate a post-shift approach that directly constructs a new scoring model by making post-hoc adjustments to the teacher, so as to maximize the robust accuracy on the validation sample [24]. (iv) AdaMargin and AdaAlpha [20]: Both Ada* techniques are motivated by the observation that the margin defined for each class y by \u03b3avg(y, pt(x)) = pty(x)\u2212 1m\u22121 \u2211 y\u2032 6=y p t y\u2032(x) correlates with whether distillation improves over one-hot training [20]. AdaMargin uses that quantity as a margin in the distillation loss, whereas AdaAlpha uses it to adaptively mix between the one-hot and distillation losses.\nWhich teacher/student combo is most robust? For each objective and baseline, we report the standard accuracy over the test set (see (1)), as well as the worst-class accuracy, which we define to be the minimum per-class recall over all classes (see (3)). For the long-tail datasets, we follow the convention in Menon et al. [23] of reporting balanced accuracy (see (2)) instead of standard accuracy. In each case, we use the 0-1 loss for evaluation. Table 1 shows results for various combinations of the proposed robust objectives and baselines. Across all datasets, the combination with best worst-class objective had at least one of either the teacher or the student apply the robust objective (Lrob or Lrob-d). These combinations also achieve higher worst-class accuracy compared to the post-shift and AdaMargin techniques; although pairing these techniques with robust teachers could be competitive. Interestingly, post-shift was often seen to over-fit to the validation set, resulting in poorer test performance. As another comparison point, prior work by [24] also perform distillation with a standard teacher (Lstd) and robust student (Lrob) with true one-hot labels on the validation set. In comparison to this, the new set of proposed robust combinations still achieves gains.\nAn inspection of the first row of Table 1 reveals counter-intuitively that the teacher\u2019s worst-class accuracy is not a direct predictor of the robustness of a subsequent student. This couples with our theoretical understanding in Section 5, which showed that the ability of a teacher to train good students is determined by the calibration of scores within each class. Perhaps surprisingly, it did not always benefit the robust student (Lrob-d) to utilize the true one-hot labels in the validation set. Instead, training the robust student with teacher labels on the validation set was sufficient to achieve the best worst-class performance. This is promising from a data efficiency standpoint, since it can be expensive to build up a labeled dataset for validation, especially if the training data is long-tailed.\nTrading off balanced vs. worst-class accuracy. We also evaluate the robust objectives in a setting where distillation is used for efficiency gains by distilling a larger teacher into a smaller student. In Figure 1, we plot the full trade-off between balanced accuracy and worst-class accuracy for the robust distillation protocols. Each point represents the outcome of a given combination of teacher and student objectives, where the teacher optimizes Ltdf with tradeoff parameter \u03b1t, and the student optimizes Ltdf-d with tradeoff parameter \u03b1s. Strikingly, Figure 1 shows that ResNet-32 students\ndistilled with robust trade-offs can be more Pareto efficient than even the larger ResNet-56 teacher models. Thus, distillation with combinations of robust losses not only helps worst-case accuracy, but also achieves better trade-offs with balanced accuracy. Similar trends prevail across our experiment setups, including self distillation and the original non-long-tailed datasets (see Appendix E).\nOverall, we demonstrate empirically and theoretically the value of applying different combinations of teacher-student objectives, not only for improving worst-class accuracy, but also to achieve efficient trade-offs between average and worst-class accuracy. Future avenues for exploration include experiments with mixtures of label types and effects of properties of the data on robustness."
        },
        {
            "heading": "A Proofs",
            "text": ""
        },
        {
            "heading": "A.1 Proof of Theorem 1",
            "text": "(i) The first result follows from the fact that the cross-entropy loss is a proper composite loss [37] with the softmax function as the associated (inverse) link function.\n(ii) For a proof of the second result, please see Menon et al. [23].\n(iii) Below, we provide a proof for the third result.\nThe minimization of the robust objective in (3) over f can be re-written as a min-max optimization problem:\nmin f :X\u2192Rm Lrob(f) = min f :X\u2192Rm max \u03bb\u2208\u2206m m\u2211 y=1 \u03bby \u03c0y\nE [\u03b7y(X) `(y, f(X))]\ufe38 \ufe37\ufe37 \ufe38 \u03c9(\u03bb,f) . (11)\nThe min-max objective \u03c9(\u03bb, f) is clearly linear in \u03bb (for fixed f ) and with ` chosen to be the cross-entropy loss, is convex in f (for fixed \u03bb), i.e., \u03c9(\u03bb, \u03baf1 + (1 \u2212 \u03ba)f2) \u2264 \u03ba\u03c9(\u03bb, f1) + (1 \u2212 \u03ba)\u03c9(\u03bb, f2), \u2200f1, f2 : X \u2192 Rm, \u03ba \u2208 [0, 1]. Furthermore, \u2206m is a convex compact set, while the domain of f is convex. It follows from Sion\u2019s minimax theorem [33] that:\nmin f :X\u2192Rm max \u03bb\u2208\u2206m \u03c9(\u03bb, f) = max \u03bb\u2208\u2206m min f :X\u2192Rm \u03c9(\u03bb, f). (12)\nLet (\u03bb\u2217, f\u2217) be such that:\n\u03bb\u2217 \u2208 argmax \u03bb\u2208\u2206m min f :X\u2192Rm \u03c9(\u03bb, f); f\u2217 \u2208 argmin f :X\u2192Rm max \u03bb\u2208\u2206m \u03c9(\u03bb, f),\nwhere for any fixed \u03bb \u2208 \u2206m, owing to the use of the cross-entropy loss, a minimizer f\u2217 always exists for \u03c9(\u03bb, f), and is given by f\u2217y (x) = log ( \u03bby \u03c0y \u03b7y(x) ) + C, for some C \u2208 R.\nWe then have from (12):\n\u03c9(\u03bb\u2217, f\u2217) \u2264 max \u03bb\u2208\u2206m \u03c9(\u03bb, f\u2217)\n= min f :X\u2192Rm max \u03bb\u2208\u2206m \u03c9(\u03bb, f) = max \u03bb\u2208\u2206m min f :X\u2192Rm \u03c9(\u03bb, f)\n= min f :X\u2192Rm\n\u03c9(\u03bb\u2217, f) \u2264 \u03c9(\u03bb\u2217, f\u2217),\nwhich tells us that there exists (\u03bb\u2217, f\u2217) is a saddle-point for (11), i.e.,\n\u03c9(\u03bb\u2217, f\u2217) = max \u03bb\u2208\u2206m \u03c9(\u03bb, f\u2217) = min f :X\u2192Rm \u03c9(\u03bb\u2217, f).\nConsequently, we have:\nLrob(f\u2217) = max \u03bb\u2208\u2206m \u03c9(\u03bb, f\u2217) = min f :X\u2192Rm \u03c9(\u03bb\u2217, f)\n\u2264 max \u03bb\u2208\u2206m min f :X\u2192Rm \u03c9(\u03bb, f) = min f :X\u2192Rm max \u03bb\u2208\u2206m \u03c9(\u03bb, f)\n= min f :X\u2192Rm\nLrob(f),\nwhere the last equality follows from (11). We thus have that f\u2217 is a minimizer of Lrob(f)\nFurthermore, because f\u2217 is a minimizer of \u03c9(\u03bb\u2217, f) over f , i.e.,\nf\u2217 \u2208 argmin f :X\u2192Rm m\u2211 y=1 \u03bb\u2217y \u03c0y E [\u03b7y(X) `(y, f(X))] ,\nit follows that:\nsoftmaxy(f\u2217(x)) \u221d \u03bb\u2217y \u03c0y \u03b7y(x).\n(iv) For the fourth result, we expand the traded-off objective, and re-write it as:\nLtdf(f) = (1\u2212 \u03b1)Lbal(f) + \u03b1Lrob(f)\n= (1\u2212 \u03b1) 1 m m\u2211 y=1 1 \u03c0y E [\u03b7y(X) `(y, f(X))] + \u03b1 max \u03bb\u2208\u2206m m\u2211 y=1 \u03bby \u03c0y E [\u03b7y(X) `(y, f(X))]\n= max \u03bb\u2208\u2206m m\u2211 y=1 ( (1\u2212 \u03b1) 1 m + \u03b1\u03bby ) 1\n\u03c0y E [\u03b7y(X) `(y, f(X))]\ufe38 \ufe37\ufe37 \ufe38\n\u03c9(\u03bb,f)\n.\nFor a fixed \u03bb, \u03c9(\u03bb, f) is convex in f (as the loss ` is the cross-entropy loss), and for a fixed f , \u03c9(\u03bb, f) is linear in \u03bb. Following the same steps as the proof of (iii), we have that there exists (\u03bb\u2217, f\u2217) such that\nLtdf(f\u2217) = max \u03bb\u2208\u2206m \u03c9(\u03bb, f\u2217) = min f :X\u2192Rm Ltdf(f),\nand\nf\u2217 \u2208 argmin f :X\u2192Rm m\u2211 y=1 ( (1\u2212 \u03b1) 1 m + \u03b1\u03bb\u2217y ) 1 \u03c0y E [\u03b7y(X) `(y, f(X))] ,\nwhich, owing to the properties of the cross-entropy loss, then gives us the desired form for f\u2217."
        },
        {
            "heading": "A.2 Proof of Theorem 2",
            "text": "Proof. Expanding the left-hand side, we have:\n|L\u0302rob-d(f)\u2212 Lrob(f)| \u2264 |L\u0302rob-d(f)\u2212 Lrob-d(f) + Lrob-d(f)\u2212 Lrob(f)| \u2264 |L\u0302rob-d(f)\u2212 Lrob-d(f)|+ |Lrob-d(f)\u2212 Lrob(f)|\n= |L\u0302rob-d(f)\u2212 Lrob-d(f)|+ \u2223\u2223\u2223\u2223\u2223maxy\u2208[m] Ex [ pty(x) `(y, f(x)) ] Ex [ pty(x) ] \u2212 max y\u2208[m] Ex [\u03b7y(x) `(y, f(x))] \u03c0y \u2223\u2223\u2223\u2223\u2223 \u2264 |L\u0302rob-d(f)\u2212 Lrob-d(f)|+ max\ny\u2208[m] \u2223\u2223\u2223\u2223\u2223Ex [ pty(x) `(y, f(x)) ] Ex [ pty(x) ] \u2212 Ex [\u03b7y(x) `(y, f(x))] \u03c0y \u2223\u2223\u2223\u2223\u2223 \u2264 |L\u0302rob-d(f)\u2212 Lrob-d(f)|+B max\ny\u2208[m] Ex [\u2223\u2223\u2223\u2223\u2223 pty(x)Ex [pty(x)] \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223\u2223 `(y, f(x)) ]\n\u2264 |L\u0302rob-d(f)\u2212 Lrob-d(f)|+B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223\u2223 pty(x)Ex [pty(x)] \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223\u2223 ] ,\nwhere the second-last step uses Jensen\u2019s inequality and the fact that `(y, f(x)) \u2265 0, and the last step uses the fact that `(y, f(x)) \u2264 B. Further expanding the first term,\n|L\u0302rob-d(f)\u2212 Lrob(f)| \u2264 \u2223\u2223\u2223\u2223maxy\u2208[m]\u03c6y(f) \u2212 maxy\u2208[m] \u03c6\u0302y(f) \u2223\u2223\u2223\u2223+B maxy\u2208[m]Ex [\u2223\u2223\u2223\u2223\u2223 pty(x)Ex [pty(x)] \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223\u2223 ]\n\u2264 max y\u2208[m] \u2223\u2223\u2223\u03c6y(f) \u2212 \u03c6\u0302y(f)\u2223\u2223\u2223+B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223\u2223 pty(x)Ex [pty(x)] \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223\u2223 ] ,\nas desired.\nA.3 Calibration of Margin-based Loss Lmar\nTo show that minimizer of the margin-based objective in (9) also minimizes the balanced objective in (6), we state the following general result:\nLemma 5. Suppose pt \u2208 F and F is closed under linear transformations. Let\nf\u0302 \u2208 argmin f\u2208F\n1\nn n\u2211 i=1 Lmar ( pt(xi), f(xi); c ) (13)\nfor some cost vector c \u2208 Rm+ . Then: f\u0302y(xi) = log ( cyp t y(xi) ) + Ci, \u2200i \u2208 [n],\nfor some example-specific constant constants Ci \u2208 R,\u2200i \u2208 [n]. Furthermore, for any assignment of example weights of w \u2208 Rn+, f\u0302 is also the minimizer of the weighted objective:\nf\u0302 \u2208 argmin f\u2208F\n1\nn n\u2211 i=1 wi m\u2211 y=1 cy p t y(xi) ` (y, f(xi)) . (14)\nProof. Following Menon et al. [23] (e.g. proof of Theorem 1), we have that for class probabilities p \u2208 \u2206m and costs c \u2208 Rm+ , the margin-based loss in (9)\nLmar (p, f ; c) = 1 m \u2211 y\u2208[m] py log ( 1 + \u2211 j 6=y exp (log(cy/cj) \u2212 (fy \u2212 fj)) ) .\nis minimized by: f\u2217y = log (cypy) + C,\nfor any C > 0. To see why this is true, note that the above loss can be equivalently written as:\nLmar (p, f ; c) = \u2212 1 m \u2211 y\u2208[m] py log ( exp (fy \u2212 log(cy))\u2211m j=1 exp (fj \u2212 log(cj)) ) .\nThis the same as the softmax cross-entropy loss with adjustments made to the logits, the minimizer for which is of the form:\nf\u2217y \u2212 log(cy) = log (py) + C or f\u2217y = log (cypy) + C.\nIt follows that any minimizer f\u0302 of the average margin-based loss in (13) over sample S, would do so point-wise, and therefore\nf\u0302y(xi) = log ( cyp t y(xi) ) + Ci, \u2200i \u2208 [n],\nfor some example-specific constant constants Ci \u2208 R,\u2200i \u2208 [n].\nTo prove the second part, we note that for the minimizer f\u0302 to also minimize the weighted objective:\n1\nn n\u2211 i=1 wi m\u2211 y=1 cy p t y(xi) ` (y, f(xi)) ,\nit would also have to do so point-wise for each i \u2208 [m], and so as long the weightswi are non-negative, it suffices that\nf\u0302(xi) \u2208 argmin f\u2208Rm m\u2211 y=1 cy p t y(xi) ` (y, f(xi)) .\nThis is indeed the case when ` is the softmax cross-entropy loss, where the point-wise minimizer for each i \u2208 [m] would be of the form softmaxy(f(x)) = cypty(x), which is satisfied by f\u0302 .\nA similar result also holds in the population limit, when (13) and (14) are computed in expectation, and the per-example weighting in (14) is replaced by an arbitrary weighting function w(x) \u2208 R+. Any scorer of the following form would then minimize both objectives:\nf\u0302y(x) = log ( cyp t y(x) ) + C(x), \u2200x \u2208 X ,\nwhere C(x) is some example-specific constant."
        },
        {
            "heading": "A.4 Proof of Proposition 3",
            "text": "Proposition (Restated). Suppose pt \u2208 F and F is closed under linear transformations. Then the final scoring function f\u0304s(x) = 1K \u2211K k=1 f k(x) output by Algorithm 1 is of the form:\nsoftmaxj(f\u0304s(x)) \u221d \u03bb\u0304jptj(x), \u2200j \u2208 [m], \u2200(x, y) \u2208 S, where \u03bb\u0304y = (\u220fK k=1 \u03bb k y/\u03c0 t y )1/K .\nProof. The proof follows from Lemma 5 with the costs c set to \u03bbk/\u03c0t for each iteration k. The lemma tells us that each fk is of the form:\nfk(x\u2032) = log ( \u03bbky \u03c0ty pty(x \u2032) ) + C(x\u2032), \u2200(x\u2032, y\u2032) \u2208 S,\nfor some example-specific constant C(x\u2032) \u2208 R. Consequently, we have that: f\u0304sy (x \u2032) = log(\u03bb\u0304yp t y(x \u2032)) + C\u0304(x\u2032), \u2200(x\u2032, y\u2032) \u2208 S,\nwhere \u03bb\u0304y = (\u220fK k=1 \u03bb k y/\u03c0 t y )1/K and C\u0304(x\u2032) \u2208 R. Applying a softmax to f\u0304s results in the desired form."
        },
        {
            "heading": "A.5 Proof of Theorem 4",
            "text": "Theorem (Restated). Suppose pt \u2208 F and F is closed under linear transformations. Suppose ` is the softmax cross-entropy loss `xent, `(y, z) \u2264 B and maxy\u2208[m] 1\u03c0ty \u2264 Z, for some B,Z > 0. Furthermore, suppose for any \u03b4 \u2208 (0, 1), the following bound holds on the estimation error in Theorem 2: with probability at least 1\u2212 \u03b4 (over draw of S \u223c Dn), for all f \u2208 F ,\nmax y\u2208[m] \u2223\u2223\u03c6y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223 \u2264 \u2206(n, \u03b4), for some \u2206(n, \u03b4) \u2208 R+ that is increasing in 1/\u03b4, and goes to 0 as n \u2192 \u221e. Fix \u03b4 \u2208 (0, 1). Then when the step size \u03b3 = 12BZ \u221a log(m) K and n\nval \u2265 8Z log(2m/\u03b4), with probability at least 1\u2212 \u03b4 (over draw of S \u223c Dn and Sval \u223c Dnval )\nLrob(f\u0304s) \u2264 min f\u2208F Lrob(f) + 2B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223]\ufe38 \ufe37\ufe37 \ufe38"
        },
        {
            "heading": "Approximation error",
            "text": "+ 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\ufe38 \ufe37\ufe37 \ufe38 Estimation error + 4BZ\n\u221a log(m)\nK\ufe38 \ufe37\ufe37 \ufe38 EG convergence .\nBefore proceeding to the proof, we will find it useful to define:\n\u03c6\u0302valy (f s) =\n1\n\u03c0\u0302t,valy\n1\nnval \u2211 (x\u2032,y\u2032)\u2208Sval pty(x \u2032) ` (y, fs(x\u2032)) .\nWe then state a useful lemma. Lemma 6. Suppose the conditions in Theorem 4 hold. Then with probability \u2264 1\u2212 \u03b4 (over draw of S \u223c Dn and Sval \u223c Dnval ), at each iteration k,\nm\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 min f\u2208F m\u2211 y=1 \u03bbk+1y \u03c6y(f) \u2264 2\u2206(n, \u03b4);\nand for any \u03bb \u2208 \u2206m: \u2223\u2223\u2223\u2223\u2223 m\u2211 y=1 \u03bby\u03c6\u0302 val y (f k+1) \u2212 m\u2211 y=1 \u03bby\u03c6y(f k+1) \u2223\u2223\u2223\u2223\u2223 \u2264 \u2206(nval, \u03b4).\nProof. We first note that by applying Lemma 5 with wi = 1,\u2200i, we have that fk+1 is the minimizer of \u2211m y=1 \u03bb k+1 y \u03c6\u0302y(f) over all f \u2208 F , and therefore:\nm\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f k+1) \u2264 m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f), \u2200f \u2208 F . (15)\nFurther, for a fixed iteration k, let us denote f\u0303 \u2208 argmin f\u2208F\n\u2211m y=1 \u03bb k+1 y \u03c6y(f). Then for the first part,\nwe have: m\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303)\n\u2264 m\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f k+1) + m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303)\n\u2264 m\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f k+1) + m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f\u0303) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303)\n\u2264 2 sup f\u2208F \u2223\u2223\u2223\u2223\u2223 m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f) \u2223\u2223\u2223\u2223\u2223 \u2264 2 sup\nf\u2208F max \u03bb\u2208\u2206m \u2223\u2223\u2223\u2223\u2223 m\u2211 y=1 \u03bby\u03c6\u0302y(f) \u2212 m\u2211 y=1 \u03bby\u03c6y(f) \u2223\u2223\u2223\u2223\u2223 \u2264 2 sup\nf\u2208F max \u03bb\u2208\u2206m m\u2211 y=1 \u03bby \u2223\u2223\u2223\u03c6\u0302y(f) \u2212 \u03c6y(f)\u2223\u2223\u2223 = 2 sup\nf\u2208F max y\u2208[m] \u2223\u2223\u03c6\u0302y(f)\u2212 \u03c6y(f)\u2223\u2223. where for the second inequality, we use (15). Applying the generalization bound assumed in Theorem 4, we have with probability \u2264 1\u2212 \u03b4 (over draw of S \u223c Dn), for all iterations k \u2208 [K],\nm\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303) \u2264 2\u2206(n, \u03b4),\nFor the second part, note that for any \u03bb \u2208 \u2206m,\u2223\u2223\u2223\u2223\u2223 m\u2211 y=1 \u03bby\u03c6\u0302 val y (f k+1) \u2212 m\u2211 y=1 \u03bby\u03c6y(f k+1) \u2223\u2223\u2223\u2223\u2223 \u2264 m\u2211 y=1 \u03bby \u2223\u2223\u2223\u03c6\u0302valy (fk+1) \u2212 \u03c6y(fk+1)\u2223\u2223\u2223 \u2264 max y\u2208[m]\n\u2223\u2223\u2223\u03c6\u0302valy (fk+1) \u2212 \u03c6y(fk+1)\u2223\u2223\u2223 \u2264 sup f\u2208F max y\u2208[m]\n\u2223\u2223\u2223\u03c6\u0302valy (f) \u2212 \u03c6y(f)\u2223\u2223\u2223 . An application of the generalization bound assumed in Theorem 4 to empirical estimates from the validation sample completes the proof.\nWe are now ready to prove Theorem 4.\nProof of Theorem 4. Note that because miny\u2208[m] \u03c0ty \u2265 1Z and n val \u2265 8Z log(2m/\u03b4), we have by a direct application of Chernoff\u2019s bound (along with a union bound over all m classes) that with probability at least 1\u2212 \u03b4/2:\nmin y\u2208[m]\n\u03c0\u0302t,valy \u2265 1\n2Z ,\u2200y \u2208 [m]\nand consequently, \u03c6\u0302valy (f) \u2264 2BZ, \u2200f \u2208 F . The boundedness of \u03c6\u0302valy will then allow us to apply\nstandard convergence guarantees for exponentiated gradient ascent [32]. For \u03b3 = 12BZ \u221a log(m) K , the updates on \u03bb will give us with probability at least 1\u2212 \u03b4/2:\nmax \u03bb\u2208\u2206m\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bby\u03c6\u0302 val y (f k) \u2264 1 K K\u2211 k=1 m\u2211 y=1 \u03bbky\u03c6\u0302 val y (f k) + 4BZ\n\u221a log(m)\nK (16)\nApplying the second part of Lemma 6 to each iteration k, we have with probability at least 1\u2212 \u03b4:\nmax \u03bb\u2208\u2206m\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bby\u03c6y(f k) \u2264 1 K K\u2211 k=1 m\u2211 y=1 \u03bbky\u03c6y(f k) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2),\nand applying the first part of Lemma 6 to the RHS, we have with the same probability:\nmax \u03bb\u2208\u2206m\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bby\u03c6y(f k)\n\u2264 1 K K\u2211 k=1 min f\u2208F m\u2211 y=1 \u03bbky\u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\n\u2264 min f\u2208F\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bbky\u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2).\nNote that we have taken a union bound over the high probability statement in (16) and that in Lemma 6. Using the convexity of \u03c6(\u00b7) in f(x) and Jensen\u2019s inequality, we have that \u2211m y=1 \u03bby\u03c6y(f\u0304\ns) \u2264 1 K \u2211K k=1 \u2211m y=1 \u03bby\u03c6y(f k). We use this to further lower bound the LHS in terms of the averaged\nscoring function f\u0304s(x) = 1K \u2211K k=1 f k(x):\nmax \u03bb\u2208\u2206m m\u2211 y=1 \u03bby\u03c6y(f\u0304 s)\n\u2264 min f\u2208F\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bbky\u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\n= min f\u2208F m\u2211 y=1 \u03bb\u0303y\u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\n\u2264 max \u03bb\u2208\u2206m min f\u2208F m\u2211 y=1 \u03bby\u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\n= min f\u2208F max \u03bb\u2208\u2206m m\u2211 y=1 \u03bby\u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\n= min f\u2208F max y\u2208[m] \u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2), (17)\nwhere in the second step \u03bb\u0303y = 1K \u2211K k=1 \u03bb k y ; in the fourth step, we swap the \u2018min\u2019 and \u2018max\u2019 using Sion\u2019s minimax theorem [33]. We further have from (17),\nmax y\u2208[m]\n\u03c6y(f\u0304 s) \u2264 min\nf\u2208F max y\u2208[m] \u03c6y(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2).\nIn other words,\nLrob-d(f\u0304s) \u2264 min f\u2208F Lrob-d(f) + 4BZ\n\u221a log(m)\nK + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2).\nTo complete the proof, we need to turn this into a guarantee on the original robust objective Lrob in (3):\nLrob(f\u0304s)\n\u2264 min f\u2208F Lrob(f) + 2 max f\u2208F \u2223\u2223Lrob(f)\u2212 Lrob-d(f)\u2223\u2223 + 4BZ\u221a log(m) K + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\n\u2264 min f\u2208F Lrob(f) + 2B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223] + 4BZ \u221a log(m) K + 2\u2206(nval, \u03b4/2) + 2\u2206(n, \u03b4/2),\nwhere we have used the bound on the approximation error in the proof of Theorem 2. This completes the proof."
        },
        {
            "heading": "B Student Estimation Error",
            "text": "We now provide a bound on the estimation error in Theorem 4 using a generalization bound from Menon et al. [21]. Lemma 7. Let F \u2286 RX be a given class of scoring functions. Let V \u2286 RX denote the class of loss functions v(x, y) = `(y, f(x)) induced by scorers f \u2208 F . LetMn = N\u221e( 1n ,V, 2n) denote the uniform L\u221e covering number for V . Fix \u03b4 \u2208 (0, 1). Suppose `(y, z) \u2264 B, \u03c0ty \u2264 1Z ,\u2200y \u2208 [m], and the number of samples n \u2265 8Z log(4m/\u03b4). Then with probability \u2265 1 \u2212 \u03b4 over draw of S \u223c Dn, for any f \u2208 F and y \u2208 [m]:\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2264 CZ (\u221aVn,y(f) log(mMn/\u03b4)\nn + log(mMn/\u03b4) n + B\n\u221a log(m/\u03b4)\nn\n) ,\nwhere Vn,y(f) denotes the empirical variance of the loss values {pty(xi) \u00b7 `(y, f(xi))}ni=1 for class y, and C > 0 is a distribution-independent constant.\nNotice the dependence on the variance that the teacher\u2019s prediction induce on the loss. This suggests that the lower the variance in the teacher\u2019s predictions, the better is the student\u2019s generalization. Similar to Menon et al. [21], one can further show that when the teacher closely approximates the Bayes-probabilities \u03b7(x), the distilled loss pty(xi) \u00b7 `(y, f(xi)) has a lower empirical variance that the loss `(yi, f(xi)) computed from one-hot labels.\nProof of Lemma 7. We begin by defining the following intermediate term:\n\u03c6\u0303y(f) = 1\n\u03c0ty\n1\nn n\u2211 i=1 pty(xi) ` (y, f(xi)) .\nThen for any y \u2208 [m],\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0303y(f)\u2223\u2223\u2223+ \u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 . (18) We next bound each of the terms in (18), starting with the first term:\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0303y(f)\u2223\u2223\u2223 = 1\n\u03c0ty \u2223\u2223\u2223\u2223\u2223Ex [pty(x) ` (y, f(x))] \u2212 1n n\u2211 i=1 pty(xi) ` (y, f(xi)) \u2223\u2223\u2223\u2223\u2223 \u2264 Z\n\u2223\u2223\u2223\u2223\u2223Ex [pty(x) ` (y, f(x))] \u2212 1n n\u2211 i=1 pty(xi) ` (y, f(xi)) \u2223\u2223\u2223\u2223\u2223 , where we use the fact that \u03c0ty \u2264 1Z ,\u2200y. Applying the generalization bound from Menon et al. [21, Proposition 2], along with a union bound over all m classes, we have with probability at least 1\u2212 \u03b4/2 over the draw of S \u223c Dn, for all y \u2208 [m]:\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0303y(f)\u2223\u2223\u2223 \u2264 C \u2032Z (\u221aVn,y(f) log(mMn/\u03b4)\nn + log(mMn/\u03b4) n\n) , (19)\nfor a distribution-independent constant C \u2032 > 0.\nWe next bound the second term in (18):\u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 1\u03c0ty \u2212 1\u03c0\u0302ty \u2223\u2223\u2223\u2223 1n n\u2211 i=1 pty(xi) \u00b7 ` (y, f(xi))\n\u2264 B \u2223\u2223\u2223\u2223 1\u03c0ty \u2212 1\u03c0\u0302ty \u2223\u2223\u2223\u2223 = B\n\u03c0ty\u03c0\u0302 t y \u2223\u2223\u03c0ty \u2212 \u03c0\u0302ty\u2223\u2223 , where in the second step we use the fact that `(y, f(x)) \u2264 B and pty(x) \u2264 1.\nFurther note that because miny\u2208[m] \u03c0ty \u2265 1Z and n \u2265 8Z log(4m/\u03b4), we have by a direct application of Chernoff\u2019s bound (and a union bound over m classes) that with probability at least 1\u2212 \u03b4/4:\nmin y\u2208[m]\n\u03c0\u0302ty \u2265 1\n2Z ,\u2200y \u2208 [m]. (20)\nTherefore for any y \u2208 [m]: \u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2264 2BZ2 \u2223\u2223\u03c0ty \u2212 \u03c0\u0302ty\u2223\u2223 . Conditioned on the above statement, a simple application of Hoeffdings and a union bound over all y \u2208 [m] gives us that with probability at least 1\u2212 \u03b4/4 over the draw of S \u223c Dn, for all y \u2208 [m]:\u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2264 2BZ2( 1\nZ\n\u221a log(8m/\u03b4)\n2n\n) = 2BZ \u221a log(8m/\u03b4)\n2n . (21)\nA union bound over the high probability statements in (19\u201321) completes the proof. To see this, note that, for any > 0 and y \u2208 [m],\nP (\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2265 ) \u2264 P\n((\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0303y(f)\u2223\u2223\u2223 \u2265 ) \u2228 (\u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2265 )) \u2264 P\n(\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0303y(f)\u2223\u2223\u2223 \u2265 ) + P(\u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2265 ) \u2264 P (\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0303y(f)\u2223\u2223\u2223 \u2265 ) + P(\u03c0\u0302ty \u2264 1Z ) \u00b7 P (\u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223 \u03c0\u0302ty \u2264 1Z\n) + P ( \u03c0\u0302ty \u2265 1\nZ ) \u00b7 P (\u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223 \u03c0\u0302ty \u2265 1Z ) \u2264 P (\u2223\u2223\u2223\u03c6y(f)\u2212 \u03c6\u0303y(f)\u2223\u2223\u2223 \u2265 ) + P(\u03c0\u0302ty \u2264 1Z ) + P (\u2223\u2223\u2223\u03c6\u0303y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223 \u03c0\u0302ty \u2265 1Z ) ,\nwhich implies that a union bound over (19\u201321) would give us the desired result in Lemma 7."
        },
        {
            "heading": "C DRO with One-hot Validation Labels",
            "text": "Algorithm 2 contains a version of the margin-based DRO described in Section 4, where instead of teacher labels the original one-hot labels are used in the validation set. Before proceeding to providing a convergence guarantee for this algorithm, we will find it useful to define the following one-hot metrics:\n\u03c6ohy (f s) =\n1\n\u03c0y Ex [\u03b7y(x) ` (y, fs(x))]\n\u03c6\u0302oh,valy (f s) =\n1\n\u03c0\u0302y\n1\nnval \u2211 (x\u2032,y\u2032)\u2208Sval 1(y\u2032 = y) ` (y\u2032, fs(x\u2032)) .\nAlgorithm 2 Distilled Margin-based DRO with One-hot Validation Labels Inputs: Teacher pt, Student hypothesis class F , Training set S, Validation set Sval, Step-size \u03b3 \u2208 R+, Number of iterations K, Loss ` Initialize: Student f0 \u2208 F , Multipliers \u03bb0 \u2208 \u2206m For k = 0 to K \u2212 1 \u03bb\u0303k+1j = \u03bb k j exp ( \u03b3R\u0302j ) ,\u2200j \u2208 [m]\nwhere R\u0302j = 1 nval 1\n\u03c0\u0302valj \u2211 (x,y)\u2208Sval `(y, fk(x)) and \u03c0\u0302valj = 1 nval \u2211 (x,y)\u2208Sval 1(y = j)\n\u03bbk+1y = \u03bb\u0303k+1y\u2211m\nj=1 \u03bb\u0303 k+1 j\n,\u2200y\nfk+1 \u2208 argmin f\u2208F\n1\nn n\u2211 i=1 Lmar ( pt(xi), f(xi); \u03bbk+1 \u03c0\u0302t ) // Replaced with a few steps of SGD\nEnd For Output: f\u0304s : x 7\u2192 1K \u2211K k=1 f k(x)\nTheorem 8. Suppose pt \u2208 F and F is closed under linear transformations. Then the final scoring function f\u0304s(x) = 1K \u2211K k=1 f k(x) output by Algorithm 2 is of the form:\nsoftmaxy(f\u0304s(x\u2032)) \u221d \u03bb\u0304ypty(x\u2032), \u2200(x\u2032, y\u2032) \u2208 S,\nwhere \u03bb\u0304y = (\u220fK k=1 \u03bb k y/\u03c0 t y )1/K . Furthermore, suppose ` is the softmax cross-entropy loss in `xent,\n`(y, z) \u2264 B, for some B > 0, and maxy\u2208[m] 1\u03c0y \u2264 Z, for some Z > 0. Suppose for any \u03b4 \u2208 (0, 1), the following holds: with probability at least 1\u2212 \u03b4 (over draw of S \u223c Dn), for all f \u2208 F ,\nmax y\u2208[m] \u2223\u2223\u03c6ohy (f)\u2212 \u03c6\u0302ohy (f)\u2223\u2223 \u2264 \u2206oh(n, \u03b4); max y\u2208[m] \u2223\u2223\u03c6y(f)\u2212 \u03c6\u0302y(f)\u2223\u2223 \u2264 \u2206(n, \u03b4), for some \u2206oh(n, \u03b4),\u2206(n, \u03b4) \u2208 R+ that is increasing in 1/\u03b4, and goes to 0 as n\u2192\u221e. Fix \u03b4 \u2208 (0, 1). Then when the step size \u03b3 = 12BZ \u221a log(m) K and n\nval \u2265 8Z log(2m/\u03b4), with probability at least 1\u2212 \u03b4 (over draw of S \u223c Dn and Sval \u223c Dnval ), for any \u03c4 \u2208 R+,\nLrob(f\u0304s) \u2264 min f\u2208F Lrob(f) + 2B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223\u03c4 \u00b7 pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223]\ufe38 \ufe37\ufe37 \ufe38 Approximation error\n+ 2\u03c4 \u00b7\u2206oh(nval, \u03b4/2) + 2\u2206(n, \u03b4/2)\ufe38 \ufe37\ufe37 \ufe38 Estimation error + 4BZ\n\u221a log(m)\nK\ufe38 \ufe37\ufe37 \ufe38 EG convergence .\nComparing this to the bound in Theorem 4, we can see that there is an additional scaling factor \u03c4 against the teacher probabilities pty(x) and in the approximation error. When we set \u03c4 = 1, the bound looks very similar to Theorem 4, except that the estimation error term \u2206oh now involves one-hot labels. Therefore the estimation error may incur a slower convergence with sample size as it no longer benefits from the lower variance that the teacher predictions may offer (see Appendix B for details).\nThe \u03c4 -scaling in the approximation error also means that the teacher is no longer required to exactly match the (normalized) class probabilities \u03b7(x). In fact, one can set \u03c4 to a value for which the approximation error is the lowest, and in general to a value that minimizes the upper bound in Theorem 8, potentially providing us with a tighter convergence rate than Theorem 4.\nThe proof of Theorem 8 is similar to that of Theorem 4, but requires a modified version of Lemma 6:\nLemma 9. Suppose the conditions in Theorem 4 hold. With probability \u2264 1 \u2212 \u03b4 (over draw of S \u223c Dn and Sval \u223c Dnval ), at each iteration k and for any \u03c4 \u2208 R+, m\u2211 y=1 \u03bbk+1y \u03c6 oh y (f k+1) \u2212 min f\u2208F m\u2211 y=1 \u03bbk+1y \u03c6 oh y (f) \u2264 2\u03c4 \u00b7\u2206(n, \u03b4) + 2B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223\u03c4 pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y\n\u2223\u2223\u2223\u2223] . Furthermore, with the same probability, for any \u03bb \u2208 \u2206m:\u2223\u2223\u2223\u2223\u2223 m\u2211 y=1 \u03bby\u03c6\u0302 oh,val y (f k+1) \u2212 m\u2211 y=1 \u03bby\u03c6 oh y (f k+1)\n\u2223\u2223\u2223\u2223\u2223 \u2264 \u2206oh(nval, \u03b4). Proof. We first note from Lemma 5 that because fk+1 \u2208 argmin\nf\u2208F\n1\nn n\u2211 i=1 Lmar ( pt(xi), f(xi); \u03bbk+1 \u03c0\u0302 ) ,\nwe have for the example-weighting wi = \u03c4,\u2200i:\n\u03c4 m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f k+1) \u2264 \u03c4 m\u2211 y=1 \u03bbk+1y \u03c6\u0302y(f), \u2200f \u2208 F . (22)\nFor a fixed iteration k, let us denote f\u0303 \u2208 argmin f\u2208F\n\u2211m y=1 \u03bb k+1 y \u03c6y(f). Then for the first part, we have\nfor any \u03c4 \u2208 R+: m\u2211 y=1 \u03bbk+1y \u03c6 oh y (f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6 oh y (f\u0303)\n\u2264 \u03c4 ( m\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303) ) + m\u2211 y=1 \u03bbk+1y \u2223\u2223\u03c6ohy (fk+1)\u2212 \u03c4\u03c6y(fk+1)\u2223\u2223\n+ m\u2211 y=1 \u03bbk+1y \u2223\u2223\u2223\u03c6ohy (f\u0303)\u2212 \u03c4\u03c6y(f\u0303)\u2223\u2223\u2223 \u2264 \u03c4\n( m\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303) ) + 2 max f\u2208F m\u2211 y=1 \u03bbk+1y \u2223\u2223\u03c6ohy (f)\u2212 \u03c4\u03c6y(f)\u2223\u2223\n\u2264 \u03c4 ( m\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303) ) + 2 max f\u2208F max \u03bb\u2208\u2206m m\u2211 y=1 \u03bb \u2223\u2223\u03c6ohy (f)\u2212 \u03c4\u03c6y(f)\u2223\u2223\n\u2264 \u03c4 ( m\u2211 y=1 \u03bbk+1y \u03c6y(f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6y(f\u0303) ) + 2 max f\u2208F max y\u2208[m] \u2223\u2223\u03c6ohy (f)\u2212 \u03c4\u03c6y(f)\u2223\u2223 \u2264 2\u03c4 sup\nf\u2208F max y\u2208[m] \u2223\u2223\u03c6\u0302y(f)\u2212 \u03c6y(f)\u2223\u2223 + 2 max f\u2208F max y\u2208[m] \u2223\u2223\u03c6ohy (f)\u2212 \u03c4\u03c6y(f)\u2223\u2223 . where the last inequality re-traces the steps in Lemma 6. Further applying the generalization bound assumed in Theorem 4, we have with probability \u2264 1\u2212 \u03b4 (over draw of S \u223c Dn), for all iterations k \u2208 [K] and any \u03c4 \u2208 R+,\nm\u2211 y=1 \u03bbk+1y \u03c6 oh y (f k+1) \u2212 m\u2211 y=1 \u03bbk+1y \u03c6 oh y (f\u0303) \u2264 2\u03c4\u2206(n, \u03b4) + 2 max f\u2208F max y\u2208[m] \u2223\u2223\u03c6ohy (f)\u2212 \u03c4\u03c6y(f)\u2223\u2223 . (23) All that remains is to bound the second term in (23). For any f \u2208 F and y \u2208 [m],\u2223\u2223\u03c6ohy (f)\u2212 \u03c4\u03c6y(f)\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223 1\u03c0yEx [\u03b7y(x) ` (y, f(x))] \u2212 \u03c4\u03c0tyEx [pty(x) ` (y, f(x))]\n\u2223\u2223\u2223\u2223 \u2264 Ex [\u2223\u2223\u2223\u2223 1\u03c0y \u03b7y(x) ` (y, f(x)) \u2212 \u03c4\u03c0ty pty(x) ` (y, f(x)) \u2223\u2223\u2223\u2223]\n= Ex [\u2223\u2223\u2223\u2223 1\u03c0y \u03b7y(x) \u2212 \u03c4\u03c0ty pty(x) \u2223\u2223\u2223\u2223 ` (y, fs(x))] \u2264 BEx [\u2223\u2223\u2223\u2223\u03b7y(x)\u03c0y \u2212 \u03c4 p t y(x) \u03c0ty\n\u2223\u2223\u2223\u2223] , where we use Jensen\u2019s inequality in the second step, the fact that `(y, z) \u2264 B is non-negative in the second step, and the fact that `(y, z) \u2264 B in the last step. Substituting this upper bound back into (23) completes the proof of the first part.\nThe second part follows from a direct application of the bound on the per-class estimation error maxy\u2208[m] \u2223\u2223\u03c6ohy (f)\u2212 \u03c6\u0302oh,valy (f)\u2223\u2223. Proof of Theorem 8. The proof traces the same steps as Proposition 3 and Theorem 4, except that it applies Lemma 9 instead of Lemma 6.\nNote that because miny\u2208[m] \u03c0y \u2265 1Z and n val \u2265 8Z log(2m/\u03b4), we have by a direct application of Chernoff\u2019s bound (along with a union bound over all m classes) that with probability at least 1\u2212 \u03b4/2:\nmin y\u2208[m]\n\u03c0\u0302oh,valy \u2265 1\n2Z ,\u2200y \u2208 [m],\nand consequently, \u03c6\u0302oh,valy (f) \u2264 2BZ, \u2200f \u2208 F . The boundedness of \u03c6\u0302oh,valy will then allow us to apply\nstandard convergence guarantees for exponentiated gradient ascent [32]. For \u03b3 = 12BZ \u221a log(m) K , the updates on \u03bb will give us:\nmax \u03bb\u2208\u2206m\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bby\u03c6\u0302 oh,val y (f k) \u2264 1 K K\u2211 k=1 m\u2211 y=1 \u03bbky\u03c6\u0302 oh,val y (f k) + 4BZ\n\u221a log(m)\nK\nApplying the second part of Lemma 6 to each iteration k, we have with probability at least 1\u2212 \u03b4:\nmax \u03bb\u2208\u2206m\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bby\u03c6 oh y (f k) \u2264 1 K K\u2211 k=1 m\u2211 y=1 \u03bbky\u03c6 oh y (f k) + 4BZ\n\u221a log(m)\nK + 2\u2206oh(nval, \u03b4/2),\nand applying the first part of Lemma 6 to the RHS, we have with the same probability, for any \u03c4 \u2208 R+:\nmax \u03bb\u2208\u2206m\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bby\u03c6 oh y (f k) \u2264 1 K K\u2211 k=1 min f\u2208F m\u2211 y=1 \u03bbky\u03c6 oh y (f) + 4BZ\n\u221a log(m)\nK + 2\u2206oh(nval, \u03b4/2)\n+ 2\u03c4\u2206(n, \u03b4/2) + 2B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223\u03c4 pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223]\n\u2264 min f\u2208F\n1\nK K\u2211 k=1 m\u2211 y=1 \u03bbky\u03c6 oh y (f) + 4BZ\n\u221a log(m)\nK + 2\u2206oh(nval, \u03b4/2)\n+ 2\u03c4\u2206(n, \u03b4/2) + 2B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223\u03c4 pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223] . Using the convexity of \u03c6(\u00b7) in f(x) and Jensen\u2019s inequality, we have that \u2211m y=1 \u03bby\u03c6y(f\u0304\ns) \u2264 1 K \u2211K k=1 \u2211m y=1 \u03bby\u03c6y(f k). We use this to further lower bound the LHS in terms of the averaged\nscoring function f\u0304s(x) = 1K \u2211K k=1 f k(x), and re-trace the steps in Theorem 4 to get\"\nmax y\u2208[m]\n\u03c6ohy (f\u0304 s) \u2264 min\nf\u2208F max y\u2208[m]\n\u03c6ohy (f) + 4BZ\n\u221a log(m)\nK + 2\u2206oh(nval, \u03b4/2)\n+ 2\u03c4\u2206(n, \u03b4/2) + 2B max y\u2208[m] Ex [\u2223\u2223\u2223\u2223\u03c4 pty(x)\u03c0ty \u2212 \u03b7y(x)\u03c0y \u2223\u2223\u2223\u2223] . Noting that Lrob(f) = maxy\u2208[m] \u03c6ohy (f) completes the proof.\nAlgorithm 3 Distilled Margin-based DRO for Traded-off Objective Inputs: Teacher pt, Student hypothesis class F , Training set S, Validation set Sval, Step-size \u03b3 \u2208 R+, Number of iterations K, Loss `, Trade-off parameter \u03b1 Initialize: Student f0 \u2208 F , Multipliers \u03bb0 \u2208 \u2206m For k = 0 to K \u2212 1 \u03bb\u0303k+1j = \u03bb k j exp ( \u03b3\u03b1R\u0302j ) ,\u2200j \u2208 [m] where R\u0302j = 1\nnval 1\n\u03c0\u0302t,valj \u2211 (x,y)\u2208Sval ptj(xi) `(j, f k(x))\n\u03bbk+1y = \u03bb\u0303k+1y\u2211m\nj=1 \u03bb\u0303 k+1 j\n,\u2200y\n\u03b2k+1y = (1\u2212 \u03b1) 1m + \u03b1\u03bb k+1 y\nfk+1 \u2208 argmin f\u2208F\n1\nn n\u2211 i=1 Lmar ( pt(xi), f(xi); \u03b2k+1 \u03c0\u0302t ) // Replaced with a few steps of SGD\nEnd For Output: f\u0304s : x 7\u2192 1K \u2211K k=1 f k(x)"
        },
        {
            "heading": "D DRO for Traded-off Objective",
            "text": "We present a variant of the margin-based DRO algorithm described in Section 4 that seeks to minimize a trade-off between the balanced and robust student objectives:\nL\u0302tdf-d(fs) = (1\u2212 \u03b1)L\u0302bal-d(fs) + \u03b1L\u0302rob-d(fs), for some \u03b1 \u2208 [0, 1]. Expanding this, we have:\nLtdf-d(f) = (1\u2212 \u03b1) 1 m m\u2211 y=1 1 \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) `(y, f(xi)) + \u03b1 max y\u2208[m] m\u2211 y=1 1 \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) `(y, f(xi))\n= (1\u2212 \u03b1) 1 m m\u2211 y=1 1 \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) `(y, f(xi)) + \u03b1 max \u03bb\u2208\u2206m m\u2211 y=1 \u03bby \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) `(y, f(xi))\n= max \u03bb\u2208\u2206m m\u2211 y=1 ( (1\u2212 \u03b1) 1 m + \u03b1\u03bby ) 1 \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) `(y, f(xi)).\nThe minimization of Ltdf-d(f) over f can then be a cast as a min-max problem:\nmin f :X\u2192Rm Ltdf-d(f) = min f :X\u2192Rm max \u03bb\u2208\u2206m m\u2211 y=1 ( (1\u2212 \u03b1) 1 m + \u03b1\u03bby ) 1 \u03c0\u0302ty 1 n n\u2211 i=1 pty(xi) `(y, f(xi)).\nRetracing the steps in the derivation of Algorithm 1 in Section 4, we have the following updates on \u03bb and f to solve the above min-max problem:\n\u03bb\u0303k+1y = \u03bb k y exp\n( \u03b3\u03b1 1\nn\u03c0\u0302ty n\u2211 i=1 pty(xi) ` ( y, fk(xi) )) ,\u2200y\n\u03bbk+1y = \u03bb\u0303k+1y\u2211m j=1 \u03bb\u0303 k+1 j ,\u2200y\n\u03b2k+1y = (1\u2212 \u03b1) 1\nm + \u03b1\u03bbk+1y\nfk+1 \u2208 argmin f\u2208F \u2211 y\u2208[m] \u03b2k+1y n\u03c0\u0302ty n\u2211 i=1 pty(xi) ` (y, f(xi)) ,\nfor step-size parameter \u03b3 > 0. To better handle training of over-parameterized students, we will perform the updates on \u03bb using a held-out validation set, and employ a margin-based surrogate for performing the minimization over f . This procedure is outlined in Algorithm 3."
        },
        {
            "heading": "E Further experiment details",
            "text": "This section contains further experiment details and additional results.\n\u2022 Appendices E.1 through E.3 contain additional details about the datasets, hyperparameters, and baselines.\n\u2022 Appendices E.4 through E.10 contain additional experimental comparisons with the AdaMargin and AdaAlpha baselines [20] and group DRO [30], and additional experimental results on CIFAR, TinyImageNet and ImageNet, along with additional trade-off plots.\nE.1 Additional details about datasets\nE.1.1 Building long tailed datasets\nThe long-tailed datasets were created from the original datasets following Cui et al. [6] by downsampling examples with an exponential decay in the per-class sizes. As done by Narasimhan and Menon [24], we set the imbalance ratio maxi P (y=i)mini P (y=i) to 100 for CIFAR-10 and CIFAR-100, and to 83 for TinyImageNet (the slightly smaller ratio is to ensure that the smallest class is of a reasonable size). We use the long-tail version of ImageNet generated by Liu et al. [19]."
        },
        {
            "heading": "E.1.2 Dataset splits",
            "text": "The original test samples for CIFAR-10, CIFAR-10-LT, CIFAR-100, CIFAR-100-LT, TinyImageNet (200 classes), TinyImageNet-LT (200 classes), and ImageNet (1000 classes) are all balanced. Following Narasimhan and Menon [24], we randomly split them in half and use half the samples as a validation set, and the other half as a test set. For the CIFAR and TineImageNet datasets, this amounts to using a validation set of size 5000. For the ImageNet dataset, we sample a subset of 5000 examples from the validation set each time we update the Lagrange multipliers in Algorithm 1.\nIn keeping with prior work [23, 24, 20], we use the same validation and test sets for the long-tailed training sets as we do for the original versions. For the long tailed training sets, this simulates a scenario where the training data follows a long tailed distribution due to practical data collection limitations, but the test distribution of interest still comes from the original data distribution. In plots, the \u201cbalanced accuracy\u201d that we report for the long-tail datasets (e.g., CIFAR-10-LT) is actually the standard accuracy calculated over the balanced test set, which is shared with the original balanced dataset (e.g., CIFAR-10).\nBoth teacher and student were always trained on the same training set.\nThe CIFAR datasets had images of size 32 \u00d7 32, while the TinyImageNet and ImageNet datasets dataset had images of size 224 \u00d7 224. These datasets do not contain personally identifiable information or offensive content. The CIFAR-10 and CIFAR-100 datasets are licensed under the MIT License. The terms of access for ImageNet are given at https://www.image-net.org/download.php.\nE.2 Additional details about training and hyperparameters\nE.2.1 Code\nWe have made our code available as a part of the supplementary material.\nE.2.2 Training details and hyperparameters\nUnless otherwise specified, the temperature hyperparameters were chosen to achieve the best worstclass accuracy on the validation set. All models were trained using SGD with momentum of 0.9 [20, 24].\nThe learning rate schedule were chosen to mimic the settings in prior work [24, 20]. For CIFAR-10 and CIFAR-100 datasets, we ran the optimizer for 450 epochs, linearly warming up the learning rate till the 15th epoch, and then applied a step-size decay of 0.1 after the 200th, 300th and 400th epochs, as done by Lukasik et al. [20]. For the long-tail versions of these datasets, we ran trained for 256\nepochs, linearly warming up the learning rate till the 15th epoch, and then applied a step-size decay of 0.1 after the 96th, 192nd and 224th epochs, as done by Narasimhan and Menon [24]. Similarly, for the TinyImageNet datasets, we train for 200 epochs, linearly warming up the learning rate till the 5th epoch, and then applying a decay of 0.1 after the 75th and 135th epochs, as done by Narasimhan and Menon [24]. For ImageNet, we train for 90 epochs, linearly warming up the learning rate till the 5th epoch, then applying a decay of 0.1 after the 30th, 60th and 80th epochs, as done by Lukasik et al. [20]. We used a batch size of 128 for the CIFAR-10 datasets [24], and a batch size of 1024 for the other datasets [20].\nWe apply an L2 weight decay of 10\u22124 in all our SGD updates [20]. This amounts to applying an L2 regularization on the model parameters, and has the effect of keeping the model parameters (and as a result the loss function) bounded.\nWhen training with the margin-based robust objective (see Algorithm 1), a separate step size \u03b3 was applied for training the main model function f , and for updating the multipliers \u03bb. We set \u03b3 to 0.1 in all experiments.\nHardware. Model training was done using TPUv2."
        },
        {
            "heading": "E.2.3 Repeats",
            "text": "For all comparative baselines without distillation (e.g. the first and second rows of Table 1), we provide average results over m retrained models (m = 5 for TinyImageNet, or m = 10 for CIFAR datasets). For students on all CIFAR* datasets, unless otherwise specified, we train the teacher once and run the student training 10 times using the same fixed teacher. We compute the mean and standard error of metrics over these m = 10 runs. For the resource-heavy TinyImageNet and ImageNet students, we reduce the number of repeats to m = 5. This methodology captures variation in the student retrainings while holding the teacher fixed. To capture the end-to-end variation in both teacher and student training, we include Appendix E.4 which contains a rerun of the CIFAR experiments in Table 1 using a distinct teacher for each student retraining. The overall best teacher/student objective combinations either did not change or were statistically comparable to the previous best combination in Table 1.\nE.3 Additional details about algorithms and baselines"
        },
        {
            "heading": "E.3.1 Practical improvements to Algorithms 1\u20133",
            "text": "Algorithms 1\u20133 currently return a scorer that averages over all K iterates f\u0304s(x) = 1K \u2211K k=1 f\nk(x). While this averaging was required for our theoretical robustness guarantees to hold, in our experiments, we find it sufficient to simply return the last model fK . Another practical improvement that we make to these algorithms following Cotter et al. [5], is to employ the 0-1 loss while performing updates on \u03bb, i.e., set ` = `0-1 in the \u03bb-update step. We are able to do this because the convergence of the exponentiated gradient updates on \u03bb does not depend on ` being differentiable. This modification allows \u03bbs to better reflect the model\u2019s per-class performance on the validation sample.\nE.3.2 Discussion on post-shifting baseline\nWe implement the post-shifting method in Narasimhan and Menon [24] (Algorithm 3 in their paper), which provides for an efficient way to construct a scoring function of the form fsy (x) = log(\u03b3yp t y(x)), for a fixed teacher pt, where the coefficients \u03b3 \u2208 Rm are chosen to maximize the worst-class accuracy on the validation dataset. Interestingly, in our experiments, we find this approach to do exceedingly well on the validation sample, but this does not always translate to good worst-class test performance. In contrast, some of the teacher-student combinations that we experiment with were seen to over-fit less to the validation sample, and as a result were able to generalize better to the test set. This could perhaps indicate that the teacher labels we use in these combinations benefit the student in a way that it improves its generalization. The variance reduction effect that Menon et al. [21] postulate may be one possible explanation for why we see this behavior.\nE.4 Different teachers on repeat trainings\nMost of the student results in the main paper in Table 1 use the same teacher for all repeat trainings of the student. This captures the variance in the student training process while omitting the variance in the teacher training process. To capture the variance in the full training pipeline, we ran an additional set of experiments where students were trained on different retrained teachers, rather than on the same teacher. We report results on all CIFAR datasets in Table 2. The best teacher/student combinations are identical for all datasets except for CIFAR-10-LT, for which the best teacher/student combinations from Table 2 were also not statistically significantly different from the best combination in Table 1 (Lbal/Lrob-d (one-hot val) vs. Lbal/Lrob-d (teacher val) in Table 1). Note that the first and second rows of Table 1 are already averaged over m retrained teachers (m = 5 for TinyImageNet, or m = 10 for CIFAR datasets), and those same m teachers are used in the repeat trainings in Table 2."
        },
        {
            "heading": "E.5 AdaAlpha and AdaMargin comparisons",
            "text": "We include and discuss additional comparisons to the AdaMargin and AdaAlpha methods [20]. These methods each define additional ways to modify the student\u2019s loss function (see Section 6). Table 3 shows results for these under the same self-distillation setup as in Table 1. For the balanced datasets, AdaMargin was competitive with the robust and standard students: on CIFAR-100 and TinyImageNet, AdaMargin combined with the robust teacher and the standard teacher (respectively) achieved worstclass accuracies that are statistically comparable to the best worst-class accuracies in Table 1 and Table 2. However, on the long-tailed datasets CIFAR-10-LT, CIFAR-100-LT, and TinyImageNet-LT (Table 4 below), AdaAlpha and AdaMargin did not achieve worst-class accuracies as high as other teacher/student combinations. This suggests that the AdaMargin method can work well on balanced datasets in combination with a robust teacher, but other combinations of standard/balanced/robust objectives are valuable for long-tailed datasets. Overall, AdaMargin achieved higher worst-class accuracies than AdaAlpha.\nE.6 TinyImageNet-LT worst-10 accuracy results\nIn this section we provide results for TinyImageNet-LT. We report worst-10 accuracies in Table 4, where the worst-k accuracy is defined to be the average of the k lowest per-class accuracies. We report worst-10 accuracies since the worst-class accuracy (as reported in Table 1) for TinyImageNetLT was usually close to 0."
        },
        {
            "heading": "E.7 Group DRO comparison",
            "text": "Sagawa et al. [30] propose a group DRO algorithm to improve long tail performance without distillation. In this section we present an experimental comparison to Algorithm 1 from Sagawa et al. [30]. This differs from our robust optimization methodology in Section 4 in two key ways: (i) we apply a margin-based surrogates of Menon et al. [23], and (ii) we use a validation set to update the Lagrange multipliers \u03bb in Algorithm 3. Table 5 shows results from running group DRO directly as specified in Algorithm 1 in Sagawa et al. [30], as well as a variant where we use the validation set to update Lagrange multipliers in group DRO (labeled as \u201cwith vali\u201d in the table). This comparison\nshows that Lrob is comparable to group DRO, and that robust distillation protocols can outperform group DRO alone."
        },
        {
            "heading": "E.8 CIFAR compressed students",
            "text": "To supplement the self-distilled results in Table 1, we provide Table 6 which gives results when distilling a larger teacher to a smaller student. As in Table 1, the combination with the best worst-class accuracy involved a robust student for CIFAR-10, CIFAR-10-LT, and CIFAR-100.\nE.9 ImageNet comparisons\nWe trained ResNet-34 teachers on ImageNet and report results of distillation to ResNet-34 and ResNet-18 students in Table 7. In Table 8, we report results of distilling a ResNet-34 teacher to a ResNet-34 student and to a ResNet-18 student on ImageNet-LT.\nE.10 Additional tradeoff plots\nSupplementing the tradeoff plot in the main paper (Figure 1), we provide the following set of plots illustrating the tradeoff between worst-case accuracy and balanced accuracy for different student architectures:\n\u2022 Long-tail datasets: self distillation (Figures 2, 3), and compressed student (Figures 1, 8). \u2022 Original balanced datasets: self distillation (Figures 5, 6), and compressed student (Figures\n7, 8)."
        }
    ],
    "title": "Robust Distillation for Worst-class Performance",
    "year": 2022
}