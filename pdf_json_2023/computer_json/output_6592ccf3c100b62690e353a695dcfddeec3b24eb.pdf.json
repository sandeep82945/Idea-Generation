{
    "abstractText": "The importance of neighborhood construction in local explanation methods has been already highlighted in the literature. And several attempts have been made to improve neighborhood quality for high-dimensional data, for example, texts, by adopting generative models. Although the generators produce more realistic samples, the intuitive sampling approaches in the existing solutions leave the latent space underexplored. To overcome this problem, our work, focusing on local modelagnostic explanations for text classifiers, proposes a progressive approximation approach that refines the neighborhood of a to-be-explained decision with a careful two-stage interpolation using counterfactuals as landmarks. We explicitly specify the two properties that should be satisfied by generative models, the reconstruction ability and the locality-preserving property, to guide the selection of generators for local explanation methods. Moreover, noticing the opacity of generative models during the study, we propose another method that implements progressive neighborhood approximation with probability-based editions as an alternative to the generator-based solution. The explanation results from both methods consist of word-level and instancelevel explanations benefiting from the realistic neighborhood. Through exhaustive experiments, we qualitatively and quantitatively demonstrate the effectiveness of the two proposed methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yi Cai"
        },
        {
            "affiliations": [],
            "name": "Arthur Zimek"
        },
        {
            "affiliations": [],
            "name": "Eirini Ntoutsi"
        },
        {
            "affiliations": [],
            "name": "Gerhard Wunder"
        }
    ],
    "id": "SP:41b2d1772bea477339687b9623b0611696ac4bfb",
    "references": [
        {
            "authors": [
                "Ameen Ali",
                "Thomas Schnake",
                "Oliver Eberle",
                "Gr\u00e9goire Montavon",
                "Klaus- Robert M\u00fcller",
                "Lior Wolf"
            ],
            "title": "Xai for transformers: better explanations through conservative propagation",
            "venue": "In International conference on machine learning,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Amini",
                "Ava P Soleimany",
                "Wilko Schwarting",
                "Sangeeta N Bhatia",
                "Daniela Rus"
            ],
            "title": "Uncovering and mitigating algorithmic bias through learned latent structure",
            "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2019
        },
        {
            "authors": [
                "Leila Arras",
                "Franziska Horn",
                "Gr\u00e9goire Montavon",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "Explaining predictions of non-linear classifiers in nlp",
            "venue": "In Proceedings of the 1st Workshop on Representation Learning for NLP,",
            "year": 2016
        },
        {
            "authors": [
                "Leila Arras",
                "Gr\u00e9goire Montavon",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "Explaining recurrent neural network predictions in sentiment analysis",
            "venue": "EMNLP 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Bach",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Frederick Klauschen",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "venue": "PloS one,",
            "year": 2015
        },
        {
            "authors": [
                "Liane Bernstein",
                "Alexander Sludds",
                "Ryan Hamerly",
                "Vivienne Sze",
                "Joel Emer",
                "Dirk Englund"
            ],
            "title": "Freely scalable and reconfigurable optical hardware for deep learning",
            "venue": "Scientific reports,",
            "year": 2021
        },
        {
            "authors": [
                "Umang Bhatt",
                "Alice Xiang",
                "Shubham Sharma",
                "Adrian Weller",
                "Ankur Taly",
                "Yunhan Jia",
                "Joydeep Ghosh",
                "Ruchir Puri",
                "Jos\u00e9 MF Moura",
                "Peter Eckersley"
            ],
            "title": "Explainable machine learning in deployment",
            "venue": "In Proceedings of the 2020 conference on fairness, accountability, and transparency,",
            "year": 2020
        },
        {
            "authors": [
                "Samuel Bowman",
                "Luke Vilnis",
                "Oriol Vinyals",
                "Andrew Dai",
                "Rafal Jozefowicz",
                "Samy Bengio"
            ],
            "title": "Generating sentences from a continuous space",
            "venue": "In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Yi Cai",
                "Arthur Zimek",
                "Gerhard Wunder",
                "Eirini Ntoutsi"
            ],
            "title": "Power of explanations: Towards automatic debiasing in hate speech detection",
            "venue": "IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA),",
            "year": 2022
        },
        {
            "authors": [
                "Yi Cai",
                "Arthur Zimek",
                "Eirini Ntoutsi"
            ],
            "title": "XPROAX-local explanations for text classification with progressive neighborhood approximation",
            "venue": "IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),",
            "year": 2021
        },
        {
            "authors": [
                "Diogo V Carvalho",
                "Eduardo M Pereira",
                "Jaime S Cardoso"
            ],
            "title": "Machine learning interpretability",
            "venue": "A survey on methods and metrics. Electronics,",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Chattopadhay",
                "Anirban Sarkar",
                "Prantik Howlader",
                "Vineeth N Balasubramanian"
            ],
            "title": "Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2018
        },
        {
            "authors": [
                "Hugh Chen",
                "Joseph D Janizek",
                "Scott Lundberg",
                "Su-In Lee"
            ],
            "title": "True to the model or true to the data",
            "venue": "In Workshop on Human Interpretability in Machine Learning (WHI)-International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Bin Dai",
                "Ziyu Wang",
                "David Wipf"
            ],
            "title": "The usual suspects? reassessing blame for vae posterior collapse",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Marina Danilevsky",
                "Kun Qian",
                "Ranit Aharonov",
                "Yannis Katsis",
                "Ban Kawas",
                "Prithviraj Sen"
            ],
            "title": "A survey of the state of explainable ai for natural language processing",
            "venue": "In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Dixon",
                "John Li",
                "Jeffrey Sorensen",
                "Nithum Thain",
                "Lucy Vasserman"
            ],
            "title": "Measuring and mitigating unintended bias in text classification",
            "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2018
        },
        {
            "authors": [
                "Christopher Frye",
                "Damien de Mijolla",
                "Tom Begley",
                "Laurence Cowton",
                "Megan Stanley",
                "Ilya Feige"
            ],
            "title": "Shapley explainability on the data manifold",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Damien Garreau",
                "Ulrike Luxburg"
            ],
            "title": "Explaining the explainer: A first theoretical analysis of lime",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Sahra Ghalebikesabi",
                "Lucile Ter-Minassian",
                "Karla DiazOrdaz",
                "Chris C Holmes"
            ],
            "title": "On locality of local explanation models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqiang Gong",
                "Ping Zhong",
                "Weidong Hu"
            ],
            "title": "Diversity in machine learning",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Bryce Goodman",
                "Seth Flaxman"
            ],
            "title": "European union regulations on algorithmic decision-making and a \u201cright to explanation",
            "venue": "AI magazine,",
            "year": 2017
        },
        {
            "authors": [
                "Riccardo Guidotti",
                "Anna Monreale",
                "Stan Matwin",
                "Dino Pedreschi"
            ],
            "title": "Black box explanation by learning image exemplars in the latent feature space",
            "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
            "year": 2019
        },
        {
            "authors": [
                "Riccardo Guidotti",
                "Anna Monreale",
                "Salvatore Ruggieri",
                "Dino Pedreschi",
                "Franco Turini",
                "Fosca Giannotti"
            ],
            "title": "Local rule-based explanations of black box decision systems",
            "venue": "arXiv preprint arXiv:1805.10820,",
            "year": 2018
        },
        {
            "authors": [
                "Riccardo Guidotti",
                "Anna Monreale",
                "Salvatore Ruggieri",
                "Franco Turini",
                "Fosca Giannotti",
                "Dino Pedreschi"
            ],
            "title": "A survey of methods for explaining black box models",
            "venue": "ACM computing surveys (CSUR),",
            "year": 2018
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "Po-Sen Huang",
                "Huan Zhang",
                "Ray Jiang",
                "Robert Stanforth",
                "Johannes Welbl",
                "Jack Rae",
                "Vishal Maini",
                "Dani Yogatama",
                "Pushmeet Kohli"
            ],
            "title": "Reducing sentiment bias in language models via counterfactual evaluation",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Vasileios Iosifidis",
                "Eirini Ntoutsi"
            ],
            "title": "Sentiment analysis on big sparse data streams with limited labels",
            "venue": "Knowledge and Information Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Jurafsky"
            ],
            "title": "Speech & language processing",
            "venue": "Pearson Education India,",
            "year": 2000
        },
        {
            "authors": [
                "Brendan Kennedy",
                "Xisen Jin",
                "Aida Mostafazadeh Davani",
                "Morteza Dehghani",
                "Xiang Ren"
            ],
            "title": "Contextualizing hate speech classifiers with post-hoc explanation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Orestis Lampridis",
                "Riccardo Guidotti",
                "Salvatore Ruggieri"
            ],
            "title": "Explaining sentiment classification with synthetic exemplars and counterexemplars",
            "venue": "In International Conference on Discovery Science,",
            "year": 2020
        },
        {
            "authors": [
                "Thibault Laugel",
                "Xavier Renard",
                "Marie-Jeanne Lesot",
                "Christophe Marsala",
                "Marcin Detyniecki"
            ],
            "title": "Defining locality for surrogates in post-hoc interpretablity",
            "venue": "In Workshop on Human Interpretability in Machine Learning (WHI)-International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Piyawat Lertvittayakumjorn",
                "Francesca Toni"
            ],
            "title": "Human-grounded evaluations of explanation methods for text classification",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Pantelis Linardatos",
                "Vasilis Papastefanopoulos",
                "Sotiris Kotsiantis"
            ],
            "title": "Explainable ai: A review of machine learning interpretability methods",
            "year": 2020
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Gabriel Erion",
                "Hugh Chen",
                "Alex DeGrave",
                "Jordan M Prutkin",
                "Bala Nair",
                "Ronit Katz",
                "Jonathan Himmelfarb",
                "Nisha Bansal",
                "Su-In Lee"
            ],
            "title": "From local explanations to global understanding with explainable ai for trees",
            "venue": "Nature machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ji Ma",
                "Jingjiao Li",
                "Zhenni Li",
                "Jiao Jiao"
            ],
            "title": "Salient region detection by integrating intrinsic and extrinsic cues without prior information",
            "venue": "Journal of Engineering Science & Technology Review,",
            "year": 2017
        },
        {
            "authors": [
                "Alireza Makhzani",
                "Jonathon Shlens",
                "Navdeep Jaitly",
                "Ian Goodfellow",
                "Brendan Frey"
            ],
            "title": "Adversarial autoencoders",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Christopher D Manning"
            ],
            "title": "Introduction to information retrieval",
            "venue": "Syngress Publishing,",
            "year": 2008
        },
        {
            "authors": [
                "Julian McAuley",
                "Jure Leskovec"
            ],
            "title": "Hidden factors and hidden topics: understanding rating dimensions with review text",
            "venue": "In Proceedings of the 7th ACM conference on Recommender systems,",
            "year": 2013
        },
        {
            "authors": [
                "Gr\u00e9goire Montavon",
                "Alexander Binder",
                "Sebastian Lapuschkin",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Layer-wise relevance propagation: an overview",
            "venue": "Explainable AI: interpreting, explaining and visualizing deep learning,",
            "year": 2019
        },
        {
            "authors": [
                "Gr\u00e9goire Montavon",
                "Sebastian Lapuschkin",
                "Alexander Binder",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
            "venue": "Pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Meike Nauta",
                "Jan Trienes",
                "Shreyasi Pathak",
                "Elisa Nguyen",
                "Michelle Peters",
                "Yasmin Schmitt",
                "J\u00f6rg Schl\u00f6tterer",
                "Maurice van Keulen",
                "Christin Seifert"
            ],
            "title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai",
            "venue": "arXiv preprint arXiv:2201.08164,",
            "year": 2022
        },
        {
            "authors": [
                "Vitali Petsiuk",
                "Abir Das",
                "Kate Saenko"
            ],
            "title": "Rise: Randomized input sampling for explanation of black-box models",
            "venue": "In British Machine Vision Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Gregory Plumb",
                "Denali Molitor",
                "Ameet Talwalkar"
            ],
            "title": "Model agnostic supervised local explanations",
            "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Samira Pouyanfar",
                "Saad Sadiq",
                "Yilin Yan",
                "Haiman Tian",
                "Yudong Tao",
                "Maria Presa Reyes",
                "Mei-Ling Shyu",
                "Shu-Ching Chen",
                "Sundaraja S Iyengar"
            ],
            "title": "A survey on deep learning: Algorithms, techniques, and applications",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2018
        },
        {
            "authors": [
                "Isha Puri",
                "Amit Dhurandhar",
                "Tejaswini Pedapati",
                "Karthikeyan Shanmugam",
                "Dennis Wei",
                "Kush R Varshney"
            ],
            "title": "Cofrnets: interpretable neural architecture inspired by continued fractions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": " why should i trust you?\u201d explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Anchors: High-precision model-agnostic explanations",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Marko Robnik-\u0160ikonja",
                "Marko Bohanec"
            ],
            "title": "Perturbation-based explanations of prediction models",
            "venue": "In Human and machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Jason Naradowsky",
                "Brian Leonard",
                "Benjamin Van Durme"
            ],
            "title": "Gender bias in coreference resolution",
            "venue": "In Proceedings of NAACL-HLT,",
            "year": 2018
        },
        {
            "authors": [
                "Wojciech Samek",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Sebastian Lapuschkin",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Evaluating the visualization of what a deep neural network has learned",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Sharath M Shankaranarayana",
                "Davor Runje"
            ],
            "title": "Alime: Autoencoder based approach for local interpretability",
            "venue": "In International conference on intelligent data engineering and automated learning,",
            "year": 2019
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Style transfer from non-parallel text by cross-alignment",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Jonas Mueller",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Educating text autoencoders: Latent representation guidance via denoising",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng"
            ],
            "title": "Societal biases in language generation: Progress and challenges",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje"
            ],
            "title": "Learning important features through propagating activation differences",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Wilson Silva",
                "Kelwin Fernandes",
                "Maria J Cardoso",
                "Jaime S Cardoso"
            ],
            "title": "Towards complementary explanations using deep neural networks",
            "venue": "In Understanding and Interpreting Machine Learning in Medical Image Computing Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Tianbao Song",
                "Jingbo Sun",
                "Bo Chen",
                "Weiming Peng",
                "Jihua Song"
            ],
            "title": "Latent space expanded variational autoencoder for sentence generation",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Erik Strumbelj",
                "Igor Kononenko"
            ],
            "title": "An efficient explanation of individual classifications using game theory",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Erik \u0160trumbelj",
                "Igor Kononenko"
            ],
            "title": "Explaining prediction models and individual predictions with feature contributions",
            "venue": "Knowledge and information systems,",
            "year": 2014
        },
        {
            "authors": [
                "Erik \u0160trumbelj",
                "Igor Kononenko",
                "M Robnik \u0160ikonja"
            ],
            "title": "Explaining instance classifications with interactions of subsets of feature values",
            "venue": "Data & Knowledge Engineering,",
            "year": 2009
        },
        {
            "authors": [
                "Meng Wang",
                "Janusz Konrad",
                "Prakash Ishwar",
                "Kevin Jing",
                "Henry Rowley"
            ],
            "title": "Image saliency: From intrinsic to extrinsic context",
            "year": 2011
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems- Volume",
            "year": 2015
        },
        {
            "authors": [
                "Junbo Zhao",
                "Yoon Kim",
                "Kelly Zhang",
                "Alexander Rush",
                "Yann LeCun"
            ],
            "title": "Adversarially regularized autoencoders",
            "venue": "In International conference on machine learning,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Explainable AI, Local explanation, Counterfactual, Neighborhood approximation, Text classification\nI. INTRODUCTION\nAdvances in machine learning, deep learning in particular, prompt their applications in diverse real-world scenarios over past decades [45]. However, the rising model complexity and the exploding number of parameters [6] keep drawing concerns over the transparency [21] of the decision-making process for the systems steered by artificial intelligence. Explainable AI (XAI) hence becomes a popular aspect that strives to uncover the underlying behaviors of black boxes wrapped by AI.\nDespite the endeavor to explain models operating tabular data [35], [44], [47] and image data [12], [43], [52], most of them are not directly applicable to text classifiers owing to the unstructured nature of texts. The relatively little attention\nleaves the field underdeveloped. Thus, the main focus of this paper is to develop local model-agnostic explanation methods for text classifiers. Approaches falling into this category first seek an interpretable local approximation of the explaining target with a simpler and, in most cases, linear surrogate model, then extract knowledge from the transparent local predictor as explanations for specific decisions instead of the whole black box because of its limited capacity. A synthetic set (known as the neighborhood) consisting of samples close to the inquiry will be constructed during the explanation procedure for training the surrogate model in question. Labeled by the black box, the synthetic data should reflect its behaviors. In practice, the quality of the neighborhood dominates the upper bound of explanation quality.\nTo generate neighboring instances, perturbation of inputs [60], [62] is a common choice. It modifies the values of numeric features within a certain range or switches categorical features to some other values. Again, the same task becomes much more challenging while dealing with textual data due to the lack of formal definitions of neighboring texts. Some existing works [47], [48] generate neighbors by randomly dropping words from the given text. But perturbations ignoring inherent connections among the features (words) result in incomplete instances, which drag the focus of explanation out of the data manifold. XSPELLS [30] proposes to generate neighboring texts in a latent space learned through a generative autoencoder. Empowered by the generative model, it generates more realistic (semantically meaningful and grammatically correct) instances in comparison to the former approaches. However, both groups of methods apply random sampling regardless of the construction space (either the original or latent), which leads to a spongy neighborhood.\nIn this work, we propose two explanation methods1 for text classifiers. We first introduce XPROA (local eXplainer with\n1The source code is available at https://github.com/caiy0220/XPROA-B.\nar X\niv :2\n30 2.\n07 73\n3v 1\n[ cs\n.C L\n] 1\n1 Fe\nPROgressive neighborhood Approximation), a local explanation method for text classifiers that implements neighborhood construction through a two-staged progressive interpolation relying on a generative model. XPROA enhances explanation qualities by refining the neighborhood, but the additional opacity of the deployed generator powered by neural networks casts a shadow over the explanation process. Originating from this concern, we further propose XPROB (local eXplainer with PRObability-Based edition), which approximates the neighborhood through iterative editions on prototypes in a transparent manner. As an extension of the conference paper [10], the main contributions of the work are summarized below: \u2022 We explicitly specify two essential properties of gener-\native models deployed for neighborhood construction in local explanations, namely the reconstruction ability and the locality-preserving property. \u2022 We propose XPROB, an explanation method that constructs the neighborhood transparently through iterative editions on prototypes guided by local n-gram context as an alternative to deploying generative models. \u2022 We design an experiment that quantitatively evaluates the stability of explanation methods. \u2022 The experimental results of dependency analysis expose the strong reliance of XPROA on the generators, which underlines the concern about the opaque explanation process of generator-based explanation solutions.\nBesides, this version refines the experiment design of quantitative explanation evaluation in the previous work for better alignment with the three Cs of interpretability, namely correctness, completeness, and compactness.\nThe rest of the paper is organized as follows. In Section II, we discuss state-of-the-art works regarding explainability. Sections III and IV introduce the proposed XPROA and XPROB, respectively. Afterward, Section V details the extraction of explanations from the surrogate model trained on the constructed neighborhood. Exhaustive experiments are conducted in Section VI for qualitatively and quantitatively evaluating the proposed methods. Finally, we conclude the paper in Section VII."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "The explainability of machine learning, especially deep learning, is drawing growing attention [15], [33]. The existing solutions in this domain can be categorized as either global or local explanation methods depending on whether or not the explanation output is input-specific. The former methods explain the target model as a whole at a global scope. They provide model-level insights into the internal functions [46] and reveal the general correlations among features and the label [34]. In contrast, the local ones analyze specific decisions of the target in detail, which leads to concrete and precise local explanations [35], [47].\nAs an alternative to the previous taxonomy, XAI approaches could be divided into the categories of model-specific and model-agnostic according to the access required on the tobe-explained model. Model-specific methods generate expla-\nnations through in-depth analysis of the inner logic of the target [40], for which full access to the model is a prerequisite. Benefiting from the access, they could efficiently deliver faithful explanations. But the analysis procedure is highly specified since models handle data in diverse ways. For example, existing model-agnostic approaches for neural networks investigate the information flow through the neurons for understanding the decision-making process [41], [57]. Although neural networks in general share a basic idea, a single explainer from this category is usually eligible for a smaller subset of them. LRP [5] is designed for analyzing the information flow on the dense and convolutional layers. And adjustments to the backpropagation rules are mandatory before being applied to RNN [4] and attention mechanisms [1].\nBy comparison, model-agnostic methods only require permission to access the input and output of a model. In principle, approaches of this kind are applicable to arbitrary black boxes [33]. The loosened requirement makes them more flexible in security-sensitive scenarios, where models are opaque to outsiders (e.g., explanation methods). Another difference between model-specific and model-agnostic methods is that the latter reveals the decision-making process indirectly through analysis of the causal relationship between the input features and the final outputs. Neighborhood construction of the inquired instance comes in first of the analysis process. A neighborhood consists of the synthetic data spatially close to the instance, on which the decision has been made. The target model assigns labels to the generated neighboring instances to form a fully labeled dataset that reflects its local behavior. The weights of the training sample could either be uniform [31] or in a distance-based manner [18]. Given the synthetic dataset, a surrogate model, chosen to be inherently transparent, will then be trained to mimic the local behaviors of the black box. This local predictor will then serve as a proxy for deriving explanations.\nAs the first step, the neighborhood construction process plays a key role in model-agnostic explanation methods as it massively influences explanation quality. The most intuitive solution is to perform a simple random perturbation [60], which alters feature values randomly within a predefined range. It iterates the slight fluctuation in the vicinity of the given input until the method collects sufficiently many neighbors. Although this simple approach generates satisfactory neighbors for most data representations, it requires finetuning for more complicated forms of data, such as textual data in our case. One difficulty of applying perturbations to text is how to modify the value of each feature (word). LIME [47] and SHAP [35] suggest word dropping \u2013 randomly removing/masking words from the input text. The solution is truly intuitive and has been shown to be effective. However, there still remain several problems: \u2022 The assumption of feature independence [61] does not\nhold for texts because of syntax and semantics, random dropping results in incomplete and meaningless entries. \u2022 The odd neighbors could shift the concentration away from the data manifold [13], [19], where the explanation\nresults might be unrepresentative of the model behaviors [17]. \u2022 Perturbations are strictly limited by the length of the text, which becomes crucial when the input contains only a handful of words since the surrogate model will not have enough samples to approximate the black box.\nAlternatively, XSPELLS [30] proposes to generate neighbors of texts with a variational autoencoder [8]. Preceding XSPELLS, ALIME [53] and ABELE [22] already deploy autoencoders for explaining models dealing with tabular data and images, respectively. ALIME still generates neighbors in the original feature space, and the autoencoder only serves as part of the weighting function for measuring instance similarity. But the neighborhood construction process of ABELE fully relies on the deployed adversarial autoencoder [37], a generator of synthetic images. XSPELLS fine-tunes the framework of ABELE to tackle the task of neighborhood generation for texts. It constructs the neighborhood of an input text through random sampling in the latent space. This way, it addresses the limitation of word dropping by producing more realistic neighboring instances with a generative model. However, sampling randomly cannot efficiently and effectively explore a high-dimensional latent space. Moreover, its concrete choice of the generative model misses the two desired features of the generator for neighborhood construction under locality constraint [19], [49], namely the reconstruction ability and the locality-preserving mapping.\nWe address the limitations in the previous works with the proposed XPROA. Although our method also adopts a generative autoencoder and achieves outstanding performance, we argue that the high dependency of explanation modules on opaque components, in this case, the generators, deserves further discussion. Another solution \u2013 XPROB \u2013 from us shows that competitive performance can be achieved without involving extra obscurity."
        },
        {
            "heading": "III. PROGRESSIVE NEIGHBORHOOD APPROXIMATION WITH",
            "text": ""
        },
        {
            "heading": "A GENERATIVE AUTOENCODER",
            "text": "We introduce in this section the proposed explanation method XPROA. Given b(\u00b7) as the function of a model to be explained, which takes a query text x as the input and returns a class label2 y\u0302 = b(x) as the classification result, the goal of the explainer is to find an explanation \u03be for the decision made by the target, i.e., b(x). Here we consider the model a black box, whose learnable parameters describing the function b(\u00b7) are wrapped and thus not accessible, but new queries on the model are unlimited.\nAs a model-agnostic method, XPROA extracts explanations of the decision-making process indirectly through explainable approximations of the black box, which refers to a transparent surrogate model m(\u00b7) trained on a synthetic dataset N that reflects the behavior of the target. But the self-explainable constraint limits the capacity of such a surrogate. In order for the weak learner m(\u00b7) to be on par with the complex\n2For simplicity, we assume a binary classification problem: y \u2208 {+,\u2212}.\nmodel b(\u00b7), the synthetic instances N should carefully address the local neighborhood of an inquiry x, where a faithful approximation is achievable. The requirement of generating spatially close samples as a compromise with the limited surrogate competence is called the locality constraint."
        },
        {
            "heading": "A. Locality-preserving neighborhood generation",
            "text": "Constructing qualified neighborhoods N is the main challenge for explaining text classifiers. Previous attempts, such as word dropping [47] and corruption [27], deliver instances with semantic and syntactic errors falling out of the manifold. For generating more realistic neighbors aligning the manifold, we utilize a generative autoencoder for the construction process.\nA generative autoencoder G consists of a deterministic encoder E : X \u2192 Z that maps from the text space to the latent space and a probabilistic decoder D : Z \u2192 X reconstructing word sequences from their latent representations. To prepare the generator for explanations, the autoencoder G is trained in an unsupervised manner upon data XG \u2208 X , which are sampled from the same domain as those fed to the black box (but please note that XG does not necessarily have intersections with the training set X \u2208 X for the classifier). During neighborhood construction, the prepared generator G first maps the target instance x to the latent space with the encoder and obtains its latent representation z = E(x). Once encoding finishes, a set of neighboring vectors Z is generated through manipulations on the pivot point3 z in the latent space, which are analogous to the process for numerical data. After the latent space manipulations, the neighborhood N of the inquiry x for training the surrogate model can be derived using the decoder N = D(Z). With the prepared generative model, we enhance the semantic and syntax correctness of the neighboring texts.\nThe essential assumption of generators leveraged in neighborhood construction is the satisfaction of the reconstruction ability and locality-preserving property. The former allows the generator to concentrate on the targeted instances. And the latter means that the generator should map similar texts to nearby latent vectors, which ensures that neighbors discovered in the latent space are close to the inquiry in text form. However, not all models possess the locality-preserving property [65]. The risks of violating the locality constraint make them unsuitable for this scenario. The generator adopted by this work, DAAE [55], learns locality-preserving latent representations by augmenting an adversarial autoencoder with a denoising objective, that it reconstructs original texts from their perturbed version. Its adequately organized latent space geometry enables the generation of high-quality and similar synthetic texts for training the surrogate. Besides, DAAE accomplishes the reconstruction task while VAE sacrifices its reconstruction ability due to the KL regularization term [14], [59]. Being capable of reconstructing given inputs allows our explanation method to correctly locate the neighborhood given by the input x rather than some others.\n3The latent space representation of the input x.\nAlgorithm 1 Neighborhood construction Input: x: query instance; L: landmark set Output: N : neighborhood of x\n1: C = \u2205 2: while not terminate() do 3: Cnew , Lnew = Approximate(L, x) 4: C = C \u222a Cnew 5: L = Lnew 6: end while 7: C = RemoveDuplicates(C) 8: N = Closest(C, n, n) # output the n closest instances for both classes 9: return N"
        },
        {
            "heading": "B. Neighborhood generation with progressive boundary approximation",
            "text": "Another main factor of latent space neighborhood construction is the manipulation strategy. The choices range from pure random perturbation, locality-based sampling, and genetic algorithms to the proposed progressive neighborhood approximation. Pure random perturbation [30], [35], [47] is the most intuitive solution, but it cannot produce compact neighborhoods and thus fails to focus on local decisions. Localitybased sampling [31] addresses the problem by constraining sampling within a fixed-size hyper-sphere as specified by the closest counterfactual to the query instance x. But since instances within the sphere are still sampled uniformly, the constructed set may not highlight the decision boundary.\nThe usage of a genetic algorithm [23] contributes to approaching local decision boundaries; however, the convergence guided by the fitness function harms the variety of the synthetic set. Moreover, its time cost is exceptionally higher than other solutions due to the genetic algorithm, which could limit its practical applications. To overcome the limitations of the sampling approaches listed above and better approximate the local data structure surrounding x, we propose a progressive approximation of the neighborhood using landmarks to delimit the neighborhood boundary and by carefully interpolating within this boundary.\nThe proposed approach (Algorithm 1) constructs the neighborhood for a query through an iterative process, which achieves better neighborhood approximations over iterations as closer counterfactuals to x are discovered and exploited. Different from the previous attempts, which determine the neighborhood with a fixed hyperparameter, we initialize neighborhoods with landmarks from real data. The initial landmarks are counterfactual instances from a retained corpus XL, which is down-sampled from the generator training set XL \u2286 XG. They help the construction process adapt to non-uniform data distribution. More specifically, we use the k-closest counterfactuals (the instances predicted by b(\u00b7) as of the opposite label of x) in the corpus XL depending on their latent distance to the pivot to initialize the landmark set L. Thereafter, the neighborhood approximation (line 3) is performed iteratively\non L with two-staged interpolations until the termination criterion is satisfied (for example, when the approximation finds no closer counterfactual in successive rounds or the repeating count reaches its limitation). In each iteration, the interpolated instances are archived in the candidate pool C (line 4), and the landmark set is updated (line 5) with the newly generated counterfactuals (a subset of Cnew ). Once the approximation terminates, we finalize the neighborhood N containing 2n samples by picking the n closest factuals and n closest counterfactuals from the candidate pool without duplication (lines 7, 8).\nInterpolation defined by (1) is a commonly used operation in autoencoder-based text generation [8], [59], [65]:\nI(zp, zq) = {zi | zi = zp + i \u00b7 (zp \u2212 zq) s+ 1 , 0 \u2264 i \u2264 s} (1) where s \u2265 0 is the number of interpolations between two poles. The operation on latent vectors implements successive and meaningful text manipulations based on two selected prototypes \u2013 zp and zq . The neighborhood approximation described in Algorithm 2 is dominated by the two-staged interpolation, which consists of: i) interpolation between pairs from the landmark set L to allow for the exploration of the neighborhood and the variety of counterfactuals; ii) interpolation between counterfactuals and the pivot point z to allow for a more localized approximation of the decision boundary.\nThe approximation process takes as input the current landmark set L and returns the updated landmarks Lnew along with the newly generated neighboring instances Cnew . The first stage (line 4) interpolates between a pair of randomly selected counterfactuals from L (line 3). The set of interpolated points Z \u2032 from the first stage is labeled with the black box y\u0302i = b(D(zi)). In the second stage, we interpolate between two sentiment polarities, namely between the target point z and every instance zi \u2208 Z \u2032 that satisfies b(D(zi)) 6= b(x) (lines 7, 8). All synthetic instances derived in this stage are decoded and added to the candidate pool for the final selection (line 11). The set Lnew absorbs the closest counterfactuals from the newly generated candidates as landmarks for the next iteration (line 12). The process repeats for k times (lines 2, 13) to better explore the local space. Note that the hyperparameter k is the one for the initialization of the landmark set, so it remains the same size over iterations.\nThrough the iterative process, an optimal set of neighboring instances is discovered. The constructed neighborhood N achieves higher diversity with the first stage interpolation and highlights the decision boundary between the pivot point and counterfactuals through the second stage."
        },
        {
            "heading": "IV. NEIGHBORHOOD APPROXIMATION WITH PROBABILITY-BASED EDITION",
            "text": "Although the generative autoencoder contributes to more realistic neighboring texts, building explanations upon another opaque model (current autoencoders are implemented mainly by neural networks with non-linear functions, which are usually considered not self-explainable) brings additional obscurity to the explanation process and limits the trustworthiness\nAlgorithm 2 Neighborhood approximation Input: x: query instance; L: landmark set Output: Cnew : new neighbors; Lnew : updated landmarks\n1: Lnew , Cnew = \u2205,\u2205 2: repeat 3: zp, zq = RandomlyDraw(L, 2) 4: Z \u2032 = I(zp, zq) # 1st interpolation: between landmarks 5: Z = \u2205 6: for zi \u2208 Z \u2032 do # 2nd interpolation: between poles 7: if b(D(zi)) 6= b(x) then 8: Z = Z \u222a I(zi, E(x)) 9: end if\n10: end for 11: Cnew = Cnew \u222aD(Z) 12: Lnew .add(ClosestCounterfactual(Z)) 13: until k times 14: return Cnew , Lnew\nof the results. Moreover, as demonstrated by our experiment (Section VI-F), the quality of explanations depends heavily on the concrete choices of the generator. The sensitivity to the opaque generative models makes the generator-based explanation methods vulnerable. To this end, we implement an alternative of the neighborhood approximation procedure with fully transparent probability-based editions \u2013 XPROB, which still generate neighbors creatively in contrast to word dropping."
        },
        {
            "heading": "A. Prototypes selection",
            "text": "The general idea of XPROB is to edit several counterfactual prototypes in a transparent and controllable way that progressively approximates the target instance. Variants created through the editions form the neighborhood N for surrogate training. Inspired by the usage of landmarks for initializing the latent neighborhood in Section III-B, we also adopt a corpus XL \u2286 X for prototype selection. The prototypes are the top-k counterfactuals according to their closeness to the input text. There are multiple options for the metric of text similarity. But to get rid of the opaque latent representations, we measure the proximity of text pairs by the cosine distance of their tf-idf vectors [38] with the vectorizer fitted on XL. Albeit the tf-idf representation biases towards less frequent terms, it helps balance the distribution of over-represented words (e.g., stopwords) and those under-represented.\nDuring the implementation, we use the identical XL for both proposed methods. The reason for naming the samples from the same set differently, namely prototype and landmark, is to distinguish the editions conducted by the two methods, where XPROB applies changes to words directly in the original textual form while XPROA performs arithmetic operations in the latent space."
        },
        {
            "heading": "B. Probability-based edition",
            "text": "Given a prototype x\u0302 = (w0, w1, ..., wl) and a word w\u2217, the probability-based edition finds the best fit for w\u2217 under the context of the prototype. In other words, it seeks an operation for integrating w\u2217 into x\u0302, which maximizes the likelihood of the manipulated version:\narg max i,j\nP (x\u0302i0, w \u2217, x\u0302l+1j ) (2)\nwhere x\u0302ji = (wi, ..., wj\u22121) is a contiguous subsequence of the text x\u0302 (denoted by x\u0302ji \u227a x\u0302) with its subscript and superscript indicating the indices of the starting and ending words, respectively. The manipulation operation defined by i and j, which satisfy 0 \u2264 i \u2264 j \u2264 l + 1, could be either an insertion at position i when i = j or a replacement of the subsequence x\u0302ji otherwise.\nInstead of taking the whole sequence into account, we concentrate on the local context described by n-gram [28]. The local context consists of the preceding and succeeding words. Since the context is given by the prototype, the goal becomes:\narg max i,j\nPpre(w \u2217|x\u0302ii\u2212n)Psuc(w\u2217|x\u0302 j+n j ) (3)\nwhich is solvable in quadratic time. Here, Ppre and Psuc denote the conditional probability of w\u2217 given the preceding and succeeding tokens, respectively:\nPpre(w \u2217|x\u0302ji ) =\nP (x\u0302ji , w \u2217)\nP (x\u0302ji ) , Psuc(w\n\u2217|x\u0302ji ) = P (w\u2217, x\u0302ji )\nP (x\u0302ji )\nWe estimate the conditional probabilities on the same corpus where the prototypes originate:\nPpre(w \u2217|x\u0302ii\u2212n) = |{x|x \u2208 XL, (x\u0302ii\u2212n, w\u2217) \u227a xkl }| |{x|x \u2208 XL, x\u0302ii\u2212n \u227a x}|\n(4)\nPsuc(w \u2217|x\u0302j+nj ) = |{x|x \u2208 XL, (w\u2217, x\u0302j+nj ) \u227a x}| |{x|x \u2208 XL, x\u0302j+nj \u227a x}|\n(5)\nThe number of tokens n defining the range of local context balances the trade-off between the correctness and creativity of the edition. Due to the limited corpus size, here we choose unigram (n = 1) to simplify the generation process. In practice, we take a minimal value of = 1|XL|+1 for Ppre and Psuc instead of 0 to avoid the vanishment of probabilities.\nFurthermore, to allow insertion and replacement at the beginning/end of a text, n padding tokens \u2329PAD\u232a are appended separately to the head and tail of the input. The padding length relies on the range of local context defined by n-gram. For unigram, the padding adds one on each end. However, we find that the edition becomes biased toward the padding token for its significantly frequent appearance (possessed by all entries) over the dataset and consequently prefers to replace the entire prototype with the target word, i.e., i = 0, j = l + 1. This abrupt change conflicts with the intention of applying gradual\nAlgorithm 3 Iterative probability-based edition Input: x: query instance; Output: N : neighborhood of x;\n1: L = initPrototypes(x,XL) 2: repeat 3: Lnew = \u2205 4: for w\u2217 in x do 5: N \u2032 = editOnPrototypes(w\u2217, L, d k|x|e)) 6: Lnew = Lnew \u222aN \u2032 7: end for 8: N = N \u222a Lnew 9: L = Lnew\n10: until enough instances in N 11: return N\nchanges. Thus, we update the goal with a penalty based on the edited length of the text:\narg max i,j\n1\nej\u2212i Ppre(w\n\u2217|x\u0302ii\u2212n)Psuc(w\u2217|x\u0302 j+n j ) (6)\nThe soft constraint on edition cost encourages the selection of operations that affect fewer prototype components at each step."
        },
        {
            "heading": "C. Neighborhood approximation through iterative edition",
            "text": "With the probability-based edition constrained by edition length, we can gradually approach the target sentence through iterative manipulation on prototypes with words w\u2217 \u2208 x (Algorithm 3). The prototype pool is initialized by the kclosest counterfactuals from the corpus (line 1) and updated by the edited version repeatedly (lines 6, 9). To balance the distributions of the ingredients in x over the constructed neighborhood, we generate equally for each word d k|x|e variants through editions on randomly chosen prototypes at a step (lines 4, 5). During the iterative process, editions on prototypes are determined greedily according to the given ingredients as defined by (6). The neighborhood set N absorbs the newly generated variants from each iteration (lines 6, 8) with the repeated instances discarded. The process repeats until enough neighboring texts have been collected (lines 2, 10).\nFor better coverage of neighborhoods, we try to edit all prototypes with every token in x and keep the additional variants (for each token, the variants generated after the objective d k|x|e has been met) as candidates in our implementation. They will be sampled as complements in case of early convergence that editions of the current epoch reach the same outcome and not enough prototypes have been gathered for the next epoch."
        },
        {
            "heading": "V. EXTRACTION OF LOCAL EXPLANATIONS",
            "text": "Explanation extraction from the neighborhood N is independent of the neighborhood construction details. Once the neighborhood N is ready, the black box b(\u00b7) labels all instances in the set. This fully labeled synthetic dataset reflecting the black box local behavior helps extract information for explanations. In particular, we propose two explanation\ncomponents: i) word-level importance (Section V-A) and ii) factual and counterfactual instances identified during the neighborhood approximation process (Section V-B)."
        },
        {
            "heading": "A. Word-level explanation",
            "text": "First, a surrogate model m(\u00b7) is trained upon the labeled neighborhood N . There are multiple choices for the surrogate model as long as the self-explainable requirement is satisfied. We adopt a linear regression model for easier quantification of feature attribution. Unlike XSPELLS [30], we prefer building the surrogate in the original text space over the latent since the latter organized by a generator is less accessible for human understanding. Considering the limited capacity of a linear regressor, texts are presented by bag-of-words (for the local vocabulary of N ) in order to simplify the complexity of the classification task. Sequential information vanishes as the trade-off between integrity and feasibility.\nThe surrogate is trained on N with the weighted square loss function defined as follows:\nL(N, x) = \u2211 xi\u2208N exp( \u2212dC (xi, x)2 \u03c32 ) \u00b7 (b(xi)\u2212m(xi))2 (7)\nThe first term in (7) computes the weight of a synthetic text xi based on its cosine distance dC to the target and the kernel width \u03c3, which is a hyper-parameter that controls the influence of the weighting function. And the second term is the square loss of the surrogate model m(\u00b7) on simulating the local behavior of the black box b(\u00b7).\nAfter training, the weights of features learned through regression are derived as feature attributions. In addition to the input words, those introduced by the creative neighborhood construction are also rated. Therefore, we further split the word-level explanation into two parts: i) intrinsic word refers to words originating from the input; ii) extrinsic word indicates external words involved during the generation process. Feature attribution quantifies the importance of the corresponding feature to the inference process. For intrinsic words, intuitively, the value reveals feature contribution to the inquired decision. And extrinsic features from the neighborhood are incorporated to facilitate further explanation of the decision [36], [63]. Extrinsic words with high attributions indicate important observations for inference in the same or similar contexts. Analogous to word deletion for intrinsic words, edition with extrinsic words could also illustrate the potential impacts of the corresponding features on the classification results via the resulting manual factual and counterfactual instances."
        },
        {
            "heading": "B. Factuals and Counterfactuals as explanation",
            "text": "Benefiting from the realistic neighboring texts, the synthetic samples themselves could also contribute to the understanding process as (counter-)factuals. Analysis of the intersections and differences between factual and counterfactual instances would highlight the irrelevant and crucial components for the predictions. Contrary to greedily picking the closest instances to the target, which results in a homogeneous (counter-)factual\nset, our selection process considers diversity on top of closeness. A diverse (counter-)factual set can better underscore the most contributing ingredients with a limited number of samples, which tend to be shared by factuals and altered in counterfactuals.\nWe construct the factual and counterfactual sets separately. To form the counterfactual set \u039e, we pick the neighboring text successively until sufficiently many have been gathered. Each time, the remaining neighbors are sorted in descending order according to their scores calculated by (8), and the set \u039e absorbs the best instance.\nsi = \u03bb(1\u2212 dC (xi, x)) + (1\u2212 \u03bb) xp 6=xq\u2211 \u039e\u222a{xi} dC (\u2206xp,\u2206xq) (|\u039e|2 + |\u039e|)/2\n(where \u2206xp = xp \u2212 x) (8)\nThe score of a candidate is a linear combination of its cosine similarity to the target x and the normalized diversity [20] of the counterfactual set if \u039e takes xi. The parameter \u03bb \u2208 [0, 1] steers the relative importance of diversity during the selection. For target instances that locate distant from the origin, diversities of vectors from their neighborhoods can be subtle. Therefore, we take the measure on their differences to the target x denoted by \u2206x\u2032. The ranking for the candidates has to be updated whenever \u039e changes (after the acceptance of the new member at the end of each step), which explains the reason for picking samples in a successive manner. The same process is also applied for preparing factuals."
        },
        {
            "heading": "VI. EVALUATION",
            "text": "We first show examples of explanations from different methods for qualitative comparison (Section VI-B). The primary objective of the experiments is to quantitatively evaluate the effectiveness of explanations in terms of correctness, completeness, and compactness (Section VI-C), followed by the verification of explanation stability through texts under similar contexts (Section VI-D). Afterward, the sensitivity analysis regarding the major hyperparameters used throughout the tests is shown in Section VI-E. Lastly, due to the concerns of adopting a black box (the generative model) for explaining another model, we design an experiment for analyzing the dependency on external resources, which exposes the potential risk of leveraging generative models in explanations (Section VI-F). Details about the experimental setting follow in the upcoming section."
        },
        {
            "heading": "A. Experimental setup",
            "text": "Datasets: We evaluated the approaches on two real-world datasets: Yelp reviews [54] and Amazon reviews polarity [64]. The Yelp dataset consists of restaurant reviews, with a maximal text length equal to 16. Possible labels of the reviews are either positive or negative. The Amazon review polarity dataset contains customer reviews about different products with ratings ranging from 1 to 5 stars [39]. A succeeding work [64] discarded all neutral reviews (rated as 3 stars) and\nre-defined the reviews with \u2264 2 stars (\u2265 4 stars) as negative (positive, respectively) for matching a binary classification task. Compromising with the constrained capacity of the generative model used in the proposed framework, we focused on short texts and only used review titles in our experiments. We split both datasets into four disjoint subsets with sizes 200K/20K/2K/4K, corresponding to the training set for the generative model and the training/validation/test set for the black box. Besides, we downsampled 20K instances uniformly from the generator training set XG as the corpus XL for the initialization of the landmark (prototype) set. Although the generator training set is notably (mostly 10 times) larger than the remaining for ensuring the quality of the generator, the setting is feasible in practice as the adopted generative autoencoder is trained in an unsupervised manner.\nText classifiers: In principle, a model-agnostic explainer is applicable to arbitrary black box b(\u00b7). And for the sake of the evaluation, we trained a Random Forest (RF) and a Deep Neural Network (DNN), which are often referred to as black boxes for their non-linearity. Regarding RF, we utilized the implementation from the scikit-learn library with the number of weak learners set to 400, and a tf-idf vectorizer is fitted on XL for preprocessing of raw texts. As for DNN, we implemented an eight-layer DNN with Pytorch. An LSTM layer [25] locates in the center of the hidden layers, which captures the sequential information of texts; the rest are fully connected layers with a ReLU activation function.\nGenerative model: We adopted DAAE, a symmetric generative autoencoder, as the generator for neighborhood construction. The hyperparameter setting for the generative model follows the suggestion of the original paper [55]. Both encoder E and decoder D are one-layer bidirectional LSTM with 1024 units. The encoder takes as input a 512-dimensional dense vector from the embedding layer and projects the LSTM outcome to the latent space (bottleneck) with a size of 128 via a fully connected layer. The decoder reconstructs texts from latent vectors following a reversed procedure. The same setting applies to both datasets. More details about the datasets, classifiers, and generative models can be found in Table I. The reconstruction loss Lrec in the table is the cross entropy between the inputs and their reconstructions.\nCompetitors: We compared the proposed explanation methods, XPROA \u2013 relying on a generative model and XPROB \u2013 neighborhood construction with probability-based edition, to the following approaches in the experiments: \u2022 LIME [47]: the most widely used local explanation\nmethod that applies a simple random perturbation to the input x for constructing its neighborhood. \u2022 XSPELLS [30]: a method generating neighbors for texts based on randomly sampling latent vectors and derives explanations from a decision tree built in the latent space. \u2022 ABELE [22]: a generator-based explanation method with the latent space neighborhood constructed by a genetic algorithm.\nDuring the tests, we adopted the same generative models for all competitors (if applied), such that the experimental\nresults give more credit to the explanation procedures rather than the quality of the generators. Note that the last method originally is designed for image classifiers, and we adapted ABELE for textual data by integrating its latent sampling approach into the framework of XPROA. The main difference between the adapted ABELE and XPROA is the replacement of the progressive neighborhood approximation with a genetic algorithm. With the remaining components kept identical, comparing the two methods highlights the impacts of the sampling and counterfactual selection strategies on the resultant explanations.\nHyperparameters for neighborhood construction: The two determining hyperparameters in XPROA are the interpolation step s and the number of landmarks k, which are set to 10 and 20, respectively, for all test cases. We also empirically selected 80 as the prototype set size k for XPROB. For both methods, the maximal neighborhood size p is 400. Further analysis of the impacts of hyperparameters is reported in Section VI-E."
        },
        {
            "heading": "B. Qualitative evaluation",
            "text": "For qualitative comparison, we list explanations from different approaches for the decisions on selected instances and assess explanation quality with human-grounded evaluation [11], [15].\nTable II presents the explanations regarding different inputs. LIME assigns importance scores to features in given texts as explanations, whereas the others, which construct the neighborhood in a meaningful and creative way, give explanations consisting of four parts: i) feature attribution of intrinsic words, which indicates their contributions to the predictions, ii) the most important extrinsic words to the local decision boundary, iii) the selected factual and iv) counterfactual examples. Most of the competitors determine feature attribution through the weights learned by the surrogate model. We illustrate the feature attribution of the intrinsic words through a saliency map for all competitors besides XSPELLS because it outputs in favor of relative frequencies of words in (counter-)factuals determined through a latent decision tree as word-level explanations (numbers in brackets). The saliency map highlights words with colored backgrounds, with the color denoting the sentiment a word contributes to and the saturation indicating its importance. Here, we use the blue (red) background referring to the positive (negative, respectively) sentiment. For the extrinsic words, we report the top-ranked by their attribution as the important ones.\nThe first instance x to be explained is \u201ci love this story\u201d \u2013 a simple input from the Amazon dataset correctly classified as positive by the RF model. As told by the saliency\nmaps, the competitors agree that the term \u201clove\u201d is the most contributing feature to the decision. XPROA further gives an extrinsic word \u201chate\u201d, an antonym of \u201clove\u201d, from the novel neighboring instances, which would affect the prediction negatively if involved in the current context. The extrinsic words from ABELE and XPROB demonstrate that sentimental adjectives describing the noun would also have prominent influences on the decisions under the current context. Meanwhile, XPSELLS, which counts word frequencies, admits that \u201clove\u201d is the most contributing feature. But it also outputs less relevant words. Especially for the word \u201cstory\u201d, its frequent appearances in factuals as well as in counterfactuals reflect the truth that it, as a neutral term, barely affects the decisions in this case. Besides, the instance-level explanations from XSEPLLS are also more general in contrast to our methods, which have better concentrations on the local context through the neighborhood approximation. Through the selected (counter)factuals, we observe that XPROA, powered by a generative autoencoder, is better at generating realistic samples for neighborhood construction. The other method, XPROB, could still involve grammatical mistakes in the produced neighboring texts because of the limited local context and the greedy choice during the iterative edition process.\nThe second instance is \u201cthe dessert is very bland .\u201d selected from the yelp dataset with a negative label assigned by the DNN model. For this input, XSPELLS fails to deliver useful information with generic words and instances. Although LIME identified the term \u201cbland\u201d as the most contributing feature, the notably low importance score poorly matches the confident decision. Combining the weak overall attribution given the high confidence and the comparably higher attribution in the first example (with a less confident decision), we suspect that the attribution scores by LIME are inconsistent while explaining various predictions. On the other hand, the last three approaches agree that \u201cbland\u201d dominate the decision, and their listed (counter-)factuals agree with the observation. However, similar to the results for the first example, (counter)factuals listed by ABELE are mostly identical. The repeated instances reflect the collapsed neighborhood, amplified by the sparse textual space, during the evolution led by the genetic algorithm. Another interesting observation that shouldn\u2019t be omitted is the two extrinsic words by XPROA, \u201csausage\u201d and \u201crooms\u201d. Intuitively, the two neutral terms should not leak information about the sentiment classification and remain less contributory. But the third sentence in the factual set (falsely classified as negative) selected by XPROA demonstrates the negative contribution of \u201csausage\u201d. The same holds for \u201crooms\u201d. Although the fifth counterfactual containing \u201crooms\u201d\nis correctly classified as positive, its confidence score (0.56) is considerably lower than the other two analogous instances, i.e., the first (0.91) and the third (0.93), that only differ from it by the one specific word. A deeper investigation into the training set uncovers the cause of the misuses. Both words are uncommon in the training set (appear less than 20 times in the training corpus with 20K samples) with highly imbalanced distributions between the two classes, which contain the same amount of instances. The word \u201crooms\u201d appears twice more frequently in the negative samples than in the positive, and the same ratio for \u201csausage\u201d is greater than 4. Given this observation, we believe that the over-representations of the two terms in the negative samples mislead the model to use them falsely as shortcuts for the negative sentiment. The higher imbalance of the term \u201csausage\u201d also explains its stronger impact on the decision.\nWith the qualitative evaluation of the two selected examples, we demonstrate that the two proposed methods correctly identify the most contributing features to the corresponding decisions and how the more detailed results could enrich the information of the explanations. The output from XPROA on the second example, in particular, highlights the great potential of extrinsic words and instance-level explanations for not just understanding but also debugging. In addition, through the comparison to the other two generative-based explanation methods, we underline the effectiveness and necessity of proper latent neighborhood construction during the generation of local explanations."
        },
        {
            "heading": "C. Quantitative evaluation",
            "text": "Lacking ground truth, quantitative evaluation of explanation methods remains challenging without consent on how it should be conducted. In this section, we quantified the competitors\u2019 performance following the three Cs of interpretability [42], [58], namely Correctness, Completeness, and Compactness. \u2022 Correctness, also known as fidelity [24], refers to the\nfaithfulness of explanations with respect to the target model b(\u00b7); \u2022 Completeness indicates the coverage of explanations on relevant observations; \u2022 Compactness requires explaining results to be concise by excluding irrelevant features.\nFor correctness, we report the fidelity and the R2 score of surrogate models, which quantify the performance of the corresponding explanation methods at imitating the target. The similarity of the surrogate to the target dominates the upper bound of the explanation correctness \u2013 one cannot interpret a model through an unfaithful proxy.\nSecond, we indirectly evaluate the completeness of explanations with explanation-guided manipulation, that is, editing the input texts and recording the change of classification results [3], [32]. Confidence drops of the predicted class after manipulations reflect the completeness regarding the coverage of relevant features. Specifically, the manipulation masks out positively contributing features and repeats negatively contributing ones. Apart from intrinsic words, for methods\nthat create novel neighbors during the explaining process, extrinsic words are also considered for manipulation (through the probability-based edition introduced in Section 3). To prevent the edition from emptying the whole input, a threshold \u03b7 (we empirically set it to 0.1) is given, and only words with an importance score exceeding the threshold are considered.\nLastly, we adopt AOPC (area over perturbation curve) [51], the cumulative sum of confidence drops after each manipulation operation, as the metric for compactness:\nAOPCx = 1\nl l\u2211 i=1 (b(x)\u2212 b(x(i))) (9)\nwhere x(i) denotes a variant of x, on which the manipulations with the top-i features have been applied. And l denotes the total number of words to be edited (with attribution \u2265 \u03b7) given by the explanation. Different from completeness, the order of editions matters in compactness evaluation. They are applied sequentially according to the descending order of the relevant features with respect to their contribution magnitudes. AOPC will give higher scores to compact explanations that assign more weights to relevant features, and vice versa. The average confidence drop per manipulation (abbreviated as DpM) is also an intuitive and convenient measure for compactness. However, it becomes less precise during the sequential manipulation procedure since the confidence drop is bounded by 1, and the impacts of lower-ranked features could be underestimated. This measure biases towards methods that applies fewer manipulations and distorts the comparison of compactness, especially for the analysis conducted on polarized classifiers, where drops could easily saturate as the classifiers tend to give high confidence scores for their predictions regardless of the output labels. We, therefore, prefer AOPC over averaged confidence drop as the metric for compactness but report both in the experiments.\nAll results of the designed experiments are presented in Table III. The reported values are the averaged performance over the test sets followed by the standard deviation. The most prominent observation is the flawless fidelity of XSPELLS as a measure of correctness. The reason behind the exceptional performance is a different choice of surrogate model. XSPELLS builds a decision tree for approaching the black box, which could always accurately separate given samples once the tree becomes deep enough, whereas the others use a linear regressor. The different surrogate also explains the absence of the R2 score. Back to the group with the linear model, despite the expanded feature space due to the novel neighboring instances, the two proposed methods surprisingly outperform LIME in terms of correctness, especially for the R2 score. This observation indicates that both progressive approximation methods constructing realistic neighborhoods succeed in highlighting local decision boundaries, which consequently simplifies the imitation task for the linear surrogate. Also, the proposed methods achieve outstanding performance in compactness and correctness. Even though XPROB dumps the opaque but powerful generative model, it remains competitive\nwith XPROA. Furthermore, it achieves the highest DpM in all test cases except for the DNN model for the yelp dataset. The exception is caused by the saturation of confidence change, as previously discussed. For this test case, editions on a single word could already cause a significant change (a confidence drop greater than 0.5). Any subsequent actions will only hold back the overall compactness regardless of the actual importance of corresponding features if measured by DpM. Therefore, LIME, the method with the fewest actions, comes out on top. Regarding the other two generator-based competitors, ABELE acquires similar figures to LIME, whereas XSPELLS has a relatively modest performance. Our adapted version of ABELE for textual data differs from XPROA in the neighborhood construction approach. And the gap between their explanation qualities underlines the effectiveness of the progressive approximation in latent space. The completeness and compactness of explanations from XSPELLS disagree with its exceptional performance on correctness. The more complicated surrogate model and the entanglement of latent space features obstruct the derivation of accessible explanations, although it better approaches the explaining target. Admittedly, XPROA and XPROB benefit from the neighboring extrinsic words during the manipulation compared to LIME. However, the performances of XSPELLS and ABELE, which also involve novel words, show that the concrete choices of extrinsic words with opposite sentimental meanings should take into consideration the local context and can have a prominent influence. In general, the results of the quantitative evaluation match the conclusion drawn from the previous qualitative analysis.\nIn the last column, we report the time complexity4 of explaining for all competitors. The time cost of the listed\n4The time costs are reported on an Intel i7-11800H CPU with a single Nvidia RTX3070 Max-Q GPU.\nexplanation methods consists of two parts, neighborhood construction and requests for the black box outcomes. LIME is the most efficient method among all competitors. Its intuitive perturbation on the target input takes trivial time during explanations, so the time cost relies mainly on the black box efficiency for labeling the neighboring instances, which causes inconsistent performances while explaining different classifiers. The same observations also apply to XPROA and ABELE. These two methods exhaustively construct neighborhoods to approach the local decision boundary with iterative processes that constantly query the target model. The copious inquiries result in the massive dependency of their time costs on black box efficiency. In fact, the three generator-based explainers in the middle are all time-consuming. Deployment of another network for generating neighboring instances delays the derivation of explanations. XPROB mitigates the time cost by unloading the generative model. But the more complicated generation scheme demands a higher computational cost, which differs from the performance of LIME by an order of magnitude. Considering the shallower classifiers adopted in the experiment, the running time of XPROB , which relies more on the construction process during the tests, will approach LIME while explaining complex models. Although the realtime constraint does not strictly apply to the domain of XAI, we still want to point out that the complexity of explanation methods could limit their potential applications in practice, e.g., debugging [7] or debiasing [9], [29] machine learning models)."
        },
        {
            "heading": "D. Stability",
            "text": "A stable explanation method ought to deliver similar explanations for similar inputs [11]. Regarding textual data, similar inputs should be semantically and syntactically close. However, the limited number of similar text sets in natural corpora\nimpedes the evaluation of explanation stability. Inspired by text data augmentation [16], [50], which serves the same purpose of creating similar texts, we manually construct a test set with pre-defined templates as shown in Table IV. Here, a test case is formed by five instances from the templates with the placeholders \u2329ADJ\u232a and \u2329NOUN\u232a replaced by a chosen adjective-noun pair. For each test case, the feature attributions of the inserted words should remain similar due to the similar context. Thus, we measure explanation stability by computing the attribution deviations for the two inserted words among the five synthetic instances.\nWe chose the yelp DNN model as the black box for the stability test. The choice is reasoned by the more accurate and polarized model predictions, through which we expect the classification behaviors regarding the listed variants to be more stable. We list the concrete choices of adjectives and nouns specified for the yelp DNN model in Table V. For each sentimental pole, we selected ten adjectives that receive the highest confidence scores for the corresponding class. The nouns possess confidence scores close to 0.5 for both classes while using a single word as input for the classification. Enumerating all combinations of adjectives and nouns results in 200 test cases.\nThe performance of all competitors regarding the stability evaluation is presented in Table VI. We excluded XSPELLS from the comparison as it cannot constantly assign importance scores to the intrinsic features, which impedes the stability analysis. For all values shown in the table, we first compute the summary statistics (mean, deviation, and set similarity) for each test case, which we call the case statistics. We then average all case statistics and report the means. Using case deviation as an example, the computation of the average case deviation can be formally described as follows:\n\u03b4 =\n\u2211 ADJ. \u2211 N. \u03b4(Templates (adj., n.))\n|{ADJ.}| \u00d7 |{N.}|\nwhere Templates (\u00b7, \u00b7) denotes the set of variants created with the templates given an adjective and a noun.\nThe first column of Table VI gives the mean of case deviations on the classification outcomes. This trivial figure does not refer to similar outputs regarding all incomes. Instead, it indicates that the black box generally produces consistent predictions for instances from the same test cases (containing the same adjective-noun pair). Similar inputs with similar predictions build the premise for the stability evaluation. The third and fifth columns list the average contribution magnitudes of inserted words, and columns 4 and 6 present the means of case deviations. We split the words into two groups according to word class and average the case statistics for words belonging to the same group. Since the words have different sentimental polarities, we use the modulus of attribution to avoid the collision of different signs in case means. Although the focus of the section is on stability, we want to point out that the case mean of the nouns gives a glimpse into the correctness. The low attributions assigned to nouns (column 5) by both XPROA and XPROB match the ground truth that the neutral terms leak little information for the sentiment classification task, which indicates the faithfulness of their explanation outcomes. Now returning to stability, the average case deviations of feature attributions are expected to be low for both word groups given the stable predictions (observed in column 1), which reflects the steadiness of the assigned importance scores across every five variants. Closely followed by XPROB, ABELE reaches the lowest deviation of the importance scores assigned to the adjectives benefiting from the compact neighborhoods. The diversity of the neighborhoods constructed by XPROA results in the highest figure. As for the nouns, XPROB clearly outperforms the others with a value of nearly only one-third of the second lowest achieved by ABELE. However, even the best method provides less steady explanations, especially considering the barely changed prediction outcomes. A possible reason for the observation is that the expanding feature space (the additional words in the longer templates) distorts the scaling of the surrogate model.\nTo mitigate the scaling effect and to have a unified measure quantifying the stability, we report the set similarity in the last column as the ultimate metric for explanation stability. The set similarity is the average cosine similarity between feature attribution vectors \u3008\u03beadj., \u03ben.\u3009 in a test case. LIME performs modestly according to the set similarity, which corresponds to its varying attributions. With its neighborhood construction strictly limited by the length of the input text, the shortage of neighboring instances causes unfaithful explanations for the shorter inputs. And this finally results in the instabil-\nity of feature attributions under similar contexts. Aligning with the observations on the feature attribution deviations, XPROB provides the most stable explanations throughout all test cases. This observation is foreseeable as the controlled generation process offers dense and realistic neighborhoods while granting the surrogate model to concentrate on the intrinsic features, which follow balanced distributions. ABELE also achieves competitive performance in the stability evaluation. We attribute this to the usage of the genetic algorithm. It ensures the neighborhoods converge to certain subspaces for similar inputs over the evolution, so the surrogates built on homologous neighborhoods deliver similar explanations. In contrast, XPROA, another generator-based explanation method, suffers from inconsistencies in explanation results as a side effect of neighborhood diversity. The two-staged interpolation with better coverage of the diverse neighboring texts largely expands the feature space by introducing novel instances and thus affects the consistency of explanations on intrinsic words."
        },
        {
            "heading": "E. Sensitivity to hyperparameters",
            "text": "To study the influence of the hyperparameters on explanation quality, we repeated the quantitative evaluation with different hyperparameter choices for XPROA and XPROB. In XPROA, the two hyperparameters dominating the latent space exploration are the interpolation step s and the number of landmarks k. The neighborhood size is another factor in the neighborhood construction process. But it is excluded from the analysis of hyperparameter sensitivity since the number of generated instances is highly dependent on the other two parameters and does not reach the limitation in most cases. We picked the interpolation steps from 6 to 14 with an interval of 2 with the landmark set having a size of 20. For the test on the landmark number, the values vary from 10 to 30 while keeping the interpolation step at 10. As for XPROB, we tested its sensitivity to the choices of the neighborhood size p (the population limitation) and the number of prototypes k. These are the only two parameters that affect the construction process. Compared to XPROA (demonstrated in Table III), the less time complexity of XPROB allows us to determine the hyperparameter choices on a larger scale to highlight their effects. For the population limitation, we chose five values that increase exponentially from 100 to 1600 with the prototype quantity set to 80. Similarly, we also picked a geometric sequence for k while the population sticks to 400.\nFor XPROA, Fig. 1a shows that the interpolation intervals from the selected range have limited influence on the three Cs. The reason for the observation is the continuous distribution in the latent space. The increase in interpolation density only produces more duplicated instances, which are later removed during the neighborhood finalization. On the other hand, the increasing size of the landmark set slightly improves the completeness and compactness, as visualized in Fig. 1b. The larger landmark set not only affects the initialization of the progressive approximation but also allows the usage of more landmarks during the iterative interpolation, which encourages latent space exploration and highlights the decision boundary.\nSimilar changes in confidence drop and in AOPC hold for XPROB with the expansion of the prototype set (Fig. 2b), which also affects the initialization along with the intermediate prototype selection. Partly owing to the larger gaps between the hyperparameter choices, the increasing tendencies are more significant. The changes in completeness suggest that more relevant features are filtered out and involved in manipulations. And for the two classifiers trained on the yelp dataset, the saturation of confidence drop holds their DpMs back, which aligns with our argument in Section VI-C about having AOPC as another compactness metric. In contrast to the changing tendency of completeness, the dropping R2 scores suggest that an excessive amount of prototypes can violate the locality constraint. The expanding prototype set includes additional parts of the decision boundary and finally introduces nonlinearity when it reaches far enough. But the explanation qualities remain steady even with a badly chosen k (= 160) because of the neighborhood approximation and the weighted loss function. The former tightens the neighborhood, and the latter encourages the surrogate to concentrate on the closer samples by assigning them higher weights. Meanwhile, Fig. 2a illustrates that enriching the neighborhood set also lays positive effects on explanation qualities. According to all listed metrics, the performances improve rapidly with the growing population, especially when the number is low, and the upward trend slows down once p reaches 400. The observation indicates that explanations become precise when the population is large enough for proper coverage of neighborhoods. After that, enrichment of neighboring samples would not further promote explanation qualities.\nAlthough we applied the same hyperparameter setting for the remaining parts of the experiments, the plots in Fig. 1 and Fig. 2 show that the optimal choices are not identical under different settings. In reality, the best hyperparameter choice depends on the data distribution and the decision boundary of the explaining target."
        },
        {
            "heading": "F. Dependency on external resources",
            "text": "As the last part of our experiments, we investigated in this subsection the dependencies of the two proposed methods on the external resources they utilized as knowledge for improving neighborhood quality.\nLike many other generator-based explanation methods [17], [22], [30], XPROA determines the neighborhood of inputs in the latent space, which fully relies on the adopted generator. Despite their importance in neighborhood constructions, there is yet little study on the impacts of generative models on explanation quality. Originating from the concern about the opacity of the generators, we investigate the dependency of explanation quality on the generative models and discuss the potential risks of the generator-based solutions for XAI. The capacity of a generative model is directly related to its ability to generate and reconstruct. In order to uncover the correlation between generator capacity and explanation quality, we alter the size of the autoencoder bottleneck with the remaining components of XPROA unchanged and perform the quanti-\ntative evaluation on the amazon DNN model. Furthermore, as discussed in Section III-A, latent space geometry also plays a crucial role in the locality-preserving generation. Therefore, we also replace the chosen DAAE with VAE [8], a well-known generative autoencoder, and compare their performances with various bottleneck sizes. DAAE differs from VAE mainly in the training scheme, it applies noises to the training samples for better latent space geometry and reconstruction ability. For a valid comparison, the remaining network structures of DAAE and VAE are identical and remain constant during the test.\nThe chosen bottleneck sizes are listed in the second column of Table VII, followed by the corresponding reconstruction losses in column 3 reported on the test set. For DAAE, the reconstruction loss correlates negatively with the size of the bottleneck. The drop is heavier for the narrower bottlenecks and becomes trivial when its size approaches the capacity of the preceding and succeeding layers. The lower reconstruction loss suggests a better fit for the locality constraint as the generator can better concentrate on the given instance. Not surprisingly, the figures for completeness (column 4) and compactness (columns 5 and 6) of explanations deploying DAAE grow accordingly when the generator capacity improves. On the contrary, the increasing capability of VAE does not necessarily improve its performance on reconstruction due to the organization of the latent space [65]. Even though the generator provides more realistic samples with the growing bottleneck size, the violation of the locality constraint results in the independence of explanation quality from generator capacity. Table VIII presents a failure of XPROA with a\ngenerator unsatisfying the assumptions on the reconstruction ability and locality-preserving property. According to the (counter-)factuals in the last entry, failing to concentrate on the given context (because of the high reconstruction loss) leads to the ignorance of multiple ingredients in the input, which causes a notably worse performance. Here, the explainer equipping VAE also reaches its best performance with the widest bottleneck. But the narrowest bottleneck (with a size equal to 32) achieving similar results in all metrics suggests that this tends to be a coincidence.\nWe also experimented with XPROB for its dependency on the prototype corpus (Table IX). Analogous to the previous method, the explanation quality of XPROB correlates positively with the external knowledge guiding the construction of neighborhoods. The performance stabilizes when the corpus size becomes large enough to represent the distribution of the target domain. For the selected classifier and dataset, it is |XL| \u2265 5k. Compared to the generator-based solution, the one with the probability-based edition is less sensitive to the change of external resources.\nThrough the sensitivity of explanations to particular choices of generators, we illustrate the heavy reliance of XPROA on the generative model as well as the necessity of fulfilling the two properties mentioned in Section III-A. The significant correlation between reconstruction ability and explanation quality suggests that reconstruction loss should be considered one of the selection criteria for the generative model. Meanwhile, the gap between the performances of explainers adopting DAAE and VAE emphasizes the importance of latent space geometry, which is unfortunately challenging to investigate due to its complexity and non-linearity. And we argue that the reliance on such inaccessible latent space may undermine the trustworthiness of the derived explanations."
        },
        {
            "heading": "VII. CONCLUSIONS",
            "text": "In this work, we proposed two progressive approaches that improve the neighborhood quality of textual data for deriving local explanations. Our first method XPROA, which follows the momentum of utilizing generative models for neighborhood preparations, approximates the underlying neighborhood of input via a two-staged interpolation in the latent space maintained by a generative autoencoder. Arise from the concern about the opacity of the generator powered by neural networks, we proposed an alternative \u2013 XPROB, which also improves explanation quality by generating realistic and suitable samples. It adopts the probability-based edition, which is fully\ntransparent, following the local n-gram context as a substitute for the generator. Both methods enhance the understanding of particular black-box decisions via explanations consisting of word-level and instance-level components.\nOur experiments show, both qualitatively and quantitatively, the effectiveness of XPROA in comparison to the other competitors. Its adoption of the generative model ensures the quality of constructed neighborhoods. As for XPROB, dropping the generative model harms the validity of the created texts to a certain extent as demonstrated by the (counter)factuals in the qualitative evaluation. But it still attains similar performance to XPROA during the quantitative evaluation following the 3 Cs of interpretability. Furthermore, the explanations are much more stable as a consequence of the transparent and controllable generation process according to Section VI-D, where XPROA appears to be modest.\nIn spite of the analysis of explanation quality, through the investigation of the dependency of XPROA on the detailed settings of the generative model, we would also like to call for more caution in involving generators in explainability. The transparency built upon opaque entities leaves potential risks in practice. Minor changes, e.g., modifying the training scheme or altering the structure of the generator networks, could already challenge explanation quality. Moreover, if taking biases widely reported on generative models [2], [26], [56] into consideration, poor choices of the generator could damage more than just the quality of explanations. For this reason, we believe an appropriate certification process is mandatory for the verification and selection of generator models deployed in explanation methods."
        }
    ],
    "title": "XPROA-B",
    "year": 2023
}