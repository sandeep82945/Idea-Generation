{
    "abstractText": "Hearing loss is one of the most significant sensory disabilities. It can have various negative effects on a person\u2019s quality of life, ranging from impeded school and academic performance to total social isolation in severe cases. It is therefore vital that early symptoms of hearing loss are diagnosed quickly and accurately. Audiology tests are commonly performed with the use of tonal audiometry, which measures a patient\u2019s hearing threshold both in air and bone conduction at different frequencies. The graphic result of this test is represented on an audiogram, which is a diagram depicting the values of the patient\u2019s measured hearing thresholds. In the course of the presented work several different artificial neural network models, including MLP, CNN and RNN, have been developed and tested for classification of audiograms into two classes normal and pathological represented hearing loss. The networks have been trained on a set of 2400 audiograms analysed and classified by professional audiologists. The best classification performance was achieved by the RNN architecture (represented by simple RNN, GRU and LSTM), with the highest out-of-training accuracy being 98% for LSTM. In clinical application, the developed classifier can significantly reduce the workload of audiology specialists by enabling the transfer of tasks related to analysis of hearing test results towards general practitioners. The proposed solution should also noticeably reduce the patient\u2019s average wait time between taking the hearing test and receiving a diagnosis. Further work will concentrate on automating the process of audiogram interpretation for the purpose of diagnosing different types of hearing loss.",
    "authors": [
        {
            "affiliations": [],
            "name": "Micha\u0142 Kassja\u0144ski"
        },
        {
            "affiliations": [],
            "name": "Marcin Kulawiak"
        },
        {
            "affiliations": [],
            "name": "Tomasz Przewo\u017any"
        }
    ],
    "id": "SP:e203fb30c0523ffcfe4cc07fa9e143e091835ed9",
    "references": [
        {
            "authors": [
                "B.O. Olusanya",
                "K.J. Neumann",
                "J.E. Saunders"
            ],
            "title": "The global burden of disabling hearing impairment: a call to action",
            "venue": "Bull World Health Organ",
            "year": 2014
        },
        {
            "authors": [
                "AA Kapul",
                "EI Zubova",
                "SN Torgaev",
                "VV. Drobchik"
            ],
            "title": "2017. Pure-tone audiometer",
            "venue": "J Phys Conf Ser,",
            "year": 2010
        },
        {
            "authors": [
                "V.P. Aras"
            ],
            "title": "Audiometry techniques, circuits, and systems, M",
            "venue": "Tech. Credit Seminar Report,",
            "year": 2003
        },
        {
            "authors": [
                "C Tukaj",
                "J Kuczkowski",
                "M Sakowicz-Burkiewicz",
                "G Gulida",
                "D Tretiakow",
                "T Mionskowski",
                "T. Pawe\u0142czyk"
            ],
            "title": "Morphological alterations in the tympanic membrane affected by tympanosclerosis: ultrastructural study",
            "venue": "Ultrastruct Pathol. 38(2):69-73,",
            "year": 2014
        },
        {
            "authors": [
                "W Narozny",
                "A Skorek",
                "D. Tretiakow"
            ],
            "title": "Does Treatment of Sudden Sensorineural Hearing Loss in Patients With COVID-19 Require Anticoagulants? Otolaryngol Head Neck Surg",
            "year": 2021
        },
        {
            "authors": [
                "Prashanth Prabhu P",
                "Jyothi Shivswamy"
            ],
            "title": "Audiological findings from an adult with thin cochlear nerves",
            "venue": "Intractable & Rare Diseases Research,",
            "year": 2017
        },
        {
            "authors": [
                "J. Przewo\u017any T. Kuczkowski"
            ],
            "title": "Hearing loss in patients with extracranial complications of chronic otitis media",
            "venue": "Otolaryngol Pol",
            "year": 2017
        },
        {
            "authors": [
                "Ersin Elba\u015f\u0131",
                "Murat Obali"
            ],
            "title": "Classification of Hearing Losses Determined through the Use of Audiometry using Data Mining, Conference: 9th International Conference on Electronics,Computer and Computation",
            "year": 2012
        },
        {
            "authors": [
                "N.G. Noma",
                "M.K.A. Ghani"
            ],
            "title": "Predicting Hearing Loss Symptoms from Audiometry Data Using Machine Learning Algorithms",
            "venue": "In Proceedings of the Software Engineering Postgraduates Workshop (SEPoW),",
            "year": 2013
        },
        {
            "authors": [
                "F Charih",
                "M Bromwich",
                "AE Mark",
                "R Lefran\u00e7ois",
                "JR. Green"
            ],
            "title": "Data- Driven Audiogram Classification for Mobile Audiometry",
            "venue": "Sci Rep 10,",
            "year": 2020
        },
        {
            "authors": [
                "R.H. Margolis",
                "G.L. Saly"
            ],
            "title": "Toward a standard description of hearing loss",
            "venue": "International journal of audiology,",
            "year": 2007
        },
        {
            "authors": [
                "MG Crowson",
                "JW Lee",
                "A Hamour",
                "R Mahmood",
                "A Babier",
                "V Lin",
                "DL Tucci",
                "TCY. Chan"
            ],
            "title": "AutoAudio: Deep Learning for Automatic Audiogram Interpretation",
            "venue": "J Med Syst. 44(9):163,",
            "year": 2020
        },
        {
            "authors": [
                "Barbour",
                "Dennis L. MD",
                "PhD",
                "Jan-Willem A. Wasmann"
            ],
            "title": "Performance and Potential of Machine Learning Audiometry, The Hearing Journal: Volume 74 - Issue 3 - p 40,43,44, http://dx.doi.org/10.1097/01",
            "year": 2021
        },
        {
            "authors": [
                "B. Aziz",
                "N. Riaz",
                "A.U. Rehman",
                "M.I. Malik",
                "K.I. Malik"
            ],
            "title": "Colligation of Hearing Loss and Chronic Otitis Media",
            "venue": "Pakistan Journal of Medical and Health Sciences",
            "year": 2021
        },
        {
            "authors": [
                "A. Raghavan",
                "U. Patnaik",
                "A.S. Bhaudaria"
            ],
            "title": "An Observational Study to Compare Prevalence and Demography of Sensorineural Hearing Loss Among Military Personnel and Civilian Population",
            "venue": "Indian Journal of Otolaryngology and Head & Neck Surgery,",
            "year": 2020
        },
        {
            "authors": [
                "S.K. Zieli\u0144ski",
                "H. Lee"
            ],
            "title": "Feature extraction of binaural recordings for acoustic scene classification",
            "venue": "Federated Conference on Computer Science and Information Systems (FedCSIS) (pp. 585-588),",
            "year": 2018
        },
        {
            "authors": [
                "I.E. Agbehadji",
                "R. Millham",
                "S.J. Fong",
                "H. Yang"
            ],
            "title": "Kestrelbased Search Algorithm (KSA) for parameter tuning unto Long Short Term Memory (LSTM) Network for feature selection in classification of high-dimensional bioinformatics datasets",
            "venue": "Federated Conference on Computer Science and Information Systems (FedCSIS) (pp. 15-20),",
            "year": 2018
        },
        {
            "authors": [
                "J. Lind\u00e9n",
                "S. Forsstr\u00f6m",
                "T. Zhang"
            ],
            "title": "Evaluating combinations of classification algorithms and paragraph vectors for news article classification",
            "venue": "Federated Conference on Computer Science and Information Systems (FedCSIS) (pp. 489-495),",
            "year": 2018
        },
        {
            "authors": [
                "Hochreiter",
                "Sepp",
                "Schmidhuber",
                "Jurgen"
            ],
            "title": "Long Short-term Memory",
            "venue": "Neural computation",
            "year": 1997
        },
        {
            "authors": [
                "Cho",
                "Kyunghyun",
                "Merrienboer",
                "Bart",
                "Gulcehre",
                "Caglar",
                "Bougares",
                "Fethi",
                "Schwenk",
                "Holger",
                "Y. Bengio"
            ],
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, http://dx.doi.org/10.3115/v1/D14-1179",
            "year": 2014
        },
        {
            "authors": [
                "LC do Carmo",
                "JA M\u00e9dicis da Silveira",
                "SA Marone",
                "FG D\u2019Ottaviano",
                "LL Zagati",
                "EM. Dias von S\u00f6hsten Lins"
            ],
            "title": "Audiological study of an elderly Brazilian population",
            "venue": "Braz J Otorhinolaryngol;74(3):342-9,",
            "year": 2018
        },
        {
            "authors": [
                "JJ Walker",
                "LM Cleveland",
                "JL Davis",
                "JS. Seales"
            ],
            "title": "Audiometry screening and interpretation",
            "venue": "Am Fam",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nH EARING IS one of the most important senses and is crucial for a human to maintain full connectivity to the world. Early on in life, hearing helps one to establish language skills which lays the groundwork for quick development during school years. In daily tasks, hearing is used in communicating with other people as well as for listening to music, television and radio, and going to the cinema or theatre.\nAccording to World Health Organization (WHO), currently, around 430 million people globally require rehabilitation services for their hearing loss [1]. Estimations show that by 2050 nearly 2.5 billion people will be living with some degree of hearing loss, at least 700 million of whom will require rehabilitation services [1]. Overall, hearing impairment has devastating consequences for interpersonal communication, psychosocial well-being, quality of life and economic inde-\npendence [2]. The consequences of hearing loss are frequently underestimated and ignoring the initial symptoms usually leads to further degradation. Once diagnosed, early intervention is the key to successful treatment. Medical and surgical treatment can cure most ear diseases, potentially reversing the associated hearing loss. Research has shown that, particularly in children, almost 60% of hearing loss is due to causes that can be prevented [1], [6], [7].\nThe standard hearing test is carried out using pure tone audiometry, which determines the hearing thresholds at different frequencies. As a rule, a frequency range of the hearing test varies within 125 \u2013 8000 Hz. The sound level of pure tones is given in dBHL, and the subject is tested in both air and bone conduction. The test results in two data series containing discrete hearing thresholds in the function of frequency, separately for both conductions. This data series is usually presented in the form of an inverted graph called audiogram. An audiogram helps to determine the degree of hearing loss, but also the type of pathology: sensorineural, conductive or mixed [3], [4].\nAccording to projections, the demand for professional audiologists will burgeon in near future [1]. Nowadays, around 78% of low-income countries have less than one otorhinolaryngologist per million inhabitants and about 93% have less than one audiologist per million inhabitants [1], [5]. In this context, introduction of expert systems based on artificial intelligence for preliminary audiogram interpretation could significantly reduce the workload of specialists, while at the same time shortening the patient\u2019s wait for a diagnosis.\nOver the last decade, a comparison of several approaches to hearing loss determination, including Decision Tree, Naive Bayes and Neural Network Multilayer Perceptron (NN) model, has been prepared by Elbas\u0327\u0131 & Obali [10]. The tests have been carried out using a set of numerical values representing Decibels corresponding to fixed frequency levels (750Hz, 1kHz, 1,5kHz, 2kHz, 3kHz, 4kHz, 6kHz, 8kHz). The achieved accuracy was 95.5% in Decision Tree, 86.5 % in Naive Bayes and 93.5 % in NN.\nA different approach was presented by Noma & Ghani [11], who developed a classification system based on the relationship between pure-tone audiometry thresholds and inner ear\nIEEE Catalog Number: CFP2285N-ART \u00a92022, PTI 163\ndisorders symptoms such as Tinnitus, Vertigo, Giddiness etc. The classifier, based on the multivariate Bernoulli model with feature transformation, has shown to provide 98% accuracy of predicting hearing loss symptoms based on audiometry results.\nRecently, Charih et al. [12] presented their Data-Driven Annotation Engine, a decision tree based audiogram classifier which considers the configuration, severity, and symmetry of participant\u2019s hearing losses and compared it to AMCLASS [13], which fulfils the same purpose using a set of general rules. Both classifiers have achieved similar accuracy of around 90% across 270 different audiometric configurations by three licensed audiologists.\nMore recently, Crowson et al. [14] adopted the ResNet-101 model to classify audiogram images into three types of hearing loss (sensorineural, conductive or mixed) as well as normal hearing using a set of training and testing images consisting of 1007 audiograms. This approach resulted in 97.5% classification accuracy, however it is limited to processing images.\nIn summary, the combination of neural networks and increased computing resources of new hardware architectures has the potential to deliver faster overall tests results and more detailed assessments[15]. This being said, however, the currently proposed solutions deliver classification accuracy in the 90-95% range, which, although very high, still leaves considerable room for error. Clinical standards suggest that the margin of error should be kept under 5%[16] and optimally should be close to 3% [17]. These requirements are met only by two of the discussed classifiers. The method proposed by Noma & Ghani achieves 98% accuracy, however it has been designed to predict significant symptoms of inner ear disorder, and thus it cannot be used for general purposes such as early detection of hearing degradation. The best audiogram classifier to date has been presented by Crowson et al., who used transfer learning to adapt an established image classifier network to analysis of audiogram images. While this approach resulted in a 97% classification accuracy, it exhibits serious limitations. Because it is an image classifier, it cannot be used with the original data series produced by tonal audiometry. This means that the data series first need to be converted into audiogram images, which may result in data loss. Moreover, although the structure of audiograms generally is similar, there can still be significant differences between audiograms generated by different hardware and software configurations. Aside from differences such as background and line colours, audiograms can also differ in the amount of presented information (eg. they may contain data for a single ear or both). A sample comparison of significant differences between audiograms obtained from different sources is presented in Figures 1 and 2. In consequence, a universal solution for classifying results of tonal audiometry cannot be based on an image classifier.\nThis study presents the development of a neural network for classification of discrete tonal audiometry data series. In the course of this study, several different neural network architectures have been trained and tested with the use of 2400 audiogram data series analysed and classified by professional audiologists. The goal of the presented study was to achieve a\nhigh enough classification accuracy for the developed network to be applicable for use in a clinical environment."
        },
        {
            "heading": "II. MATERIALS & METHODS",
            "text": ""
        },
        {
            "heading": "A. Data",
            "text": "The study has been conducted with the use of 2400 data series containing results of pure tone audiometry tests performed from 2020 to 2021 by clinicians working at the Otolaryngology Clinic of the University Clinical Centre in Gdansk, Poland. The data contains 650 examples of normal hearing and 1750 examples of pathological hearing loss. The tests had been performed in a soundproof booth, according to ISO 8253 and ISO 8253 standards. Air conduction tests employed TDH39P headphones, while bone conduction testing involved a Radioear B-71 bone-conduction vibrator. The data series have been analysed and labelled by expert audiologists from the\nMedical University of Gdansk Department of Otolaryngology according to established methodology [9]. In consequence, the dataset has been classified into two subsets: hearing pathology and normal hearing."
        },
        {
            "heading": "B. Preprocessing",
            "text": "The input data series contained numerical information about tonal points, defined as loudness (dB) for a given frequency (Hz), in XML format. The dataset included the following range of frequencies:\n125Hz, 250Hz, 375Hz, 500Hz, 750Hz, 1000Hz, 1500Hz,\n2000Hz, 3000Hz, 4000Hz, 6000Hz, 8000Hz.\nEvery tested frequency has been assigned a loudness level in the range from -10dB to 120dB. If certain frequencies had not been registered during the hearing test, they have not been included in the corresponding data series."
        },
        {
            "heading": "C. Testing methodology",
            "text": "Using the prepared dataset, three different neural network architectures have been trained to interpret tonal audiometry data and in order to differentiate normal hearing (N) from pathological hearing loss (P). The tested architectures included Multilayer Perceptron (MLP), Convolutional (CNN) and Recurrent (RNN) neural networks, all of which have been previously applied to data classification problems [18], [19], [20]. The general workflow of the presented study is shown in Fig. 3. Each model has been assessed using k-fold cross-validation, which consists of dividing the data into k subsets and training the model k-times with k-1 subsets, with a different subset being used for testing in every iteration. The presented research used k = 5, which resulted in train to test dataset proportions of 80% to 20%, respectively.\nAfter revealing the best performing architecture, further tests and optimizations would be carried out in order to improve classification accuracy."
        },
        {
            "heading": "III. RESULTS",
            "text": "The purpose of the initial tests was to reveal the best neural network architecture model for classification of pure tone audiometry data. The tested neural network architectures included MLP, CNN and RNN. The results of those tests are presented in Table I.\nAs it can be seen, initial research revealed that the best classification performance has been produced by the RNN architecture model. Once the most promising neural network architecture has been identified, three of its variants have been trained and optimized in terms of hyper parameters, including number of nodes and hidden layers, dropout layers, learning and decay rate. The first model consisted of a simple RNN, second one was based on Gated Recurrent Units (GRU) [22] and the last one used Long Short-Term Memory (LSTM) [21]. The results of these tests are shown in Table II.\nReceiver Operating Characteristics (ROC) curves with corresponding Area Under the Curve (AUC) parameters for these models are presented in Fig. 4.\nThe cross validation scores for k = 5 with LSTM classifier are given in Table III. The average accuracy was 98.08% (+/- 0.17%).\nA detailed analysis of classification performance achieved by the tested RNN models can be made using a confusion matrix, which visualizes the number of True Positives (TP\n- patients who have been properly classified with hearing loss), True Negatives (TN - patients who have been properly classified with good hearing), False Positives (FP - patients who have been improperly classified as hearing loss) and False Negatives (FN - patients who have been improperly classified with good hearing). The confusion matrix for the tested RNN models is presented in Figures 5, 6 and 7."
        },
        {
            "heading": "IV. DISCUSSION",
            "text": "Initial tests have shown that the simple RNN architecture model delivers noticeably better pure tone audiometry classification results in comparison to MLP and CNN models, achieving accuracy of 96.04% versus 94.58% and 95.63% respectively (Tab. I). The chosen network architecture appears to have the largest impact on classification accuracy, as further tests and optimizations resulted in minor improvements. Optimization of parameters such as the number of nodes and hidden layers, dropout layers as well as learning and decay rate improved the accuracy of simple RNN from 96.04% to 96.46%. In comparison, applying the same optimization process to MLP and CNN models did not result in markedly improved evaluation parameters. A possible explanation for this could be the fact that RNN have been designed to process time series data, and structurally pure tone audiometry results could be interpreted as a special case of time series. This could be further explored by testing the effectiveness of more advanced RNN models such as GRU and LSTM. As it can be seen in Tab. II, both of these models obtained more than 97% accuracy, with the highest out-of-training set accuracy being achieved by LSTM at 98.12%. While these results, which have been cross-validated using the 5-fold method, would seem to indicate a general prevalence of the RNN architecture in processing audiometry data, establishing an effectiveness hierarchy of RNN models is a more complex matter. Although LSTM has shown the best classification accuracy, when analysed in terms of confusion matrix, the lowest number of False Positives (FP) was obtained by GRU (Figures 6 and 7), with LSTM taking second place. In comparison, the simple RNN produced over 62% more False Positives than LSTM and 85% more than GRU.\nOverall, simple RNN and GRU performed equally well in terms of False Negatives (FN), producing them only in 0.8% of cases, whereas LSTM significantly outperformed the other models with only one case of error occurring. It can be argued that when classifying results of pure tone audiometry tests, the FN number is more important than FP because it shows that a patient does not have hearing loss when they actually do. In this case the patient may not receive treatment and\nget worse because their disease was undetected. On the other hand, a false positive would only result in the patient being unnecessarily referred to an audiologist, who would properly interpret the test results and inform the patient that their level of hearing is normal.\nSumming up, it can be said that the 98.12% classification accuracy achieved by LSTM fulfills the established margin of error criteria and is significantly better than the 97.5% classification accuracy offered by the best existing algorithm for audiogram data classification, proposed by Crowson et al. [14]. While some of the difference could be attributed to the rival method providing a larger set of classes, the presented method provides an additional advantage in the type of processed data: it works with original tonal audiometry data series instead of audiogram images and therefore is more universal. The only rival method also designed for processing tonal audiometry data series, presented by Elbas\u0327\u0131 & Obali [10], provides an even lower 95.5% classification accuracy.\nIn terms of classifying pure tone audiometry data, the only existing solution with a similar classification accuracy level (98%, proposed by Noma & Ghani [11]), has been designed to predict significant symptoms of inner ear disorder and thus cannot be used for general classification of tonal audiometry test results."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "The presented work aimed to develop a neural network for classification of discrete tonal audiometry data series with accuracy high enough for medical application. In the course of this study, several different neural network architectures, including MLP, CNN and RNN, have been trained and tested with the use of 2400 audiogram data series analysed and classified by professional audiologists. The highest classification accuracy was achieved with an optimized LSTM RNN at 98.12%. The high accuracy of the obtained neural network, particularly the low number of False Negatives (0.2%), allows for its application at the Otolaryngology Clinic of the University Clinical Centre in Gdansk, Poland. Results of pure tone audiometry tests, which thus far needed to be examined by professional audiologists, can now be classified with the developed neural network under the supervision of general practitioners. This change may result in a significant reduction of the workload of audiology specialists, as they will no longer need to deal with patients whose symptoms are not caused by hearing loss (which may amount to over 10% of all patients subjected to pure tone audiometry tests) [23], [24]. After it has been further tested in practice, the developed solution could be introduced directly in the audiometry laboratory, ensuring that the patient receives a first interpretation of the performed tests as soon as they have been completed. Further work will concentrate on expanding the classifier for the purpose of diagnosing different types of hearing loss."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "The authors would like to thank M. Grono, K. Koz\u0301min\u0301ski, P. Mierzwin\u0301ska and A. Romanowicz who helped to create the\npure tone audiometry test dataset used in this study."
        }
    ],
    "title": "Development of an AI-based audiogram classification method for patient referral",
    "year": 2022
}