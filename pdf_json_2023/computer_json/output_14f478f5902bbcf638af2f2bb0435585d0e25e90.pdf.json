{
    "abstractText": "The Light Field (LF) deblurring task is a challenging problem as the blur images are caused by different reasons like the camera shake and the object motion. The single image deblurring method is a possible way to solve this problem. However, since it deals with each view independently and cannot effectively utilize and maintain the LF structure, the restoration effect is usually not ideal. Besides, the LF blur is more complex because the degree is affected by the views and depth. Therefore, we carefully designed a novel LF deblurring network based on the LF blur characteristics. On one hand, since the blur degree varies a lot in different views, we design a novel view adaptive spatial convolution to deblur blurred LFs, which calculates the exclusive convolution kernel for each view. On the other hand, because the blur degree also varies with the depth of the object, a depth perception view attention is designed to deblur different depth areas by selectively integrating information from different views. Besides, we introduce an angular position embedding to maintain the LF structure better, which ensures the model correctly restores the view information. Quantitative and qualitative experimental results on synthetic and real images show that the deblurring effect of our method is better than other stateof-the-art methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zeqi Shen"
        },
        {
            "affiliations": [],
            "name": "Shuo Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhuhao Zhang"
        },
        {
            "affiliations": [],
            "name": "Qihua Chen"
        },
        {
            "affiliations": [],
            "name": "Xueyao"
        },
        {
            "affiliations": [],
            "name": "Youfang Lin"
        }
    ],
    "id": "SP:0889273b8e486277da259a0970192278847b36eb",
    "references": [
        {
            "authors": [
                "Ayan Chakrabarti"
            ],
            "title": "A Neural Approach to Blind Motion Deblurring",
            "venue": "In ECCV. Springer,",
            "year": 2016
        },
        {
            "authors": [
                "Jiaxin Chen",
                "Shuo Zhang",
                "Youfang Lin"
            ],
            "title": "Attention-based Multi-Level Fusion Network for Light Field Depth Estimation",
            "venue": "In AAAI. AAAI Press,",
            "year": 2021
        },
        {
            "authors": [
                "Liang Chen",
                "Faming Fang",
                "Tingting Wang",
                "Guixu Zhang"
            ],
            "title": "Blind Image Deblurring With Local Maximum Gradient Prior",
            "venue": "In CVPR",
            "year": 2019
        },
        {
            "authors": [
                "Donald G. Dansereau",
                "Anders P. Eriksson",
                "J\u00fcrgen Leitner"
            ],
            "title": "Motion Deblurring for Light Fields",
            "venue": "arXiv preprint arXiv:1606.04308",
            "year": 2016
        },
        {
            "authors": [
                "Qing Guo",
                "Wei Feng",
                "Ruijun Gao",
                "Yang Liu",
                "Song Wang"
            ],
            "title": "Exploring the Effects of Blur and Deblurring to Visual Object Tracking",
            "venue": "IEEE Trans. Image Process",
            "year": 2021
        },
        {
            "authors": [
                "Hailin Jin",
                "Paolo Favaro",
                "Roberto Cipolla"
            ],
            "title": "Visual Tracking in the Presence of Motion Blur",
            "venue": "In CVPR",
            "year": 2005
        },
        {
            "authors": [
                "Jing Jin",
                "Junhui Hou",
                "Jie Chen",
                "Sam Kwong"
            ],
            "title": "Light Field Spatial Super- Resolution via Deep Combinatorial Geometry Embedding and Structural Consistency Regularization",
            "venue": "In CVPR",
            "year": 2020
        },
        {
            "authors": [
                "Dong Jing",
                "Shuo Zhang",
                "Runmin Cong",
                "Youfang Lin"
            ],
            "title": "Occlusion-aware Bi-directional Guided Network for Light Field Salient Object Detection",
            "venue": "In ACM Multimedia",
            "year": 2021
        },
        {
            "authors": [
                "Neel Joshi",
                "C. Lawrence Zitnick",
                "Richard Szeliski",
                "David J. Kriegman"
            ],
            "title": "Image deblurring and denoising using color priors",
            "venue": "In CVPR",
            "year": 2009
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980",
            "year": 2014
        },
        {
            "authors": [
                "Orest Kupyn",
                "Volodymyr Budzan",
                "Mykola Mykhailych",
                "Dmytro Mishkin",
                "Jiri Matas"
            ],
            "title": "DeblurGAN: BlindMotionDeblurring Using Conditional Adversarial Networks",
            "venue": "In CVPR",
            "year": 2018
        },
        {
            "authors": [
                "Orest Kupyn",
                "Tetiana Martyniuk",
                "Junru Wu",
                "Zhangyang Wang"
            ],
            "title": "DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better",
            "venue": "In ICCV",
            "year": 2019
        },
        {
            "authors": [
                "Dongwoo Lee",
                "Haesol Park",
                "In Kyu Park",
                "Kyoung Mu Lee"
            ],
            "title": "Joint Blind Motion Deblurring and Depth Estimation of Light Field",
            "year": 2018
        },
        {
            "authors": [
                "Hee Seok Lee",
                "Junghyun Kwon",
                "Kyoung Mu Lee"
            ],
            "title": "Simultaneous localization, mapping and deblurring",
            "venue": "In ICCV",
            "year": 2011
        },
        {
            "authors": [
                "Yan Li",
                "Qiong Wang",
                "Lu Zhang",
                "Gauthier Lafruit"
            ],
            "title": "A Lightweight Depth Estimation Network for Wide-Baseline Light Fields",
            "venue": "IEEE Trans. Image Process",
            "year": 2021
        },
        {
            "authors": [
                "Zhong Li",
                "Yu Ji",
                "Jingyi Yu",
                "Jinwei Ye"
            ],
            "title": "3D Fluid Flow Reconstruction Using Compact Light Field PIV",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Samuel Lumentut",
                "Tae Hyun Kim",
                "Ravi Ramamoorthi",
                "In Kyu Park"
            ],
            "title": "Deep Recurrent Network for Fast and Full-Resolution Light Field Deblurring",
            "venue": "IEEE Signal Process. Lett. 26,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Samuel Lumentut",
                "Williem",
                "In Kyu Park"
            ],
            "title": "6-DOF motion blur synthesis and performance evaluation of light field deblurring",
            "venue": "Multim. Tools Appl. 78,",
            "year": 2019
        },
        {
            "authors": [
                "Seungjun Nah",
                "Tae Hyun Kim",
                "Kyoung Mu Lee"
            ],
            "title": "Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring",
            "venue": "In CVPR",
            "year": 2017
        },
        {
            "authors": [
                "Ryan S. Overbeck",
                "Daniel Erickson",
                "Daniel Evangelakos",
                "Matt Pharr",
                "Paul E. Debevec"
            ],
            "title": "A system for acquiring, processing, and rendering panoramic light field stills for virtual reality",
            "venue": "ACM Trans. Graph. 37,",
            "year": 2018
        },
        {
            "authors": [
                "Jinshan Pan",
                "Haoran Bai",
                "Jinhui Tang"
            ],
            "title": "Cascaded Deep Video Deblurring Using Temporal Sharpness Prior",
            "venue": "In CVPR",
            "year": 2020
        },
        {
            "authors": [
                "Jin-shan Pan",
                "Zhe Hu",
                "Zhixun Su",
                "Ming-Hsuan Yang"
            ],
            "title": "Deblurring Text Images via L0-Regularized Intensity and Gradient Prior",
            "year": 2014
        },
        {
            "authors": [
                "Jin-shan Pan",
                "Deqing Sun",
                "Hanspeter Pfister",
                "Ming-Hsuan Yang"
            ],
            "title": "Blind Image Deblurring",
            "venue": "Using Dark Channel Prior. In CVPR",
            "year": 2016
        },
        {
            "authors": [
                "Liyuan Pan",
                "Yuchao Dai",
                "Miaomiao Liu",
                "Fatih Porikli"
            ],
            "title": "Simultaneous Stereo Video Deblurring and Scene Flow Estimation",
            "venue": "In CVPR",
            "year": 2017
        },
        {
            "authors": [
                "Dongwon Park",
                "Dong Un Kang",
                "Jisoo Kim",
                "Se Young Chun"
            ],
            "title": "2019. Multi- Temporal Recurrent Neural Networks For Progressive Non-Uniform Single Image Deblurring With Incremental Temporal Training",
            "year": 1911
        },
        {
            "authors": [
                "Pratul P. Srinivasan",
                "Ren Ng",
                "Ravi Ramamoorthi"
            ],
            "title": "2017. Light Field Blind Motion Deblurring",
            "venue": "In CVPR",
            "year": 2017
        },
        {
            "authors": [
                "Maitreya Suin",
                "Kuldeep Purohit",
                "A.N. Rajagopalan"
            ],
            "title": "Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring",
            "venue": "arXiv preprint arXiv:2004.05343",
            "year": 2020
        },
        {
            "authors": [
                "Jian Sun",
                "Wenfei Cao",
                "Zongben Xu",
                "Jean Ponce"
            ],
            "title": "Learning a convolutional neural network for non-uniform motion blur removal",
            "venue": "In CVPR",
            "year": 2015
        },
        {
            "authors": [
                "Xintao Wang",
                "Kelvin C.K. Chan",
                "Ke Yu",
                "Chao Dong",
                "Chen Change Loy"
            ],
            "title": "EDVR: Video Restoration With Enhanced Deformable Convolutional Networks",
            "venue": "In CVPR",
            "year": 2019
        },
        {
            "authors": [
                "Yingqian Wang",
                "Longguang Wang",
                "Jun-Gang Yang",
                "Wei An",
                "Jingyi Yu",
                "Yulan Guo"
            ],
            "title": "Spatial-Angular Interaction for Light Field Image Super-Resolution",
            "year": 2020
        },
        {
            "authors": [
                "Yingqian Wang",
                "Jun-Gang Yang",
                "Longguang Wang",
                "Xinyi Ying",
                "Tianhao Wu",
                "Wei An",
                "Yulan Guo"
            ],
            "title": "Light Field Image Super-Resolution Using Deformable Convolution",
            "venue": "IEEE Trans. Image Process",
            "year": 2021
        },
        {
            "authors": [
                "Henry Wing Fung Yeung",
                "Junhui Hou",
                "Xiaoming Chen",
                "Jie Chen",
                "Zhibo Chen",
                "Yuk Ying Chung"
            ],
            "title": "Light Field Spatial Super-Resolution Using Deep Efficient Spatial-Angular Separable Convolution",
            "venue": "IEEE Trans. Image Process. 28,",
            "year": 2019
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman H. Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Multi-Stage Progressive Image Restoration",
            "venue": "In CVPR",
            "year": 2021
        },
        {
            "authors": [
                "Hongguang Zhang",
                "Yuchao Dai",
                "Hongdong Li",
                "Piotr Koniusz"
            ],
            "title": "Deep Stacked Hierarchical Multi-Patch Network for Image Deblurring",
            "venue": "In CVPR",
            "year": 2019
        },
        {
            "authors": [
                "Jiawei Zhang",
                "Jinshan Pan",
                "Jimmy S.J. Ren",
                "Yibing Song",
                "Linchao Bao",
                "Rynson W.H. Lau",
                "Ming-Hsuan Yang"
            ],
            "title": "Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks",
            "venue": "In CVPR",
            "year": 2018
        },
        {
            "authors": [
                "Kaihao Zhang",
                "Wenhan Luo",
                "Yiran Zhong",
                "Lin Ma",
                "Wei Liu",
                "Hongdong Li"
            ],
            "title": "Adversarial Spatio-Temporal Learning for Video Deblurring",
            "venue": "IEEE Trans. Image Process",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Zhang",
                "Song Chang",
                "Youfang Lin"
            ],
            "title": "End-to-End Light Field Spatial Super-Resolution Network Using Multiple Epipolar Geometry",
            "venue": "IEEE Trans. Image Process",
            "year": 2021
        },
        {
            "authors": [
                "Shen Zheng",
                "Yuxiong Wu",
                "Shiyu Jiang",
                "Changjie Lu",
                "Gaurav Gupta"
            ],
            "title": "Deblur-YOLO: Real-Time Object Detection with Efficient Blind Motion Deblurring",
            "venue": "In IJCNN",
            "year": 2021
        },
        {
            "authors": [
                "Zhihang Zhong",
                "Ye Gao",
                "Yinqiang Zheng",
                "Bo Zheng"
            ],
            "title": "Efficient Spatio- Temporal Recurrent Neural Network for Video Deblurring",
            "year": 2020
        },
        {
            "authors": [
                "Shangchen Zhou",
                "Jiawei Zhang",
                "Jinshan Pan",
                "Wangmeng Zuo",
                "Haozhe Xie",
                "Jimmy S.J. Ren"
            ],
            "title": "Spatio-Temporal Filter Adaptive Network for Video Deblurring",
            "venue": "In ICCV",
            "year": 2019
        },
        {
            "authors": [
                "Shangchen Zhou",
                "Jiawei Zhang",
                "Wangmeng Zuo",
                "Haozhe Xie",
                "Jinshan Pan",
                "Jimmy S. Ren"
            ],
            "title": "DAVANet: Stereo Deblurring With View Aggregation",
            "venue": "In CVPR",
            "year": 2019
        },
        {
            "authors": [
                "Wenhui Zhou",
                "Enci Zhou",
                "Gaomin Liu",
                "Lili Lin",
                "Andrew Lumsdaine"
            ],
            "title": "Unsupervised Monocular Depth Estimation From Light Field Image",
            "venue": "IEEE Trans. Image Process",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "KEYWORDS light field deblurring, view adaptive, depth perception"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "There are many reasons for image blur, such as camera shake and object motion. Image blur seriously affects the visual quality of images and the performance of some common computer vision tasks such as tracking [5, 6], SLAM [14], and object detection [39]. Therefore, how to remove the image blur effectively and efficiently has attracted extensive attention from researchers. In recent years, with the rapid development of deep learning, some well-designed single image deblurring algorithms [12, 19, 28, 34, 35] have made remarkable progress. The single image blur can be modeled as:\n\ud835\udc3c\ud835\udc35 (\ud835\udc65,\ud835\udc66) = \u222b \ud835\udc43\ud835\udc61\ud835\udc65,\ud835\udc66 \ud835\udc3c\ud835\udc61\ud835\udc46 (\ud835\udc65,\ud835\udc66) \ud835\udc51\ud835\udc61, (1)\nwhere \ud835\udc3c\ud835\udc35 (\ud835\udc65,\ud835\udc66) denotes the blurred image, \ud835\udc3c\ud835\udc61\ud835\udc46 (\ud835\udc65,\ud835\udc66) denotes the sharp image, \ud835\udc43\ud835\udc61\ud835\udc65,\ud835\udc66 denotes the motion path of the pixel (\ud835\udc65,\ud835\udc66) within the exposure time \ud835\udc61 . The goal of the deblurring task is to recover \ud835\udc3c\ud835\udc46 from \ud835\udc3c\ud835\udc35 .\nWith the maturity of commercial Light Field (LF) cameras, which can obtain images from different views through one shot, researchers and enterprises have paid more attention to Light Field images (LFs).\nLimited by hardware equipment, it is easy to cause image blur when shooting LFs, which limits the application scene of LFs. Therefore, the progress of LF deblurring task can promote the development of various LF applications such as virtual reality [20], salient object detection [8], 3D reconstruction [16] and depth estimation [15].\nIn this paper, we use 4D function \ud835\udc3f(\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) \u2208 R\ud835\udc48\u00d7\ud835\udc49\u00d7\ud835\udc4b\u00d7\ud835\udc4c to represent LFs, where (\ud835\udc62, \ud835\udc63) and (\ud835\udc65,\ud835\udc66) are the angular and spatial coordinates, respectively. If we fixed (\ud835\udc62, \ud835\udc63), the Sub-Aperture Image (SAI) \ud835\udc3c\ud835\udc62,\ud835\udc63 can be obtained. If we fixed (\ud835\udc65,\ud835\udc66), the Micro-lens Images \ud835\udc40\ud835\udc65,\ud835\udc66 can be obtained. Different from the single image blur, the LF blur can be modeled as:\n\ud835\udc3f\ud835\udc35 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) = \u222b \ud835\udc43\ud835\udc61\ud835\udc62,\ud835\udc63,\ud835\udc65,\ud835\udc66 \ud835\udc3f\ud835\udc61\ud835\udc46 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) \ud835\udc51\ud835\udc61, (2)\nwhere \ud835\udc3f\ud835\udc35 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) denotes the blurred LFs, \ud835\udc3f\ud835\udc61\ud835\udc46 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) denotes the sharp LFs, \ud835\udc43\ud835\udc61\ud835\udc62,\ud835\udc63,\ud835\udc65,\ud835\udc66 denotes the motion path of the pixel (\ud835\udc65,\ud835\udc66) in view (\ud835\udc62, \ud835\udc63) within the exposure time \ud835\udc61 . Compared with the single image, which has only spatial dimension (\ud835\udc65,\ud835\udc66), the LFs have extra angular dimension (\ud835\udc62, \ud835\udc63). Therefore, the blur phenomenon in LFs is more complex than the single image. Fig. 1 is an example of LF deblurring task. On one hand, when the light field camera moves, the motion path of each view relative to the object is different, so the blur degree of each view is also different. On the other hand, similar to a single image, objects with different depths produce different degrees of blur, i.e., blur is more serious in pixels with small depth and less in pixels with large depth.\nSingle image methods [34, 35] are possible ways to deal with LF deblurring task. However, for the LF blur is affected by the views and depth, if the single image methods are directly applied to the LF deblurring task, i.e., dealing with each view independently, their results cannot maintain the LF structure. Since the LFs are collected regularly in the angular dimension, the LFs have the consistent structure, i.e., the correspondence between different views.\nar X\niv :2\n30 3.\n06 86\n0v 1\n[ cs\n.C V\n] 1\n3 M\nar 2\n02 3\nConference\u201917, July 2017, Washington, DC, USA Zeqi Shen1,2 , Shuo Zhang1,2,3 , Zhuhao Zhang1 , Qihua Chen1 ,Xueyao Dong1 , Youfang Lin1,2,3\nFurthermore, since the single image methods cannot effectively utilize the LF angular and depth information, their results are still blurred in some blur areas. Although the LFs with abundant angular information has greater potential to deal with this problem, LF deblurring methods [13, 17, 27] are still in their infancy. Traditional methods [13, 27] are often time-consuming. Although the LF method [17] based on deep learning process images quickly, they destroy the LF consistent structure, because the LFs are divided into multiple groups in their network. By analyzing the current research status of the single image and LF methods, we summarize three urgent problems in the LF deblurring task: 1) How to deal with the problem of different blur degrees in different views. 2) How to solve the different blur degrees caused by different depths in the same view. 3) How to maintain the LF consistency.\nIn this paper, based on the above problems we find and summarize, we propose a novel end-to-end learning-based framework. For the different blur degrees in different views, our method calculates the exclusive convolution kernels for different views to perceive and deal with the corresponding blur. For the pixels with different blur degrees from different depths, the mixed range between the sharp pixel we want to recover and the other view pixels is different in the corresponding micro-lens image. Therefore, we designed a depth perception view attention for micro-lens images to pick out the sharp pixel information, which is mixed in other view pixels. Then the sharp images are restored pixel by pixel based on implicit depth information. Besides, an angular position embedding is applied to depth perception view attention to maintain the LF consistency.\nOur main contributions can be summarized: \u2022 We propose a novel view adaptive spatial convolution to deblur the different views, which automatically adapts to the blur degree of different views.\n\u2022 We design a novel depth perception view attention to restore sharp images pixel by pixel, which deals with the different blurs caused by different depths. Besides, the angular position embedding is applied to maintain the LF consistency. \u2022 Quantitative and qualitative experimental results on synthetic and real images show that our method is superior to other state-of-art methods."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "This section will introduce the related works from single image and LF methods. Since the application scenarios of the blind methods are more widely than unblinded methods, we only analyze the blind deblurring method that the blur kernel is unknown."
        },
        {
            "heading": "2.1 Single Image Deblurring Method",
            "text": "In the past few decades, single image deblurring task has been studied extensively and remarkable achievements have been achieved. Traditional methods remove image blur by various prior knowledge such as color [9], gradient [3], dark channel [23],and L0 regularization [22]. However, these methods are often computationally complex and cannot achieve satisfactory results on complex real datasets.\n[1, 29] are the pioneers to solve the deblurring problem based on deep learning, and made remarkable progress. Following these\npioneers, multi-scale convolution [19, 28, 34, 35] and Recurrent Neural Networks [25, 36] are applied to model the spatial variation blur kernel in dynamic scenes. Inspired by the success of Generative Adversarial Networks, [11, 12] generated the sharp images more in line with human visual perception.\nAlthough the single image methods are very effective for processing single image deblurring, the LF blur is different from single image. The single image methods have some limitations in processing LFs: 1) The single image methods do not use the abundant angular information in LFs, so that the depth information is missing during the deblurring processing. By contrast, our LF method makes full use of depth information by combining complementary information from all views in LFs, so that the blur boundaries in different depths can be better handled. 2) Using the single image methods to deal with each view separately cannot maintain the LF angular structure very well, which will break the corresponding relationships between views in LFs. By contrast, we process all views at one time, which maintains the LF structure well."
        },
        {
            "heading": "2.2 LF Deblurring Method",
            "text": "The LFs belong to multi-images, but it is different from other multiimage types such as video and stereo images. Although these methods [21, 24, 30, 37, 40, 41] are excellent in dealing with blur in their field, we do not describe them in detail for their image types are different with us.\nWith the maturity of the commercialization of light field cameras, a variety of applications based on light fields have been brought. Although LFs have been widely studied in the fields of superresolution [7, 32] and depth estimation [2, 43], there is little research on LF deblurring. Dansereau et al. [4] first proposed the non-blind LF deblurring method based on the known camera motion. Srinivasan et al. [27] are the first to model the 3-DOF camera motion and proposed a blind LF deblurring method on a 3-DOF dataset. Different from 3-DOF camera motion, 6-DOF methods [13, 18] deal with the LF deblurring task on the 6-DOF motion. However, the computational complexity of these traditional methods is too high, and the deblurring effect is seriously limited by the estimation of camera motion.\nThe existing LF deblurring method [17] based on deep learning does not make full use of the LF spatial and angular structure. Since the LFs are divided into multiple groups, the LF structure is destroyed. Different from them, the LFs are input as a whole in our method. Therefore, the restoration results of all views are obtained only by one calculation, which effectively maintains the LF structure."
        },
        {
            "heading": "3 MOTIVATION",
            "text": "Inspired by stereo deblurring scenes [42], we found there are two characteristics of blurred LFs, which are useful for the LF deblurring task. On one hand, the blur degree of the same object is different in different views. On the other hand, the blur degree of different objects with various depths is also different. Fig. 2 is a schematic diagram of blurred LFs to explain the two characteristics. For convenience, we only show two cameras in Fig. 2, which can be easily extended to the case of LF.\nView Adaptive Light Field Deblurring Networks with Depth Perception Conference\u201917, July 2017, Washington, DC, USA\nIn Fig. 2 (a), the building is on the same vertical line as one of the cameras and the light emitted from the building reaches the image plane after passing through the camera lens. We suppose the camera moves in a simple straight line along the Y-axis. As the two cameras move along the Y-axis, the position that the light of the building reaches on the left image plane \ud835\udc651 does not change, but on the right image plane, the position change from \ud835\udc653 to \ud835\udc652. For more complex rotational motion, this phenomenon will be more obvious. Therefore, the blur degree of the building in different views is different since the different relative motion between the different view cameras and the object. If we share the parameters of the convolution kernel for all views when designing the model, the model uses the same convolution kernel to deal with different blur degrees, which is unreasonable. Noting this, we design a novel view adaptive spatial convolution, which dynamically calculates the exclusive convolution kernel for each viewwithout significantly increasing the parameters. The detail is introduced in Sec. 4.1.\nIn Fig. 2 (b), the person and the building have different depths. As the camera move along the X-axis, the position that the light of the building reaches on the image changes from \ud835\udc651 to \ud835\udc655, while the position that the light of the person reaches on the image changes from \ud835\udc654 to \ud835\udc651. As we can see from Fig. 2 (b), \ud835\udc655 \u2212 \ud835\udc651 is less than \ud835\udc651\u2212\ud835\udc654, which means that when the camera moves, the blur degree of the objects with different depths is different. Generally, the blur degree of objects with a small depth is large, and the blur degree of objects with a large depth is small.\nTo make full use of the LF angular information, we introduce the micro-lens image. In the micro-lens image, the sharp pixel we want to recover is mixed with other view pixels. For the different blur degrees brought by depth, the mixed range between the sharp pixel and other pixels is also different. If the blur degree of the sharp pixel is serious, the mixed range is large, otherwise, it is small. When restoring a sharp pixel, we should focus on the positions where the sharp pixel is mixed. Therefore, we design a depth perception view attention to find the area where the sharp pixels are mixed. Then the sharp images are restored pixel by pixel by fusing the corresponding weighted micro-lens image features. The detail is introduced in Sec. 4.2."
        },
        {
            "heading": "4 OUR METHOD",
            "text": "The overview architecture of our proposed method is shown in Fig. 3. The blurred LFs are first fed into the View Adaptive Spatial Convolution (\ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36) module, which dynamically calculates the exclusive convolution kernel for each view. After that, the sharp LFs can be reconstructed pixel by pixel through the Depth Perception View Attention (\ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34) module, which calculates different attention weights for objects with different depths to pick out the sharp pixel information in the corresponding micro-lens images. Besides, to maintain the consistency of LFs, the Angular Position Embedding (\ud835\udc34\ud835\udc43\ud835\udc38) module is introduced into \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module."
        },
        {
            "heading": "4.1 View Adaptive Spatial Convolution",
            "text": "As mentioned in Sec. 3, the blur degrees of the same object are different in different views. There are two intuitive ways to deal with it. 1) The model ignores the blur degrees difference of different views and shares the same convolution kernel for all views. 2) The model simply assigns a convolution kernel to each view. For the first way, if all views share the same convolution kernel, it is difficult for a model to perceive different blur degrees. The lack of the ability to perceive the blur degrees will lead the model to deal with the average blur degree of all views. For the second way, if simply assign a convolution kernel to each view when building the model, the number of model parameters will increase exponentially. For example, assuming that the parameter required to process one view is \ud835\udc36\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 \u00d7 \ud835\udc58 \u00d7 \ud835\udc58 , where \ud835\udc36\ud835\udc56\ud835\udc5b , \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 , and \ud835\udc58 denotes the number of input channels, the number of output channels, and the convolution kernel size, respectively. Therefore, the parameter required to process \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 views according to the above method is \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \u00d7 \ud835\udc36\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 \u00d7 \ud835\udc58 \u00d7 \ud835\udc58 . In order to enable the model to perceive the difference of blur degrees between different views without significantly increasing the number of parameters, \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module is cleverly designed, which is shown in Fig. 3.\nFirstly, the blurred LFs \ud835\udc3f\ud835\udc35 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) are input into the \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module in the form of SAIs. Then the exclusive convolution kernel for each view are generated. Take one view as an example, the SAI features are fed into one average pooling and two full connections to generate the corresponding view adaptive convolution kernel \ud835\udc4a\ud835\udc62,\ud835\udc63 \u2208 R\ud835\udc58\u00d7\ud835\udc58 . Then our method extracts the view adaptive spatial features based on spatial convolution by\ud835\udc4a\ud835\udc62,\ud835\udc63 . Finally, the all view adaptive features \ud835\udc39\ud835\udc49\ud835\udc34 \u2208 R\ud835\udc48\u00d7\ud835\udc49\u00d7\ud835\udc4b\u00d7\ud835\udc4c\u00d7\ud835\udc36 are fused through angular convolution.\n\ud835\udc39\ud835\udc49\ud835\udc34 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) = \ud835\udc3b\ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 (\ud835\udc3f\ud835\udc35 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66)). (3)\nThe number of \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module is \ud835\udc36\ud835\udc56\ud835\udc5b \u00d7\ud835\udc36\ud835\udc3e +\ud835\udc36\ud835\udc3e \u00d7\ud835\udc36\ud835\udc3e +\ud835\udc36\ud835\udc3e \u00d7\ud835\udc36\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 \u00d7\ud835\udc58 \u00d7\ud835\udc58 , where\ud835\udc36\ud835\udc3e denotes the number of nodes in the two full connection layers. In this paper, \ud835\udc36\ud835\udc3e =4 < \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64=25, which denotes that our model has fewer parameters. For more details, please refer the codes in the supplementary materials.\nBy using the plug and play \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module iteratively, our model not only effectively perceives the difference of blur degrees in different views but also does not significantly increase the number of parameters.\nConference\u201917, July 2017, Washington, DC, USA Zeqi Shen1,2 , Shuo Zhang1,2,3 , Zhuhao Zhang1 , Qihua Chen1 ,Xueyao Dong1 , Youfang Lin1,2,3"
        },
        {
            "heading": "4.2 Depth Perception View Attention",
            "text": "As mentioned in Sec. 3, the blur degree of objects with different depths in the same view is different. To make full use of the LF angular information, we introduce the micro-lens image, where the the sharp pixel we want to recover is mixed with other pixels from other views. Since the different blur degrees, the mixed range in micro-lens images is also different. Therefore, we designed the \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module to find the mixed area for the different blur degrees pixels and then pick out the sharp pixel information based on implicit depth information, which is shown in Fig. 3.\nSpecifically, the view adaptive features \ud835\udc39\ud835\udc49\ud835\udc34 from the\ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module are fed into two branches. In the lower branch, our method generates the view exclusive LF features \ud835\udc39 \ud835\udc63\ud835\udc52\ud835\udc62,\ud835\udc63 \u2208 R\ud835\udc4b\u00d7\ud835\udc4c\u00d7\ud835\udc48\ud835\udc49\ud835\udc36 for all views, through a 2D convolution layer that the output channel number is\ud835\udc48 \u00d7\ud835\udc49 times larger than the input channel number.\nIn the upper branch, the view adaptive features \ud835\udc39\ud835\udc49\ud835\udc34 are fed into another 2D convolution layer to change channel dimension from\ud835\udc36 to\ud835\udc48\ud835\udc49 , then\ud835\udc48 \u00d7\ud835\udc49 depth perception features \ud835\udc39\ud835\udc51\ud835\udc5d\ud835\udc62,\ud835\udc63 \u2208 R\ud835\udc4b\u00d7\ud835\udc4c\u00d7\ud835\udc48\ud835\udc49 can\nbe obtained. To maintain interaction between all views, we reorganize all depth perception features as \ud835\udc39\ud835\udc5b\ud835\udc51\ud835\udc5d\ud835\udc62,\ud835\udc63 \u2208 R\ud835\udc4b\u00d7\ud835\udc4c\u00d7\ud835\udc48\ud835\udc49 . Taking the (u,v) view as an example, the \ud835\udc62\ud835\udc63-th channel dimension of all depth perception features \ud835\udc39\ud835\udc51\ud835\udc5d\ud835\udc62,\ud835\udc63 is connected as the feature \ud835\udc39 \ud835\udc5b\ud835\udc51\ud835\udc5d \ud835\udc62,\ud835\udc63 . To further maintain the consistency of LFs, the \ud835\udc34\ud835\udc43\ud835\udc38 module is applied to the\ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34module to generate the depth perception features with angular position embedding \ud835\udc39\ud835\udc4e\ud835\udc51\ud835\udc5d\ud835\udc62,\ud835\udc63 , which is introduced in Sec. 4.3. After that, the \ud835\udc39\ud835\udc4e\ud835\udc51\ud835\udc5d\ud835\udc62,\ud835\udc63 are fed into several full connection layers to get Depth Perception View Attention Weight\ud835\udc4a \ud835\udc51\ud835\udc5d\ud835\udc62,\ud835\udc63 \u2208 R\ud835\udc4b\u00d7\ud835\udc4c\u00d7\ud835\udc48\ud835\udc49\ud835\udc36 , which denotes the whether the position in the micro-lens image has information that needs to be restored. Then, the sharp features \ud835\udc39\ud835\udc60\u210e\ud835\udc4e\ud835\udc5f\ud835\udc5d\ud835\udc62,\ud835\udc63 \u2208 R\ud835\udc4b\u00d7\ud835\udc4c\u00d7\ud835\udc36 can be obtained by the view exclusive LF features \ud835\udc39 \ud835\udc63\ud835\udc52\ud835\udc62,\ud835\udc63 and the View Attention Weight\ud835\udc4a \ud835\udc51\ud835\udc5d \ud835\udc62,\ud835\udc63 :\n\ud835\udc39 \ud835\udc60\u210e\ud835\udc4e\ud835\udc5f\ud835\udc5d \ud835\udc62,\ud835\udc63 (\ud835\udc65,\ud835\udc66, \ud835\udc50) = \u2211\ufe01 ?\u0302?\ud835\udc63 \ud835\udc39 \ud835\udc63\ud835\udc52\ud835\udc62,\ud835\udc63 (\ud835\udc65,\ud835\udc66,\ud835\udc62 \u2217 \ud835\udc63 \u2217 \ud835\udc50) \u2217\ud835\udc4a \ud835\udc51\ud835\udc5d \ud835\udc62,\ud835\udc63 (\ud835\udc65,\ud835\udc66,\ud835\udc62 \u2217 \ud835\udc63 \u2217 \ud835\udc50) . (4)\nView Adaptive Light Field Deblurring Networks with Depth Perception Conference\u201917, July 2017, Washington, DC, USA\nFinally, the sharp LFs \ud835\udc3f\ud835\udc46 can be generated from the sharp features \ud835\udc39\ud835\udc60\u210e\ud835\udc4e\ud835\udc5f\ud835\udc5d through a 2D convolution.\n\ud835\udc3f\ud835\udc46 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66) = \ud835\udc3b\ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 (\ud835\udc39\ud835\udc49\ud835\udc34 (\ud835\udc62, \ud835\udc63, \ud835\udc65,\ud835\udc66)) . (5) Our method takes the whole LFs as the input and uses and maintains the LF spatial and angular structure as much as possible. On the contrary, the past deep learning LF methods only input part of the LFs into the network at a time. They not only cannot make full use of the LF structure but also need multiple calculations to obtain the results of all views."
        },
        {
            "heading": "4.3 Angular Position Embedding",
            "text": "Without the angular position embedding, the model cannot know which view is being processed. The lack of positional embedding causes the model to tend to handle the average of all views, which destroys the LF structure. Therefore, the \ud835\udc34\ud835\udc43\ud835\udc38 module is necessary for the\ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34module, which can be seen in Fig. 3. Since the angular dimension of the LFs is fixed when acquiring the LFs, we directly use the absolute position as position embedding. Taking the (u,v) view as an example, the angular coordinates (u,v) is concatenated with \ud835\udc39\ud835\udc5b\ud835\udc51\ud835\udc5d\ud835\udc62,\ud835\udc63 to get \ud835\udc39 \ud835\udc4e\ud835\udc51\ud835\udc5d \ud835\udc62,\ud835\udc63 \u2208 R\ud835\udc4b\u00d7\ud835\udc4c\u00d7(\ud835\udc48\ud835\udc49+2) , which changes the channel dimension from\ud835\udc48 \u00d7\ud835\udc49 to\ud835\udc48 \u00d7\ud835\udc49 + 2.\n\ud835\udc39 \ud835\udc4e\ud835\udc51\ud835\udc5d \ud835\udc62,\ud835\udc63 (\ud835\udc65,\ud835\udc66) = \ud835\udc3b\ud835\udc34\ud835\udc43\ud835\udc38 (\ud835\udc39 \ud835\udc5b\ud835\udc51\ud835\udc5d \ud835\udc62,\ud835\udc63 (\ud835\udc65,\ud835\udc66), (\ud835\udc62, \ud835\udc63)) . (6)"
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we first introduce the dataset and training details. Then the proposed method is compared with other single image methods and LF methods on synthetic and real images quantitatively and qualitatively. Next, the ablation study is taken to prove the effectiveness of the designed modules. Finally, the limitation of our method is discussed."
        },
        {
            "heading": "5.1 Training Details and Datasets",
            "text": "5.1.1 Dataset. There are two main kinds of datasets (3-DOF and 6-DOF) in the LF deblurring task. In the 3-DOF dataset, the camera shifts in X, Y, and Z directions. Besides the 3-DOF motions, the camera rotates in X, Y, and Z directions in the 6-DOF dataset. In this paper, we trained models on 3-DOF and 6-DOF datasets separately, since the camera motions are different. We picked 200 LFs from Stanford Lytro [26] as a 3-DOF training dataset and 40 LFs as a 3-DOF test dataset. The simulation method of blurred images is consistent with that of [27]. Since [18] only published 40 images of 6-DOF datasets. We selected the 3 images used in [18] as the 6-DOF test dataset and others as the training dataset.\n5.1.2 Training Details. In this paper, we crop the training LFs into 5 \u00d7 5 \u00d7 64 \u00d7 64 \u00d7 3 patches. Image flipping and image rotation are applied as data augmentation. We use RGB images as input and output. The angular size of input and output are both 5 \u00d7 5. The number of \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 modules is 8. The proposed model is optimized using the Adam [10] algorithm with a batch size of 4. The initial learning rate is set as 1\ud835\udc52 \u2212 3 in the first 200 epochs and is then divided by 10 every 100 epochs. We implement the model with the PyTorch framework and the training process roughly takes 2 days for the 3-DOF dataset and 1 days for the 6-DOF dataset with 4 Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00GHz with a Titan XP GPU.\n5.1.3 Loss Function. Our method uses the L1 Loss as supervision:\n\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc3f1 = \ud835\udc3f\ud835\udc46 \u2212 \ud835\udc3f\ud835\udc54\ud835\udc61 1 , (7)\nwhere \ud835\udc3f\ud835\udc54\ud835\udc61 denotes the ground truth."
        },
        {
            "heading": "5.2 Quantitative Comparison",
            "text": "We compared two single image methods DMPHN [35] and MPRNet [34] with excellent results in the field of single image deblurring. Although there are some deblurring methods based on LFs, most of them do not publish codes and datasets [13, 17, 18]. Fortunately, LFBMD [27] opened their codes so that we can easily compare with them. Since the LF super-resolutionmethods also explore the LF spatial and angular information, which is conducive to deblurring, we modify three LF super-resolution methods SAS [33], InterNet [31], and MEGNet [38] for LF deblurring task.\nTab. 1 is the quantitative results of 3-DOF and 6-DOF test datasets. The PSNR, SSIM, NCC, and LMSE are used for the evaluation index. The higher the values of PSNR, SSIM, and NCC denote the better results. LMSE is the opposite. Tab. 1 shows our method is much better than other single image and LF methods in all evaluation indexes on 3-DOF test datasets. On the 6DOF test dataset, our method also achieved the best results on PSNR and LMSE, and other evaluation indexes also achieved comparable results. Because the single image methods DMPHN [35] and MPRNet [34] process each view alone, they cannot effectively use the angular information of the LFs, i.e., they cannot obtain complementary information from other views. On the contrary, our LF method takes the whole LFs as the input, which fully explores and makes use of the angular and spatial information, so it achieves a better restoration effect. For other LF methods LFBMD [27], SAS [33], InterNet [31], and MEGNet [38], they also use the angular information of LFs, but they do not consider that the blur degree is affected by the views and depth, so they cannot perceive the difference of views and depth well. Different from them, our method designs the \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module and\ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34module to deal with the different blur degrees in different views and depths, which makes our method more effective.\nTab. 2 shows the average time of processing a 5 \u00d7 5 \u00d7 500 \u00d7 336 LFs by various methods and the parameters of various models. For the single image methods DMPHN [35] and MPRNet [34], time is the sum of all views. LFBMD [27] is a traditional method that the parameter is not displayed. LFBMD [27] needs to predict the motion trajectory of the camera first and needs a complex optimization process, so it is very time-consuming to process an LF, which greatly limits the application scenarios. The time consumption of LF methods based on deep learning is similar to that of single image methods. However, the parameters of the LF methods are much smaller than those based on single image. Specifically, the parameters of our method are 30 times less than MPRNet [34] and 33 times less than DMPHN [35]."
        },
        {
            "heading": "5.3 Qualitative Comparison",
            "text": "Fig. 4 is the qualitative comparison with single image methods. We show the result of the central view and enlarge some areas to make the comparison more obvious. In Fig. 4, our results are more clear than the results of the single image methods, and the details are better preserved. To better show the performance of\nConference\u201917, July 2017, Washington, DC, USA Zeqi Shen1,2 , Shuo Zhang1,2,3 , Zhuhao Zhang1 , Qihua Chen1 ,Xueyao Dong1 , Youfang Lin1,2,3\nvarious methods for maintaining the LF structure, we introduce the Epipolar Plane Images (EPIs). In EPIs, the slope of the line segment represents the disparity between two adjacent views. If the LF structure is maintained well, the line segment will not bend or break. Since the single image methods process each view separately, there are bending and fracture in the EPIs of the single image methods in Fig. 4. On the contrary, our method takes the LFs as a whole and introduce the \ud835\udc34\ud835\udc43\ud835\udc38 module to embed angular position, the line\nsegment in our EPIs is a straight line, which fully proves that our method can maintain the structural consistency of LFs.\nFig. 5 is the qualitative comparison with LF methods on the test dataset. Fig. 5 shows that the result of LFBMD is still blurred and is easy to cause color deviation. This is because LFBMD needs to manually set multiple hyperparameters, which limits the application scenario. When the scene becomes complex, the setting of hyperparameters will not be accurate enough, which will easily lead to\nView Adaptive Light Field Deblurring Networks with Depth Perception Conference\u201917, July 2017, Washington, DC, USA\nthe deterioration. Different from it, the LF methods based on deep learning do not need to set complex hyperparameters, since they learn automatically from a large amount of data. Because these LF methods SAS [33], InterNet [31], and MEGNet [38] are not specially designed for deblurring tasks, they cannot perceive the different blur degrees in different views and depths. Compared with our methods, the results of these methods have some obvious artifacts, which can be seen in the enlarged image. Fig. 7 is the qualitative\ncomparison of real scenes. Since there is no published real LF blur dataset, we use a Lytro Illum LF camera to obtain the real blur scene for visual comparison. Our results are sharper than the single image methods, which shows our method is more robust in real scenes.\nConference\u201917, July 2017, Washington, DC, USA Zeqi Shen1,2 , Shuo Zhang1,2,3 , Zhuhao Zhang1 , Qihua Chen1 ,Xueyao Dong1 , Youfang Lin1,2,3"
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "To verify the effectiveness of our method, we did some ablation studies on the 3-DOF test dataset. To ensure the fairness of the ablation study, we keep the parameters of the models almost the same. The experimental results are shown in Tab 3. First, we replace the \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module with the ordinary convolution (w/o \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36) which is shared by all views. The significant decline in the evaluation indexes shows that ordinary convolution cannot learn the prior knowledge that the blur degree of different views is different. On the contrary, the\ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module calculates an exclusive convolution kernel for each view effectively. Then the\ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34module is removed completely (w/o \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34) to verify it can generate the different view attention weights for regions with different depths. The experimental results in Tab. 3 show that the \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module improves the performance by fusing the angular information in the microlens image. Without the \ud835\udc34\ud835\udc43\ud835\udc38 module (w/o \ud835\udc34\ud835\udc43\ud835\udc38), the performance decreases significantly. This is because the \ud835\udc34\ud835\udc43\ud835\udc38 module helps to maintain the LF structural consistency. Besides, if there is no \ud835\udc34\ud835\udc43\ud835\udc38 module, the \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module cannot realize what information needs to obtain from the micro-lens image for the specific view, which\nexplains why the performance without \ud835\udc34\ud835\udc43\ud835\udc38 module is worse than that without \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module. Fig. 6 shows the error map of ablation study. We can see that no matter \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module, \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module, or \ud835\udc34\ud835\udc43\ud835\udc38 module is missing, there will be more error areas, which shows the effectiveness of the structure we designed again. Our method fully explores the LF blur characteristics by the \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 and \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module. Therefore, our results are sharper and the details are restored more perfectly."
        },
        {
            "heading": "5.5 Limitation",
            "text": "Fig. 8 is a failure case of all single image and LF methods. In this scene, blur is very serious in all views. Our LF method cannot effectively obtain useful information from other views to restore the sharp image. Therefore, the ability of deblurring degenerates to the single image deblurring methods. Nevertheless, the detail of our result is still better than the single image methods, especially the restoration of trunk texture."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In this paper, we proposed a novel LF deblurring method based on deep learning. First, the \ud835\udc49\ud835\udc34\ud835\udc46\ud835\udc36 module is designed to effectively deal with different blur degrees in different views by calculating an exclusive convolution kernel for each view. Besides, we designed a \ud835\udc37\ud835\udc43\ud835\udc49\ud835\udc34 module to deal with the different blur degrees in different depths. Through this module, the sharp results are recovered by the weighted micro-lens image based on implicit depth information. Furthermore, the \ud835\udc34\ud835\udc43\ud835\udc38 module is also applied to maintain the overall consistency of the LFs. Our LF method takes the whole LFs as a whole and utilizes the complementary angular information, which makes it to better maintain the structural consistency of the LFs than the single image methods. Quantitative and qualitative experimental results show that our method is much better than other single image and LF deblurring methods.\nView Adaptive Light Field Deblurring Networks with Depth Perception Conference\u201917, July 2017, Washington, DC, USA"
        }
    ],
    "title": "View Adaptive Light Field Deblurring Networks with Depth Perception",
    "year": 2023
}