{
    "abstractText": "Generative flow networks (GFlowNets) are a family of algorithms that learn a generative policy to sample discrete objects x with non-negative reward R(x). Learning objectives guarantee the GFlowNet samples x from the target distribution p\u2217(x) \u221d R(x) when loss is globally minimized over all states or trajectories, but it is unclear how well they perform with practical limits on training resources. We introduce an efficient evaluation strategy to compare the learned sampling distribution to the target reward distribution. As flows can be underdetermined given training data, we clarify the importance of learned flows to generalization and matching p\u2217(x) in practice. We investigate how to learn better flows, and propose (i) prioritized replay training of high-reward x, (ii) relative edge flow policy parametrization, and (iii) a novel guided trajectory balance objective, and show how it can solve a substructure credit assignment problem. We substantially improve sample efficiency on biochemical design tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Max W. Shen"
        },
        {
            "affiliations": [],
            "name": "Emmanuel Bengio"
        },
        {
            "affiliations": [],
            "name": "Ehsan Hajiramezanali"
        },
        {
            "affiliations": [],
            "name": "Andreas Loukas"
        },
        {
            "affiliations": [],
            "name": "Kyunghyun Cho"
        },
        {
            "affiliations": [],
            "name": "Tommaso Biancalani"
        }
    ],
    "id": "SP:03eeacda17e14ed5b77ab9bb6b0b88cf61ce8a15",
    "references": [
        {
            "authors": [
                "J. Andreas"
            ],
            "title": "Measuring compositionality in representation learning",
            "venue": "In None (ed.), International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "T. Deleu",
                "A. G\u00f3is",
                "C. Emezue",
                "M. Rankawat",
                "S. LacosteJulien",
                "S. Bauer",
                "Y. Bengio"
            ],
            "title": "Bayesian structure learning with generative flow",
            "venue": "networks. arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "M. Jain",
                "E. Bengio",
                "Garcia",
                "A.-H",
                "J. Rector-Brooks",
                "B.F.P. Dossou",
                "C. Ekbote",
                "J. Fu",
                "T. Zhang",
                "M. Kilgour",
                "D. Zhang",
                "L. Simine",
                "P. Das",
                "Y. Bengio"
            ],
            "title": "Biological sequence design with gflownets",
            "venue": "arXiv, abs/2203.04115,",
            "year": 2022
        },
        {
            "authors": [
                "N. Malkin",
                "M. Jain",
                "E. Bengio",
                "C. Sun",
                "Y. Bengio"
            ],
            "title": "Trajectory balance: Improved credit assignment in gflownets",
            "venue": "In Advances in Neural Information Processing Systems, volume abs/2201.13259,",
            "year": 2022
        },
        {
            "authors": [
                "M. Minsky"
            ],
            "title": "Steps toward artificial intelligence",
            "venue": "Proc. IRE,",
            "year": 1961
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018",
            "venue": "URL http://incompleteideas.net/ book/the-book-2nd.html",
            "year": 2018
        },
        {
            "authors": [
                "B. Trabucco",
                "X. Geng",
                "A. Kumar",
                "S. Levine"
            ],
            "title": "Designbench: Benchmarks for data-driven offline model-based optimization, 2022",
            "venue": "URL https://arxiv.org/ abs/2202.08450",
            "year": 2022
        },
        {
            "authors": [
                "D. Zhang",
                "N. Malkin",
                "Z. Liu",
                "A. Volokhova",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative flow networks for discrete probabilistic modeling",
            "venue": "arXiv, abs/2202.01361,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A Generative Flow Network (GFlowNet) learns a policy for generating discrete objects x, such as graphs, strings, or sets, by iteratively taking actions that add simple building blocks to partial objects (substructures) (Bengio et al., 2021a;b). GFlowNets view the data-generating Markov decision process (MDP) as a flow network, where \u201cwater\u201d, \u201cunnormalized probability\u201d, or non-negative reward R(x) flows through the MDP, from the source node (a \u201cnull\u201d object), to intermediate nodes (partial objects), to sink nodes (complete objects). GFlowNets can be seen as an amortized\n1Genentech, South San Francisco, USA 2Prescient Design, Genentech, South San Francisco, USA 3Recursion Pharmaceuticals, Salt Lake City, Utah 4Department of Computer Science, New York University, New York, USA. Correspondence to: Max W. Shen <shen.max@gene.com>.\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\nalternative to local exploration methods (e.g., MCMC) that can learn from data to potentially discover new, distant x with high R(x). They have been applied to active learning, biological sequence design, and various probabilistic learning tasks (Bengio et al., 2021a; Jain et al., 2022; Zhang et al., 2022; Deleu et al., 2022).\nGFlowNets aim to solve a challenging unnormalized density estimation problem. Standard learning objectives guarantee that the GFlowNet\u2019s learned distribution over x matches the target distribution p\u2217(x) \u221d R(x) when training loss is globally minimized over all states or trajectories, but many practical domains of interest can have exponentially many x and exponentially many trajectories per x, making this infeasible with a practical amount of data and training time.\nTo gain insight into GFlowNet learning behavior under practical constraints, we design an efficient GFlowNet evaluation scheme that precisely compares the learned sampling distribution to the target reward distribution. We discover that a primary challenge during GFlowNet training is learning to reduce the probability assigned to low-reward x, and that GFlowNets can continue to oversample low-reward x even after extensive training time.\nWhen the space of x is large, not all MDP states are seen during training, and a GFlowNet\u2019s ability to match the target distribution depends on how it generalizes from the flow it learned during training. GFlowNets were originally designed for the setting where any given R(x) is compatible with multiple flows (Bengio et al., 2021a) - that is, flows are underdetermined. Despite this, learned flows have remained understudied, with little to no sense of which learned flows may be more desirable than others. Our analyses ground a clear notion of optimality for learned flows: a flow is better if it improves generalization to unseen states and helps a GFlowNet match the target distribution better.\nWe analyze how existing training objectives learn flows, and identify a credit assignment problem where the substructures of x most responsible for reward R(x) can be off the sampling trajectory used to reach x, and under-credited by popular training objectives. This is important for compositional reward functions, where the value of R(x) is partially determined by the substructures of x. We propose guided trajectory balance, a novel learning objective that is the first\nar X\niv :2\n30 5.\n07 17\n0v 1\n[ cs\n.L G\n] 1\n1 M\nay 2\n02 3\nsolution to this credit assignment problem.\nThis objective, alongside our proposals of prioritized replay training and relative edge flow parametrization, and even user choice of data-generating MDPs, can change how GFlowNets learn to distribute flow during training. In experiments on biochemical design tasks, we demonstrate that these changes in learned flows can significantly impact sample efficiency and convergence to the target distribution, with up to 10\u00d7 improvement. Our work deepens our understanding of the impact of learned flows on GFlowNet behavior, and establishes an fundamental open question: how can we induce GFlowNets to learn more optimal flows, and thereby improve their ability to solve unnormalized density estimation problems."
        },
        {
            "heading": "2 Preliminaries",
            "text": "A Generative Flow Network (GFlowNet) learns a generative policy for constructing an object by taking a sequence of actions (see (Bengio et al., 2021b) for a more complete description). This policy acts in a user-defined deterministic Markov decision process (MDP) which must satisfy certain constraints described below. The MDP has a state space S, a set of legal actions As for each state s, a deterministic transition function S \u00d7 As \u2192 S, and reward function R. GFlowNets view this MDP as a type of directed graph called a flow network where states correspond to nodes and the MDP transition function defines directed edges between nodes. The children of a state are states reachable by outgoing edges, and parents are the sources of its incoming edges. States with no outgoing edges are called terminal states (sinks), and referred to as x \u2208 X . GFlowNets require the MDP to be defined by the user such that this graph is acyclic, has exactly one state without incoming edges, s0 called the initial state (source), and has R : X \u2192 R\u22650.\nA complete trajectory is a sequence of states \u03c4 , (s0 \u2192 s1 \u2192 ... \u2192 sn) starting from source s0 to a sink sn with (st \u2192 st+1) \u2208 Ast for all t. We denote T as the set of all complete trajectories. A trajectory flow is a nonnegative function F : T \u2192 R\u22650 describing the unnormalized probability flowing along each complete trajectory \u03c4 from the source to a sink. For any state s, the state flow F (s) = \u2211 {\u03c4\u2208T :s\u2208\u03c4} F (\u03c4) describes the total amount of unnormalized probability flowing through state s and, for any edge s \u2192 s\u2032, the edge flow is F (s \u2192 s\u2032) =\u2211 {\u03c4\u2208T :(s\u2192s\u2032)\u2208\u03c4} F (\u03c4). A trajectory flow F (\u03c4) is Markovian if there exist distributions PF (\u00b7|s) over the children of every non-terminal state s, and a constant Z, such that for any complete trajectory \u03c4 , we have PF (\u03c4) = F (\u03c4)/Z with PF (\u03c4 = (s0 \u2192 s1 \u2192 ...\u2192 sn)) = \u220fn t=1 PF (st|st\u22121).\nThis PF (st+1|st) is called a forward policy, which can sample complete trajectories from F . We also consider\nPB(st\u22121|st), a backward policy. When F is Markovian, these policies relate to flows by: PF (s\u2032|s) = F (s\u2192s \u2032) F (s) , and PB(s|s\u2032) = F (s\u2192s \u2032)\nF (s\u2032) .\nGenerative Flow Network. A GFlowNet is a learning algorithm with parameters \u03b8 comprising a model of a Markovian flow F\u03b8, and an objective function (Bengio et al., 2021b). The flow model can be uniquely determined by either:\n\u2022 edge flows F\u03b8(s\u2192 s\u2032), which induces a forward policy PF (s \u2032|s), or\n\u2022 initial state flow Z\u03b8 = F\u03b8(s0) and forward policy P \u03b8F (st+1|st), or\n\u2022 terminal state flows F\u03b8(x) and backward policy P \u03b8B(st\u22121|st) .\nWe call the GFlowNet\u2019s learned sampling distribution p\u03b8(x), which is sampled by starting at s0 and iteratively sampling P \u03b8F (st+1|st) to reach a terminal state x. Learning objectives are designed to match p\u03b8(x) to the target distribution, p\u2217(x) , R(x)/ \u2211 X R(x).\nTrajectory balance objective (Malkin et al., 2022). This objective uses a forward policy parameterization by learning Z\u03b8 and a forward policy P \u03b8F , but also learns a backward policy P \u03b8B as a training artifact. Trajectory balance is motivated by a detailed balancing condition, which is satisfied for a flow F with state flow function F (s) if F (s)PF (s\n\u2032|s) = F (s\u2032)PB(s|s\u2032) for all edges s \u2192 s\u2032. The trajectory balance objective attempts to enforce detailed balancing over a complete trajectory:\nLTB(\u03c4) = ( log Z\u03b8 \u220fn t=1 P \u03b8 F (st|st\u22121)\nR(x) \u220fn t=1 P \u03b8 B(st\u22121|st)\n)2 (1)\n(Malkin et al., 2022) showed that if a GFlowNet achieves a global minima of this objective over trajectories from a training policy with full support, then it samples from the target distribution. When many trajectories can lead to the same x, there can be many global optima for the trajectory balance objective - flows are underdetermined. However, for any fixed choice of PB , there is a unique global minima corresponding to a PF that samples from the target distribution (Bengio et al., 2021b). (Madan et al., 2022) study an extension of trajectory balance that learns from partial episodes.\nMaximum entropy GFlowNets. (Zhang et al., 2022) showed that when PB is fixed as the uniform distribution, the unique global minima of the trajectory balance objective is a Markovian flow F with maximum flow entropy H[F ] , E\u03c4\u223cPF \u2211n\u22121 t=0 H[PF (\u00b7|st)].\nTraining. GFlowNets are trained with stochastic gradient descent to optimize the learning objective on states or trajectories sampled from a training policy, which is usually chosen as a mixture of P \u03b8F and a uniform action policy to encourage exploration during training. In RL terms, GFlowNet training is off-policy. Importantly, GFlowNet training is a bootstrapping process: the current policy is used to sample new x at each training round. As R(x) is defined by the user, it is computed on each new x, and this set {x,R(x)} is used to update the GFlowNet policy. We define X as the set of all x seen so far in training. It is initially empty and expands with each training round."
        },
        {
            "heading": "3 Evaluating GFlowNets and underfitting",
            "text": "In prior work, GFlowNets have been evaluated in several ways, typically focusing on their ability to discover novel and diverse modes with high reward. However, their ability to match the target distribution has been empirically studied less thoroughly, especially on real-world data, despite this ability\u2019s central importance in the GFlowNet machinery.\nLearning to match the target distribution is non-trivial. This theoretically occurs when loss is globally minimized over all states or trajectories. However, in high-dimensional settings where |X | is exponentially large, visiting each state or trajectory even once is infeasible.\nEvaluating GFlowNets can be challenging. In many MDPs of interest, there are exponentially many trajectories per x, which can make computing p\u03b8(x) costly by requiring dynamic programming. And, |X | is often exponentially large, making it infeasible to precisely characterize the target distribution. Several studies evaluate GFlowNets by spearman correlation between log p\u03b8(x) and R(x) on held-out data (Madan et al., 2022; Nica et al., 2022) , but this correlation is 1.0 when log p\u03b8(x) = cR(x) for any constant c > 0, even though it is only when c = 1 that the GFlowNet matches the target distribution.\nWe design our benchmarks with biological data and rewards, constrain them to have enumerable X , and use GFlowNet samples for evaluation rather than computing p\u03b8(x). This enables an exact, precise, and efficient view of how well GFlowNets match statistics of the target reward distribution. We study the Anderson-Darling statistic which computes goodness-of-fit between GFlowNet samples of reward and the target reward distribution, and also compare mean GFlowNet reward from samples, Ep\u03b8(x)[R(x)], to expected target reward R(x)2/Z.\nWe remark that this evaluation scheme only compares to the target reward distribution. Nevertheless, this scheme enables us to report that a key challenge in GFlowNet training is underfitting the target distribution, meaning that they sample low rewards with too high probability. We discuss under-\nfitting here and propose improvements, and defer benchmark details and experimental results to \u00a77.\nGFlowNets underfit target distributions during training. In our experiments (see \u00a77), mean GFlowNet sampling reward Ep\u03b8(x)[R(x)] initializes significantly below the target mean reward, and very slowly reaches, or never reaches the target mean reward, even over more than tens of thousands of active training rounds. At initialization with random parameters, we reason that GFlowNet mean sampling rewards is expected to be low, as P \u03b8F (st+1|st) has high entropy, so p\u03b8(x) generally also has high entropy (though it depends on the choice of MDP, see \u00a7A.1).\nThis is consistent with previous work, which has reported linear regression slope of 0.58 between log p\u03b8(x) and logR(x) on a small molecule task (Bengio et al., 2021a) after training completed, indicating oversampling of lowreward x. To encourage discovery of high reward x, it is common to train on rewards raised to a high exponent, such as 3 to 10. For instance, 10\u00d7 higher binding affinity molecules can be more than 1000\u00d7 rarer in X , reducing their relative probability unless reward is taken as binding affinity raised to a power. This increases reward skew, decreasing target distribution entropy and widens the gap to high-entropy GFlowNet initializations. In prior work, GFlowNets have failed to sufficiently increase sampled rewards when training on increasingly skewed rewards.\nRemark 1. A primary practical challenge during GFlowNet training is reducing their probability of sampling low-reward x.\nOur observations affect the practical use of GFlowNets. GFlowNet training is conventionally stopped at convergence, or when training resources are depleted, but our experiments show that GFlowNets can converge below the target mean, and fail to reach the target mean even after extensive training time. It has not been common practice to monitor the sampling mean reward nor to compare it to the target mean, which is typically unknown. As a consequence, in practice, many GFlowNets may oversample low-reward x, unbeknowst to users.\nThis motivates us to understand the training behavior of GFlowNets, and explore strategies that may assist them in learning to sample high rewards with higher probability more quickly. These strategies can help GFlowNets match the target distribution more quickly.\nReward-prioritized replay training (PRT). We propose prioritized replay training (PRT) that focuses on high reward data, as a simple strategy for increasing sampled rewards. At each training round, in addition to training on trajectories sampled from the training policy, we form a replay batch from X , all data seen so far, so that \u03b1% of the batch is sampled from the top \u03b2 percentile of R(x), and the rest of\nthe batch is sampled from the bottom 1\u2212 \u03b2 percentile. To train on the replay batch of x, we sample trajectories for them using PB , then train on the trajectories.\nReplay training is common in RL, where it can significantly improve sample efficiency (Fedus et al., 2020). A similar strategy is error-prioritized experience replay (Schaul et al., 2015), which preferentially samples replays with high temporal-difference error. (Jain et al., 2022) propose replay training with GFlowNets from a fixed initial dataset. In contrast, our reward-prioritized replay training is motivated by fixing the observation that GFlowNets can struggle with oversampling low-reward x."
        },
        {
            "heading": "4 Flow Distribution and Generalization",
            "text": "We now turn to studying flow distribution, which is how a GFlowNet chooses to flow \u201cunnormalized probability\u201d over nodes and edges in the MDP graph. In this section, we clarify how flow distribution is important for generalization and efficiently matching the target distribution. As flows are usually underdetermined, this clarification leads to the question: how can GFlowNets learn better flows?\nThe first factor in flow distribution is the user\u2019s choice of the data-generating MDP. For any given data x, a user usually enjoys ample choice among different MDPs. Strings can be generated autoregressively, by starting from an empty string, and iteratively appending characters. In this autoregressive MDP, there is exactly one trajectory ending in any x, which means there is only one flow compatible with R(x) over all x. However, if the legal action set is expanded to prepending and appending, then the resulting prepend/append MDP has 2k\u22121 trajectories ending in a string of length k, and there are many flows over the MDP compatible with R(x). Finally, note that graph-generating MDPs typically have more than two attachment points, and even more trajectories per x.\nThe original motivation for the GFlowNet machinery, and its primary novelty, are in MDPs with many trajectories per x (Bengio et al., 2021a). It is in this setting that autoregressive or standard RL methods learn biased generative probabilities, while GFlowNets do not. When trajectories and x are one-to-one, GFlowNets reduce to a known method: discrete-action soft Q-learning. As such, we focus on the many-to-one setting, where flows are underdetermined given R(x).\nThe role of generalization during training. When X is high-dimensional and large, it is useful to conceptually divide states, trajectories, and x into those seen in training so far, and those not yet seen. The quality of a GFlowNet\u2019s policy over X , in particular how closely p\u03b8(xunseen) matches p\u2217(xunseen), critically depends on how well a GFlowNet generalizes from the data seen so far in training. As GFlowNets train in a bootstrapping manner, improving gen-\neralization at any training step improves future training. Finally, GFlowNets can only ever train on a tiny fraction of X , so their final ability to match the target distribution also depends on generalization.\nAs p\u03b8(xunseen) depends on learned forward policy P \u03b8F or equivalently on learned edge flows F\u03b8(s\u2192 s\u2032), we state:\nRemark 2. Flow distribution matters for generalization, which matters for matching the target distribution over X . Moreover, flows are generally underspecified given data {x,R(x)}.\nThis remark grounds a notion of the quality of a flow distribution: a flow distribution is better if it helps a GFlowNet generalize better.\nRelative edge flow parametrization (SSR). Our clarification of the role of generalization in matching the target distribution led us to investigate inductive biases in GFlowNet parametrizations that could impact generalization behavior. GFlowNets typically use a policy parametrization, where P \u03b8F (s) is a neural net mapping S \u2192 A, which we call \u201cSA\u201d. If this policy learns that certain actions are favorable in a state s, it can generalize to prefer the same actions in new states similar to s, but this can sometimes be the wrong generalization. This concept is related to optimal transport regularization (Anonymous, 2023) to explicitly encourage similar action policies in similar states.\nWe propose an alternative parametrization, where relative edge flows are parametrized by a neural net f\u03b8 : S,S \u2032 \u2192 R+, which we call \u201cSSR\u201d. We define the probability of transitioning forward from s, s\u2032 as f\u03b8(s, s \u2032)/ \u2211 c\u2208children(s) f\u03b8(s, c). Intuitively, SSR can \u201csee\u201d the child state, unlike SA. Conceptually, SSR can help the GFlowNet generalize to favor taking different actions at new states than actions seen in training, based on the child state. We remark that relative edge flow parametrization is less efficient and no more expressive than SA.\nSSR is closely related to directly parametrizing state flows F (s) (for example, in the flow matching objective (Bengio et al., 2021a)) which may help the GFlowNet generalize from patterns of states with high and low flow to discover new states associated with high reward. Related work also includes (Madan et al., 2022) which parametrizes state flows to learn from partial episodes, and hypothesizes a benefit to state flow generalization, but their method does not use learned state flows for decision-making."
        },
        {
            "heading": "5 Compositionality and Credit Assignment",
            "text": "We have established that flow distribution is important for generalization to unseen x, and matching the target distribution over X . Flows are also generally underspecified given data {x,R(x)}, and it is an open question of whether\nGFlowNets prefer to learn certain flow distributions over others. In this section, we continue by investigating how GFlowNet learning objectives learn to distribute flow. In particular, we establish this remark (see fig. 3 for intuition):\nRemark 3. When there are many trajectories for each x, the manner in which trajectory balance and maximum entropy GFlowNets learn flows can inadequately credit substructures of x associated with high reward. Such substructures can exist when the reward function is compositional.\nWe build up to this remark in pieces: first discussing compositional reward functions, the substructure credit assignment problem, and finally studying TB and MaxEnt objectives.\nCompositional reward functions. We have established that generalization from seen R(X) to unseen x is important for GFlowNets to match the target distribution when X is large. However, it could be that R(x) has no learnable relationship with x - in such a case, generalization is impossible, and no flow distribution is better than any other.\nThus, the core premise that GFlowNets have an advantage over MCMC for unnormalized density estimation relies on the assumption that generalization of R(x) is possible. Compositional reward functions, where the value of R(x) depends primarily on properties of subparts or substructures of x and interactions among them (Andreas, 2019; Szabo\u0301, 2020), are an important example of such learnable R(x).\nFor example, important properties of small molecules are often caused by the presence of molecular substructures, such as benzene rings. These examples highlight that when R(x) is compositional, substructures of x can be associated with higher reward. We call these important substructures.\nGFlowNets that generate discrete objects iteratively take actions that combine simple building blocks, progressively passing through states corresponding to \u201cpartial objects\u201d to generate complete objects x. When R(x) is compositional, GFlowNets that assign higher flow to internal MDP states s corresponding to important substructures may generalize better by 1) increasing p\u03b8(xunseen) for downstream xunseen that contain s, increasing the probability of sampling xunseen associated with higher reward, and 2) enabling the GFlowNet to generalize from s and discover new substructures sunseen associated with high reward.\nThe substructure credit assignment problem. In reinforcement learning, the credit assignment problem concerns determining how the success of a system\u2019s overall performance is due to the various contributions of the system\u2019s components (Minsky, 1961). A long trajectory of actions is taken by a learning agent in an environment to receive a final reward, and the agent must learn to assign credit to the actions most responsible for the reward.\nWe argue that GFlowNets face an analogous challenge -\nassigning flow, or credit, to important substructures most responsible for R(x) - which we call the substructure credit assignment problem. But in RL, credit assignment is limited to actions taken in a specific trajectory arriving at reward. However, for GFlowNets, in the typical case where there are exponentially many trajectories ending in any x, the substructures most responsible for R(x) are often not on the training trajectory used to reach x.\nExisting learning objectives inadequately address the substructure credit assignment problem. Trajectory balance (TB) and MaxEnt objectives take gradients steps to minimize loss on the generative trajectory, from the training policy, used to reach x, which usually does not contain important substructures. Furthermore, TB underspecifies flow: many different flows all globally minimize loss, and there is no discernible inductive bias favoring any particular flow distribution. MaxEnt fixes PB as the uniform distribution, achieving a unique credit assignment solution, but one that diffuses credit for R(x) as widely as possible, assigning minimal credit to important substructures.\nPrior objectives under-credit important substructures. To formalize intuitions, we study the behavior of TB and MaxEnt in a simplified yet representative framework. We adopt the tabular model setting used to study algorithms in RL (Sutton & Barto, 2018), and consider a simple MDP with relevant properties. Due to space constraints, we provide informal summaries here, and complete statements and proofs in the appendix.\nDefinition 5.1 (Sequence prepend/append MDP setting). In this MDP, s0 is the empty string, states are strings, and actions are prepending or appending a symbol in an alphabet A. This MDP generates strings of length n. There is a fixed dataset {x, x\u2032} with R(x) = R(x\u2032). Denote s\u2217 as the important substructure, defined as longest substring shared by x, x\u2032, with length k. Denote sk(x) the set of k-length substrings of x.\nRemark 4. In this MDP, there are 2n\u22121 trajectories ending in any x, and each passes through exactly one string of length k, which is either s\u2217, or any other string in the set {sk(x) \\ s\u2217} where \\ denotes set subtraction. We study the flow F (s\u2217) passing through s\u2217, and the sum of flows passing through {sk(x) \\ s\u2217}, using F (sk(x) \\ s\u2217) ,\u2211 s\u2032\u2208{sk(x)\\s\u2217} F (s \u2032) . Proposition 5.2 (MaxEnt assigns minimal credit to important substructures). In setting 5.1, suppose the GFlowNet is trained with the maximum entropy learning objective. Then, at the global minima,\nE\n[ F (s\u2217)\nF (sk(x) \\ s\u2217)\n] = 2\nn\u2212 k (2)\nwith expectation over random positions of s\u2217 in x, x\u2032.\nThis proposition states that, for many values of n, k of practical interest, MaxEnt assigns a low minority of flow to s\u2217, and prefers to sample trajectories through non-important substructures with high probability.\nTo reason about trajectory balance in setting 5.1, we confine it to tabular trajectory flow updates, and ensure equal training steps on x, x\u2032.\nDefinition 5.3 (Tabular, fixed-data TB). Suppose that all tabular trajectory flows F (\u03c4) = at initialization, such that F (x) < R(x) for x, x\u2032 at initialization. We take m/2 training steps on each of x, x\u2032, sampling a trajectory \u03c4\u2192x ending in that x with probability proportional to current F (\u03c4\u2192x), then update F (\u03c4\u2192x)\u2190 F (\u03c4\u2192x) + \u03bb(R(x)\u2212 ). Proposition 5.4 (TB tends to reduce credit to important substructures). Suppose setting 5.1 and TB training 5.3. Let t index training steps. At any training step t, if Ft\u22121(s\u2217) < (n\u2212k)Es\u223csk(x)\\s\u2217 [Ft\u22121(s)] holds (with expectation under a uniform distribution), then the expected trajectory flow update over training trajectories \u03c4 has:\nE \u03c4\n[ Ft(s \u2217) ] \u2212Ft\u22121(s\u2217) <\nE \u03c4\n[ Ft(sk(x) \\ s\u2217) ] \u2212 Ft\u22121(sk(x) \\ s\u2217).\nTB learning updates prefer to increase flow through nonimportant substructures at the expense of s\u2217, except in the rare situation where F (s\u2217) is significantly larger, by a factor of (n\u2212 k), than the average flow through competing nonimportant substructures. In \u00a7C.7, we interpret TB as a Po\u0301lya urn process where \u201crich gets richer\u201d: trajectories with high flow are sampled more in training, further increasing flow.\nIn summary, TB and MaxEnt inadequately address the substructure credit assignment problem, and likely do not learn to distribute flow optimally."
        },
        {
            "heading": "6 Guided Trajectory Balance",
            "text": "We introduce a novel learning objective called guided trajectory balance that enables flexible control over credit assignment and flow distribution. We propose a specific guide that solves the substructure credit assignment problem.\nIn general, suppose that for some x and R(x), we would like to assign credit (i.e., flow) for R(x) preferentially along some trajectories. We express this preference over trajectories ending in x with a guide distribution p(\u03c4\u2192x).\nWe emphasize that the guide p(\u03c4\u2192x) does not have to be fixed, and does not have to be Markovian with respect to the MDP. A particularly powerful approach is defining guides using X , the set of all x collected so far during training, which we demonstrate in a following section. The resulting\nguide p(\u03c4\u2192x|X) changes with each new active round during training, as X grows.\nTo formulate a valid and general GFlowNet learning objective, we propose a guided trajectory balance constraint for a trajectory \u03c4\u2192x ending in x. Theorem 6.1 shows that if a GFlowNet satisfies this constraint at all x and all trajectories, then it samples from the target distribution, which achieves the same asymptotic guarantees as prior GFlowNet learning objectives.\nTheorem 6.1 (Guided trajectory balance constraint). Suppose that for any x, and any trajectory \u03c4\u2192x = (s0 \u2192 s1 \u2192 ... \u2192 sn = x) ending in x, the guided trajectory balance constraint (3) holds.\nZ n\u220f t=1 PF (st|st\u22121) = p(\u03c4\u2192x)R(x) (3)\nThen PF samples x with probability R(x)/Z. Proof. Use \u220fn t=1 PF (st|st\u22121) = PF (\u03c4\u2192x), then sum over all trajectories ending in x to get ZPF (x) = R(x).\nIn general, however, the guide distribution may not be Markovian, and constraint 3 cannot be satisfied everywhere by standard GFlowNets with Markovian flow. We solve this with a two-phase optimization approach. Note how guided trajectory balance (3) relates to trajectory balance (1), in that p(\u03c4\u2192x) plays the role of PB . We propose to first train PB to match p(\u03c4\u2192x), using:\nLback-GTB(\u03c4\u2192x) = ( log \u220fn t=1 P \u03b8 B(st|st\u22121)\np(\u03c4\u2192x)\n)2 (4)\nwhich learns a PB that acts as a Markov approximation to p(\u03c4\u2192x). Once converged, we freeze PB , then learn PF with fixed PB using trajectory balance, which recovers a proper GFlowNet learning objective: by the uniqueness property ((Bengio et al., 2021b), Corollary 1), for any fixed Markovian PB , there is a unique global minima to the trajectory balance loss which corresponds to a Markovian PF that samples from the target distribution.\nTraining considerations. Training trajectories can be sampled from the training policy. Alternatively, x can be sampled from the training policy, and a training trajectory \u03c4\u2192x can be resampled from the guide.\nIn practice, a useful PF can be learned faster by alternating updates to PB and PF . PF may also train on a target that mixes the current PB and guide p(\u03c4\u2192x) with weight \u03b1 \u2208 [0, 1]:\nLforward-GTB(\u03c4) = (\u03c8f \u2212 \u03c8b)2 (5)\n\u03c8f , logZ\u03b8 + n\u2211 t=1 logP \u03b8F (st|st\u22121) (6)\n\u03c8b , logR(x) + \u03b1 log p(\u03c4\u2192x) (7) + (1\u2212 \u03b1) ( n\u2211 t=1 logP \u03b8B(st|st\u22121) )\nSubstructure guide. We propose a particular guide suited for compositional reward functions, used to train substructure GFlowNets. This guide finds important substructures by looking at parts of x associated with high reward over all of X seen so far in training, and guides credit assignment towards these empirically important substructures.\nWe say that a state s is a substructure of x, or s \u2208 x, if there is a directed path in the MDP from s to x. A motivating insight is that for many compositional objects, \u2208 is efficient to compute (subset function for sets, substring function for strings). For graphs, \u2208 corresponds to the NP-complete graph isomorphism problem, but this is fast in practice when many node or edge features are distinct. Furthermore, \u2208 can always be computed by breadth-first search in the MDP DAG, which bounds its time complexity to O(V + E).\nOur guide defines p(\u03c4\u2192x|X) = \u220fn\u22121 t=0 p(st+1|st, x,X), where state transition probabilities are:\np(st+1|st, x,X) = \u03c6(st+1|x,X)\u2211\ns\u2032\u2208children(st) \u03c6(s \u2032|x,X)\n, (8)\nwith score function \u03c6(s|x,X) that favors children that are substructures of x with high average reward:\n\u03c6(s|x,X) =  E{x\u2032\u2208X:s\u2208x}R(x \u2032), if s \u2208 x\n0 otherwise. (9)\nFor ease of presentation, this description ignores corner cases. We provide a complete description in \u00a7A.2. In practice, parallelization and an efficiency trick reduce added overhead to negligible amounts (see \u00a7A.2).\nSubstructure GFlowNets credit important shared substructures. In setting C.3, we showed that MaxEnt and TB objectives inadequately credit the important shared substructure, s\u2217. In contrast, substructure GFlowNets assign maximal credit to s\u2217 (Proposition C.9), demonstrating that substructure-guided trajectory balance helps to solve the substructure credit assignment problem."
        },
        {
            "heading": "7 Experiments",
            "text": "To precisely and efficiently evaluate how well GFlowNets match the target reward distribution, we design benchmarks with enumerable X , and use GFlowNet sampled rewards for evaluation. The Anderson-Darling (AD) statistic, a statistical metric of goodness-of-fit between samples and a target distribution, is strongly correlated with error between mean of the samples and the target mean in our experiments (median R2 = 0.87, \u00a7B, Fig. B.1), and the AD statistic is near zero (indicating no significant difference) when the mean\u2019s error is near zero. As a result, we report error against the mean for ease of interpretation. In table 1, we report relative error of the mean at convergence, or number of training rounds required to first reach the target mean. In figure 1, we depict training curves for the same runs in table 1. We report mean \u00b1 standard error in table 1 and figure 1 using three replicates.\nIn five tasks, we report results from baselines TB and MaxEnt, and compare to the effects of our three proposed training modifications: prioritized replay training (PRT), relative edge flow policy parametrization mapping S \u00d7 S \u2192 R (SSR), and substructure guided trajectory balance (Sub). We used the ablation series of TB, TB+PRT, TB+PRT+SSR, and Sub+PRT+SSR to compare the individual effects of adding PRT, SSR, and substructure guidance respectively. We change the minimal number of hyperparameters and training details necessary to add each proposal (see \u00a7B for full experimental details).\nBag. (|X | \u2248 100B) A multiset of size 13, with 7 distinct elements. The base reward is 0.01. A bag has a \u201csubstructure\u201d if it contains 7 or more repeats of any letter: then it has reward 10 with 75% chance, and 30 otherwise. In this MDP, actions are adding an element to a bag.\nSIX6 (TFBind8). (|X | = 65, 536) A string of length 8 of nucleotides. The reward is wet-lab measured DNA binding activity to a human transcription factor, SIX6, from (LA et al., 2016; Trabucco et al., 2022). Following (Jain et al., 2022), we use a reward exponent of 3. Though an autoregressive MDP is conventionally used for strings, in order to compare TB with substructure guidance which is only meaningful in generative MDPs with more than one trajectory per x, we use a sequence prepend/append MDP. We compare to an autoregressive MDP in a following section.\nPHO4. (|X | = 1, 048, 576) 10 nucleotide string, with wetlab measured DNA binding activity to yeast transcription factor PHO4 (LA et al., 2016; Trabucco et al., 2022).\nQM9. (|X | = 58, 765) A small molecule graph. Reward is from a proxy deep neural network predicting the HOMOLUMO gap from density functional theory. We build using 12 building blocks with 2 stems, and generate with 5 blocks per molecule.\nsEH. (|X | = 34, 012, 224) A small molecule graph. Reward is from a proxy model trained to predict binding affinity to soluble epoxide hydrolase from AutoDock. We build using 18 blocks with 2 stems, and use 6 blocks per molecule."
        },
        {
            "heading": "Results.",
            "text": "PRT, SSR, and Sub improve GFlowNet training. In Table 1, we observe that across all 5 experimental settings, the best model includes at least one of our proposals PRT, SSR, and substructure guidance. The best model matches the target mean in the shortest number of rounds - in the case of sEH, 9-fold faster than TB - or achieves the highest mean when all models underfit the target distribution. We discuss the individual effects of each proposal in following sections.\nBaselines and GFlowNets can struggle with underfitting. Our benchmark results show that within a reasonable amount of training time, TB converges to policies that undersample the target mean and place too much weight on low rewards in two out of five environments. This is particularly notable in SIX6 where over 50,000 training rounds TB sampled 800,000 data points (batch size of 16 per round), a relative ratio over 12\u00d7 over the state space size |X | = 65, 536. This convergence under the mean despite significantly oversampling the state space size shows the difficulty of finding a global minima over all states and trajectories in practice, and may signal challenges in GFlowNets training in larger state spaces. Underfitting also varied by environment: all models struggled in PHO4 in particular.\nTB and MaxEnt are often similar. MaxEnt has a unique global optima from PB distributing flow uniformly, while TB could in principle learn different flows with lower entropy PB . Prior work has not yielded much insight into the similarities and differences between MaxEnt and TB. In our experiments, we empirically observe that the training curves of MaxEnt and TB are similar, and generally have worse sample efficiency and convergence than our proposals.\nPrioritized replay training improves sample efficiency. Prioritized replay training (PRT) improves on baselines. It achieves the highest performance in PHO4 out of all models, and reduces the number of rounds to match the target mean in sEH by 3-fold, from 45840 to 19390. In environments bag and SIX6, prioritized replay training does not improve the final converged GFlowNet (Table 1), but noticeably improves the sample efficiency of improving mean error in\nlearning curves (compare green vs. blue, Fig. 1).\nEffect of relative edge flow parametrization. Relative edge flow parametrization (SSR) further improves training by a significant amount (compare red vs. green, Fig. 1), and in particular noticeably shifts the learning curve in SIX6 by a factor up to 3\u00d7. In Bag and sEH, SSR improves the time to match the target mean by 2-fold. These experiments show that SSR can meaningfully change the training behavior of GFlowNets, and improve them substantially on some tasks. Further work may continue to investigate SSR and develop further improved parametrizations.\nEffect of substructure guidance. Substructure guidance has the biggest effect in SIX6 and sEH, where it enables the only model able to match the target mean in SIX6, and improves sample efficiency in matching the target mean by 2\u00d7 in sEH (Table 1). Overall, these results confirm that different credit assignment preferences can significantly impact GFlowNet training, convergence, and sample efficiency.\nThe benefit of our proposed substructure guide can depend on how compositional R(x) is. Further, failures of the substructure guide to improve training may be due to overfitting.\nAutoregressive MDP. In general, users can have many choices in designing generative MDPs. TFBind8 is a stringgeneration environment, for which autoregressive or appendonly MDPs have commonly been used for GFlowNet training. We trained a GFlowNet using an autoregressive MDP, and find final converged mean at 97.3% after 50,000 training rounds with a TB baseline. Note that in an autoregressive MDP, standard TB is equivalent to MaxEnt and substructure guidance has no effect, as there is only one trajectory per x. Adding PRT and PRT+SSR do not impact the final GFlowNet policy, achieving 96.9% and 96.6% respectively, but significantly improve sample efficiency, reaching their best policies by rounds 8000 and 3500 respectively, which is a 6-fold and 14-fold improvement (Fig. 2).\nCan it be beneficial to use an MDP with more trajectories per x? For SIX6, we show the answer is that it can be. Using a prepend/append MDP alongside substructure guidance, PRT, and SSR is the only combination in our experiments that managed to match the target mean in SIX6 (Table 1). This result shows that favoring autoregressive MDPs may not always be best, as their constraints on building order can limit which x can be built from certain substructures.\nMode discovery and diversity. We confirm that improved sample efficiency and sampling higher reward x with higher frequency also improves mode discovery, while retaining diversity. In sEH, the TB baseline discovered 140 modes by active round 10,000, while Sub+PRT+SSR discovered 867 modes (6\u00d7 more). The Tanimoto diversity (\u2191 is better) among the top 100 reward-ranked samples was 0.55, slightly better than 0.517 for the TB baseline. This result is consistent with other experiments, where we found that training strategy had no significant impact on sample diversity."
        },
        {
            "heading": "8 Discussion",
            "text": "In this work, we have identified challenges with GFlowNet training to learn to match the target distribution in a reasonable number of training steps, and contributed a conceptual understanding of GFlowNet training in terms of generalization, flow distribution, and credit assignment. We evaluated three proposals for improving GFlowNet training: prioritized replay training, relative edge flow parametrization, and substructure-guided trajectory balance.\nWe discussed challenges in GFlowNet evaluation, and studied GFlowNet learning behavior on benchmark tasks with biochemical data using the Anderson-Darling statistic between GFlowNet sampled rewards and the target reward distribution, and the difference in sampled mean to target mean. We learned that a key challenge in GFlowNet training is learning to not undersample the mean. However, by evaluating with sampled rewards, we sacrificed exactness for efficiency. While matching the target mean is better than undersampling it, the target mean can be matched while p\u03b8(x) 6= p\u2217(x) for all x. In future work, more insights into GFlowNet learning behavior may be uncovered by more thoroughly comparing p\u03b8(x) to p\u2217(x) on enumerable MDPs with real-world reward functions.\nWe have shown that altering flow distributions can significantly change generalization and training behavior, and developed a novel training objective that enables flexible control over credit assignment. While we have defined one notion of optimality of flow distribution based on generalization and convergence, it remains an open question how best to learn favorable flow distributions."
        },
        {
            "heading": "A Appendix: Notes",
            "text": "A.1 Oversampling low-reward x at initialization Using default PyTorch neural network initializations, we find that PF (st+1|st) outputting real numbers taken as logits has close to a uniform distribution. We can understand this with a simple model: suppose PF is exactly a uniform distribution, all states have exactly the same number of actions A, and all trajectories have the same length n: then the probability of any trajectory is 1/An. It is difficult to precisely characterize the entropy of p\u03b8(x) at random neural net initialization because it depends on how many trajectories end in each x in the MDP; denote this Nx. We can extend the above simple model to find that at initialization, the flow ending in x is proportional to Nx/An. Thus p\u03b8(x) has highest entropy and is the uniform distribution when Nx is identical for all x. However, if Nx varies substantially, then p\u03b8(x) will have lower entropy.\nHowever, we note that the common practice of using high reward exponents (from 3 to 10) typically induces a very low entropy reward distribution, which can make it very unlikely in practice that at initialization, a GFlowNet\u2019s sampled mean is higher than the target mean.\nA.2 Substructure Guide. Our substructure guide uses a scoring function to favor children states s that are a member of x in X with high reward.\nIn equation 9, we presented a simplified scoring function for presentation purposes. The exact scoring function is:\n\u03c6(s|xtarget, X) =  E{x\u2208X\\{xtarget}:s\u2208x}R(x), if s \u2208 xtarget0, otherwise, (10) In words: the score \u03c6 scores a state s, given target xtarget and X , the set of all observed x so far in training. If the state s is not in (\u2208) the target x, then the score is 0 (bottom line). If the state s is in the target (the state s is a substructure of the target x), then the score is the average reward over the subset of x in X , where for each x in that subset, s is a substructure of that x. This subset excludes the target: we only care about other x in X that contains s.\nWhen this scoring function is used to build a guide distribution over trajectories ending in xtarget, it prefers trajectories along paths that include substructures/states s that are substructures of high-reward x, other than xtarget.\nThe main difference with the simplified equation 9 is that here, we exclude the target x from the collection X . This focuses on substructures that are shared with other x.\nOne corner case is when no child states are members of any x in X except for the target x: then the score for all children is zero. In this case, we set p(st+1|st) to the uniform distribution over children s that are members of the target x. We refer readers to the code for a very precise description of the algorithm.\nImplementation details. We parallelize guide computation on CPUs, and observe no significant overhead on GPU model training, at the cost of using slightly out-of-date X . We sample guide trajectories efficiently by progressively filtering X as the sampled trajectory lengthens: note that if a state s is not in some x, then any descendant of s cannot be in x.\nA.3 Additional figures."
        },
        {
            "heading": "B Appendix: Experimental Details",
            "text": "Our code is available at https://github.com/maxwshen/gflownet.\nImplementation details. We found it useful to clip gradient norms to a maximum of 10.0. We also clipped policy logit predictions to a minimum of -50.0 and a maximum of 50.0. We initialized log Z\u03b8 to 5.0, which is less than the true Z = \u2211 xR(x) in every environment. In our experiments, every active training round we sampled a batch of 16 x using the current training policy, and updated the model using the sample batch, then optionally performed one replay training update. For monitoring, every 10 active rounds we sampled 128 x from the GFlowNet policy (not the training policy). To evaluate the model at a certain round while reducing sample noise, we aggregate over all true policy samples from the last 500 rounds, for an effective sample size of 6400.\nThere are several design choices for substructure gflownets. One can sample training trajectories from the training policy, or sample x from the training policy and resample a training trajectory using the guide distribution. We found that the former worked better in practice, though it can increase loss and gradient norm variance, which bounding gradient norm helped with. The mixing weight, \u03b1, in equation 5 is another choice. We found that \u03b1 = 1 was generally preferred, which eliminates the PB term and focuses solely on the guide likelihood term.\nFor prioritized replay training, we focus on the top 10% ranked by reward and randomly sample among them to be 50% of the batch, with the bottom 90% ranked comprising the remainder of the batch.\nBag. (|X | \u2248 100B) A multiset of size 13, with 7 distinct elements. The base reward is 0.01. A bag has a \u201csubstructure\u201d if it contains 7 or more repeats of any letter: then it has reward 10 with 75% chance, and 30 otherwise. In this MDP, actions are adding an element to a bag. As this is a constructed setting, we use a small neural net policy with two layers of 16 hidden units. We use an exploration epsilon of 0.10.\nSIX6 (TFBind8). (|X | = 65, 536) A string of length 8 of nucleotides. The reward is wet-lab measured DNA binding activity to a human transcription factor, SIX6, from (LA et al., 2016; Trabucco et al., 2022). Following (Jain et al., 2022), we use a reward exponent of 3. Though an autoregressive MDP is conventionally used for strings, in order to compare TB with substructure guidance which is only meaningful in generative MDPs with more than one trajectory per x, we use a sequence prepend/append MDP. We use a neural net with two layers of 128 hidden units and an exploration epsilon of 0.01.\nPHO4. (|X | = 1, 048, 576) 10 nucleotide string, with wet-lab measured DNA binding activity to yeast transcription factor PHO4 (LA et al., 2016; Trabucco et al., 2022). We use a reward exponent of 3, and scale rewards to a maximum of 10. We use a neural net with three layers of 512 hidden units and an exploration epsilon of 0.01.\nQM9. (|X | = 58, 765) A small molecule graph. Reward is from a proxy deep neural network predicting the HOMO-LUMO gap from density functional theory. We use a reward exponent of 5, and scale rewards to a maximum of 100. We build using 12 building blocks with 2 stems, and generate with 5 blocks per molecule. As all the blocks have two stems, we treat the MDP as a sequence prepend/append MDP. In general, the stem locations on the graph blocks are not symmetric, so reward is not invariant to string reversal. We use a neural net with two layers of 1024 hidden units and an exploration epsilon of 0.10. We measure diversity using 1 - Tanimoto similarity.\nsEH. (|X | = 34, 012, 224) A small molecule graph. Reward is from a proxy model trained to predict binding affinity to soluble epoxide hydrolase from AutoDock. Specifically, we trained a gradient boosted regressor on the graph neural network predictions from the proxy model provided by (Bengio et al., 2021a) over the entire space of X , with the goal of memorizing the model. Our proxy achieved a mean-squared error of 0.375, and pearson correlation of 0.905. We use a reward exponent of 6, which is less than 10 used in prior work, and scale rewards to a maximum of 10. We build using 18 blocks with 2 stems, and use 6 blocks per molecule. As all the blocks have two stems, we treat the MDP as a sequence prepend/append MDP. In general, the stem locations on the graph blocks are not symmetric, so reward is not invariant to string reversal. We use a neural net with two layers of 1024 hidden units and an exploration epsilon of 0.05. We define modes as the top 0.5% of X as ranked by R(x). We measure diversity using 1 - Tanimoto similarity.\nB.1Anderson-Darling statistic. In figure B.1, we depict the relationship between the Anderson-Darling statistic between GFlowNet samples and the target distribution, and the relative error between the GFlowNet sampled mean and the target mean."
        },
        {
            "heading": "C Appendix: Proofs",
            "text": "In this section, we study the credit attribution behavior of various GFlowNet learning objectives in a simplified yet representative setting."
        },
        {
            "heading": "C.0.1 DEFINITIONS",
            "text": "Definition C.1. (Sequence prepend/append MDP). A sequence prepend/append MDP is an MDP where s0 is the empty string, states correspond to strings, and actions are prepending or appending a symbol in some discrete alphabet A to a string. We consider this MDP to only generate strings x of length n, i.e., the exit action only occurs at states corresponding to strings of length n, and the exit action is the only action available at those states.\nRemark 5. While simple, this is representative of many MDPs that construct compositional objects by combining subparts, such as building graphs by connecting new nodes to existing nodes (often, > 2 insertion points that increase as the graph enlarges).\nDefinition C.2. (Tabular GFlowNet, with trajectory flow parameterization). A tabular GFlowNet uses a table to store the flow network. We consider a trajectory flow parameterization, where the trajectory flow F : T \u2192 R\u22650 is stored as a table. In contrast to the function approximation setting, updating any entry in the table does not change any other entry. We assume all trajectory flows F (\u03c4) are uniformly initialized to a small positive constant . A trajectory flow F (\u03c4) is updated with y with F (\u03c4)\u2190 F (\u03c4) + \u03bb(y \u2212 F (\u03c4)) for some step size 0 < \u03bb \u2264 1. Remark 6. The tabular setting has historically been used to develop, motivate, and study properties of reinforcement learning algorithms (Sutton & Barto, 2018). Note that the trajectory flow parameterization is inefficient in practice, because computing PF on a single state can require summing over exponentially many trajectories. However, it is a useful parameterization for theoretical analysis for various reasons.\nDefinition C.3. (Setting A). This setting comprises a sequence prepend/append MDP, and a fixed dataset {x, x\u2032}, each of length n, with R(x) = R(x\u2032). Let s\u2217 denote the longest string shared by x, x\u2032, with length k < n: i.e., there is no string of length k + 1 shared by both x, x\u2032. Let sk(x) denote the set of k-length substrings of some x.\nRemark 7. In this setting, every trajectory is a sequence of strings of length (0, 1, 2, ..., n), thus every trajectory contains exactly one substring of length k for every k \u2264 n. As a result, a trajectory ending in x must pass through either s\u2217 (the shared substring) or some substring s\u2032 \u2208 sk(x) (a non-shared substring). Our analysis will concern the credit assignment behavior of various learning objectives in how they assign flow to F (s\u2217) compared to F (sk(x) \\ s\u2217)."
        },
        {
            "heading": "C.0.2 PROPERTIES",
            "text": "We now state various properties about setting C.3.\nProperty 1. (Exponentially many trajectories for each x). In a sequence prepend/append MDP, there are 2n\u22121 trajectories that end in a specific string x of length n.\nProof. To build a string s of length n, we can choose an initial zero-based index i for our first character, then we can take i prepend actions and n\u2212 i\u2212 1 append steps in any order. There are ( n\u22121 i ) n\u2212 1 choose i such sequences of prepend and\nappend actions. Summing over choices of i, which can be viewed as summing over a row of Pascal\u2019s triangle, there are\u2211n\u22121 i=0 ( n\u22121 i ) = 2n\u22121 trajectories.\nRemark 8. This property shows how \u201ceasy\u201d it is to design an MDP with exponentially many trajectories for each x. Note that an autoregressive, append-only MDP has exactly one trajectory for each x. The sequence prepend/append MDP has two \u201cinsertion points\u201d, points at which new content can be added, and has exponentially many trajectories for each x. In general, MDPs that construct objects by combining building blocks will likely have many insertion points. MDPs that do not have specific insertion positions, such as sets, also have at least exponentially many trajectories for each x.\nProperty 2. (Number of trajectories through a state s to some x). In a sequence prepend/append MDP, suppose s of length k is preceded by a characters in x of length n. There are ( n\u2212k a ) 2k\u22121 trajectories passing through s that end at x.\nProof. There are 2k\u22121 trajectories from the root to s (property 1). There are ( n\u2212k a ) action sequences corresponding to\ndifferent orderings of prepending and appending, to start from s and end at x. So in total there are ( n\u2212k a ) 2k\u22121 trajectories passing through s\u2217 that end at x.\nProperty 3. Uniform trajectory flow distribution \u21d0\u21d2 Uniform forward and backward policies. In a sequence prepend/append MDP, the flow has a uniform trajectory flow distribution F (\u03c4) if and only if PF and PB are uniform at every state.\nProof. (Uniform trajectory flow distribution =\u21d2 Uniform forward and backward policies). Let the uniform F (\u03c4) = . Recall that F (s, s\u2032) = \u2211 \u03c4\u2208T :(s,s\u2032)\u2208\u03c4 F (\u03c4). Suppose s has length k and therefore s\n\u2032 has length k + 1. Using property 1, there are 2k\u22121 trajectories from s0 to s, and there are 2n\u2212k\u22121 trajectories from s\u2032 to any string of length n, for a total of 2n\u22122 trajectories that pass through the edge s\u2192 s\u2032. Using F (\u03c4) = , we have for any s with length k and s\u2032 with length k + 1, F (s, s\u2032) = 2n\u22122 .\nAt any state corresponding to a string of length k, PF is only a distribution over strings of length k + 1, and PB over k \u2212 1, because the MDP always adds a single character with every action. As PF and PB sample with probability proportional to the edge flow, and the edge flow is identical for every edge, PF and PB are uniform.\nProof. (Uniform trajectory flow distribution \u21d0= Uniform forward and backward policies).\nAny trajectory \u03c4 = (s0, s1, ..., sn) is a sequence of strings of length (0, 1, 2, ..., n). We consider the forward policy first. Note that at s0, there are |A| actions/children, corresponding to adding each letter in the alphabet A. For any string of length 1 to n\u2212 1, there are 2|A| actions/children. For any string of length n, there is 1 action (exiting). Using this, we have for any \u03c4 ,\np(\u03c4) = PF (s1|s0) n\u22121\u220f t=1 PF (st+1|st) (11)\n= 1\n|A| n\u22121\u220f t=1 1 2|A| . (12)\nAs this doesn\u2019t depend on the states in \u03c4 , we have p(\u03c4) = p(\u03c4 \u2032) for any trajectories \u03c4, \u03c4 \u2032 \u2208 T . Note that F (\u03c4) = Zp(\u03c4).\nThe backward policy case proceeds similarly, using the observation that each string of length 2 to n has 2 parents."
        },
        {
            "heading": "C.0.3 MAXIMUM ENTROPY GFLOWNETS",
            "text": "Recall that the maximum entropy learning objective learns P \u03b8F using the trajectory balance constraint:\nZ\u03b8 n\u22121\u220f t=0 P \u03b8F (st+1|st) = R(x) n\u22121\u220f t=0 PB(st|st+1) (13)\nfor a trajectory \u03c4 ending in x. The maximum entropy objective fixes PB(st\u22121|st) as the uniform distribution. The trajectory balance constraint is an algebraic manipulation of the simpler detailed balancing constraint\nF (s)PF (st+1|st) = F (st+1)PB(st|st+1) (14)\nthat \u201cchains\u201d the detailed balancing constraint over a complete trajectory. Other constraints can be derived by extending the chain over partial trajectories. Note that the trajectory balance constraint with a uniform PB fully determines PF (st+1|st) when all R(x) are known.\nTheorem C.4. (Maximum entropy GFlowNets attribute minimal credit to shared substructures). In setting C.3, suppose the GFlowNet is trained with the maximum entropy learning objective. Then, at the global minima,\nE\n[ F (s\u2217)\nF (sk(x) \\ s\u2217)\n] = \u0398 ( 1\nn\u2212 k\n) (15)\nover uniformly random positions of s\u2217 in x, x\u2032.\nRemark 9. As all trajectories to x must pass through either the shared substring s\u2217 (with length k) or a non-shared substring in sk(x)\\s\u2217, credit assignment for x, r(x) can be evaluated by how much flow, or reward, is assigned to s\u2217 versus sk(x)\\s\u2217. This theorem states that a minority of credit is assigned to s\u2217, and it decreases linearly with n\u2212 k.\nProof. As a uniform backward policy induces a uniform trajectory distribution over trajectories connecting x to the root (property 3), and there are 2n\u22121 such trajectories (property 1), each trajectory ending in x has trajectory flow r(x)/2n\u22121.\nBy property 2, there are ( n\u2212k a ) 2k\u22121 trajectories passing through s\u2217 that end at x. Multiplying by the flow over each trajectory, the total flow passing through s\u2217 ending at just x is\n( n\u2212k a ) 2k\u22121R(x)\n2n\u22121 . (16)\nThe average number of trajectories over a uniform distribution on a, which can range from 0 to n\u2212 k, is:\n1\nn\u2212 k + 1 n\u2212k\u2211 a=0 ( n\u2212 k a ) = 2n\u2212k n\u2212 k + 1 = E a [( n\u2212 k a )] (17)\nThus, the expected flow passing through s\u2217 and ending at just x, over uniformly random positions of s\u2217 in x is:\nE a\n[( n\u2212k a ) 2k\u22121R(x)\n2n\u22121\n] = R(x)\nn\u2212 k + 1 (18)\nThe total expected flow of s\u2217 to both x, x\u2032 is R(x)+R(x \u2032) n\u2212k+1 , while the total expected flow through sk(x) \\ x \u2217 is (n\u2212k)R(x)n\u2212k+1 . When R(x) = R(x\u2032), the ratio is 2/(n\u2212 k), which scales as \u0398(1/(n\u2212 k)).\nRemark 10. In a set MDP, a similar argument shows that the fraction of flow going through s\u2217 out of all flow going through all subsets of size k scales as \u0398(1/k!)."
        },
        {
            "heading": "C.0.4 TRAJECTORY BALANCE GFLOWNETS",
            "text": "The trajectory balance learning objective is more challenging to analyze because it has many global minima which are reached by a self-modifying learning procedure, where in each step the GFlowNet policy generates samples x which are used to update the policy.\nDefinition C.5. (Trajectory balance training procedure: Fixed dataset, tabular GFlowNet setting).\nThis definition builds on setting C.3, and the definitions of a tabular GFlowNet (definition C.2). We suppose that , the initial value of all trajectory flows, is sufficiently small and R(x) is sufficiently large such that, at initialization,\nFinit(x) < R(x). (19)\nWe consider training for m steps, with some learning rate \u03bb. During each training step, we:\n1. Sample a trajectory \u03c4 ending in x or x\u2032 according to the current GFlowNet policy. Under our tabular trajectory flow parameterization, we presume this is done by sampling a trajectory with probability proportional to its flow, among all trajectories ending in either x, x\u2032 (see remark 12).\n2. The GFlowNet is updated according to the trajectory balance learning objective. We update F (\u03c4)\u2190 F (\u03c4) + \u2206, where we label \u2206 = \u03bb(R(x)\u2212 Finit(\u03c4)) as the positive, constant flow update.\nIf \u03bb = 2/m, then when we have performed exactly m/2 training steps on x and x\u2032, we have F (x) = R(x) and F (x\u2032) = R(x\u2032). Remark 11. This training procedure is closely related to standard GFlowNet active learning schemas (Bengio et al., 2021a). The condition that every trajectory ends in an x in the fixed dataset is also used in the backward trajectory data augmentation training procedure used in (Zhang et al., 2022; Jain et al., 2022). This setting is conceptually equivalent to standard active learning when R(x) has the property that learning on any x\u2032\u2032, R(x\u2032\u2032) for x\u2032\u2032 /\u2208 X negligibly impacts the learned flow network. Note that this training procedure has full support over states and trajectories in a tabular GFlowNet where all trajectory flows are initialized to a small positive constant. Remark 12. (Non-Markovian trajectory sampling). Sampling a trajectory proportional to its flow when each trajectory has a separate tabular flow value induces a non-Markovian trajectory distribution, which acts as a relaxation of the standard GFlowNet procedure where trajectories are sampled according to the Markovian policy PF or PB . For theoretical analysis, this simplifying assumption ensures that the principle of \u201ctabular independence\u201d is obeyed for trajectory sampling - changing the flow of some trajectory \u03c4 does not change the relative likelihood ratio of sampling any other two trajectories \u03c4 \u2032, \u03c4 \u2032\u2032.\n\u2217 \u2217 \u2217 Theorem C.6. (Trajectory balance tends to reduce credit to shared substructures). In setting C.3, suppose the GFlowNet is trained with trajectory balance according to the steps in C.5. Let t index training steps. At any training step t, if\nFt\u22121(s \u2217)\nEs\u223csk(x)\\s\u2217 [Ft\u22121(s)] < n\u2212 k (20)\nholds (where the expectation is on a uniform distribution over the set), then the expected learning update over training trajectories has:\nE \u03c4\n[ Ft(s \u2217) ] \u2212 Ft\u22121(s\u2217) < E\n\u03c4\n[ Ft(sk(x) \\ s\u2217) ] \u2212 Ft\u22121(sk(x) \\ s\u2217). (21)\nRemark 13. This proposition states that, unless F (s\u2217) dominates F (s) for any s that is not a shared k-length substring, each trajectory balance training step tends to increase flow more for non-shared substrings than for s\u2217.\nProof. Recall the following definitions: s\u2217 is the longest substring shared between x, x\u2032, and has length k. The set of k-length strings in x is denoted by sk(x), and there are n\u2212 k + 1 such strings.\nAs any trajectory from x to s0 must pass through exactly one string in sk(x), and the state flow is defined as the total flow summed over all trajectories passing through that state, the probability of sampling a trajectory that passes through s\u2217 is\np(s\u2217) = F (s\u2217)\nF (sk(x)) = F (s\u2217)\u2211 s\u2208sk(x) F (s) . (22)\nSuppose Ft\u22121(s\u2217) < Ft\u22121(sk(x) \\ x\u2217). Using the fact that there are n\u2212 k items in sk(x) \\ x\u2217, this condition is equivalent to equation 20. This means that pt\u22121(s\u2217) < pt\u22121(sk(x) \\ s\u2217).\nNote that the expected increase in flow from one training step is:\nE \u03c4\n[ Ft(s \u2217) ] \u2212 Ft\u22121(s\u2217) = \u2206pt\u22121(s\u2217) (23)\nE \u03c4\n[ Ft(sk(x) \\ s\u2217) ] \u2212 Ft\u22121(sk(x) \\ s\u2217) = \u2206pt\u22121(sk(x) \\ s\u2217) (24)\nUsing pt\u22121(s\u2217) < pt\u22121(sk(x) \\ s\u2217), we arrive at our proposition.\n\u2217 \u2217 \u2217\nWhen trajectories are sampled according to their flow and not by a Markovian policy, the \u201crich-get-richer\u201d property can be understood through a Po\u0301lya urn model.\nTheorem C.7. (Trajectory balance training as a Po\u0301lya urn model). In setting C.3, suppose the GFlowNet is trained with trajectory balance according to the steps in C.5. After m training steps, we have:\np\n( Ffinal(s\n\u2217)\u2212 Finit(s\u2217) Ffinal(sk(x, x\u2032))\u2212 Finit(sk(x, x\u2032)) > \u03c8\n) \u2264 (25)\n1\u2212 BetaBinomialCDF ( \u03c8m;n = m, \u03b1\n\u03b1+ \u03b2 =\ne\n\u03c0 \u221a n\u2212 k\n, \u03b1+ \u03b2 = 2n\u22121\n\u2206\n) (26)\nfor e \u03c0 \u221a n\u2212k \u2264 \u03c8 \u2264 1 and n\u2212 k \u2265 3. Remark 14. A consequence of the Po\u0301lya urn model is that the fraction of total flow attributed during training that passes through s\u2217 follows a beta-binomial distribution. As each training step increases flow by a constant amount, this fraction is equal to the fraction of trajectories sampled through s\u2217 during the \u201crich-get-richer\u201d training process.\nProof. The Po\u0301lya urn model applies as follows: we have one color of ball in the urn for each k-length substring in either x, x\u2032, where we recall that s\u2217 is the only k-length substring shared by both x, x\u2032. There are therefore 2(n\u2212 k) + 1 colors. We denote the set of k-length substrings in either x, x\u2032 as sk(x, x\u2032). At each training step, we sample a trajectory \u03c4 ending in either x, x\u2032 which must pass through exactly one s \u2208 sk(x, x\u2032) with probability proportional to F (s); in the Po\u0301lya urn, this corresponds to sampling a ball from the urn with color c. We then increment the flow along \u03c4 . Due to the tabular trajectory flow parameterization, and tabular trajectory sampling assumption in C.5, this incrementing step does not change the flow to any other state in sk(x). In the Po\u0301lya urn, this corresponds to adding a ball with color c to the urn.\nThe distribution of ball colors sampled from the Po\u0301lya urn, after m steps, follows a Dirichlet-multinomial distribution. As we only care about s\u2217 compared to sk(x, x\u2032) \\ s\u2217, we can reduce to two colors, which follows a Beta-binomial distribution. We label \u2019white balls\u2019 for s\u2217 and \u2019black balls\u2019 for sk(x, x\u2032) \\ s\u2217.\nAt initialization, the number of white balls for s\u2217 is determined by F (s\u2217|x, x\u2032), which depends on its position in x and x\u2032 (property 2). We apply an upper bound on F (s\u2217|x) to remove this dependence, which will let us treat x, x\u2032 identically and simplify our exposition.\nRecall that a denotes how many characters precede s\u2217 in x. We use the upper bound (see lemma C.8):\nmax 0\u2264a\u2264n\u2212k ( n\u2212 k a ) \u2264 e2 n\u2212k \u03c0 \u221a n\u2212 k . (27)\nFor some x\u0302 in {x, x\u2032}, recall that there are 2n\u22121 trajectories ending in x\u0302. We upper bound the number of trajectories passing through s\u2217 ending in x\u0302 as:\n( n\u2212 k a ) 2k\u22121 \u2264 e2 n\u2212k \u03c0 \u221a n\u2212 k 2k\u22121 (28)\nAs both x, x\u2032 are the same length, and s\u2217 is the only k-length substring in x and x\u2032, the upper bound on the proportion of trajectories with s\u2217 are equal for x and x\u2032. This corresponds to an \u201cupper-bounded\u201d Po\u0301lya urn where the number of white balls for trajectories with s\u2217 ending in x or x\u2032 is proportional to 28, and the number of black balls for any other trajectory ending in x or x\u2032 is proportional to 2n\u22121. We continue our analysis using this upper-bounded Po\u0301lya urn.\nWe now solve for the parameters of the beta-binomial distribution that models the urn\u2019s draws. Let \u03b1 denote the initial number of white balls for s\u2217. As \u201cone ball\u201d is \u2206 flow,\n\u03b1 = e2n\u2212k\n\u03c0 \u221a n\u2212 k\n2k\u22121\n\u2206 (29)\nSimilarly, let \u03b1+ \u03b2 denote the initial total number of balls.\n\u03b1+ \u03b2 = (2n\u2212k) 2k\u22121\n\u2206 (30)\nWhen n\u2212 k = 3, we have \u03b1(\u03b1+\u03b2) = e \u03c0 \u221a n\u2212k = 0.499555773, which is the mean of the normalized beta binomial confined between 0 and 1. As the beta binomial distribution\u2019s mean and variance are largest when \u03b1/(\u03b1+ \u03b2) = 0.5, and decrease as \u03b1/(\u03b1 + \u03b2) decreases below 0.5, we have that for any threshold \u03c8 greater than or equal to the mean \u03b1/(\u03b1 + \u03b2), our upper bounded beta binomial has a larger cumulative probability density above \u03c8 than any beta binomial with smaller mean. Therefore, our upper-bounded beta binomial satisfies the bounds in the proposition.\n\u2217 \u2217 \u2217 Lemma C.8. (Upper bound on the largest element in a row of Pascal\u2019s triangle).\nFor any natural number n,\nmax 0\u2264a\u2264n\n( n\na\n) \u2264 e2 n\n\u03c0 \u221a n . (31)\nProof. Note that the series ( n 0 ) , ( n 1 ) , ..., ( n n ) corresponds to a row of Pascal\u2019s triangle.\nFirst, consider when n is even. Then ( n a ) is maximized when a = n/2. We therefore seek an upper bound to ( n n/2 ) . We use Stirling\u2019s approximation:\n( n\nn/2\n) \u2264 n!\n(n2 !) 2\n(32)\n\u2264 en n+ 12 e\u2212n\n( \u221a\n2\u03c0(n2 ) n+1 2 e\u2212n/2)2\n(33)\nFollowing algebraic manipulations, we get\n( n\nn/2\n) \u2264 e2 n\n\u03c0 \u221a n\n(34)\nNow, consider when n is odd. Then ( n a ) is maximized when a is n/2 rounded up or down. This corresponds to ( n n+1 2 ) ,\nwhich we note is equal to ( n n\u22121 2 ) . By the recurrence relation of Pascal\u2019s triangle, we have\n( n n+1 2 ) + ( n n\u22121 2 ) = ( n+ 1 n+1 2 ) (35)(\nn n+1 2\n) = 1\n2 ( n+ 1 n+1 2 ) . (36)\nApplying the same upper bounding strategy for the even case on (n+1 n+1 2 ) , we get\n( n n+1 2 ) \u2264 1 2 e2n+1 \u03c0 \u221a n+ 1\n(37)\n\u2264 e2 n\n\u03c0 \u221a n+ 1\n(38)\nAs this upper bound is less than the upper bound of e2 n\n\u03c0 \u221a n , we can apply that bound whether n is odd or even."
        },
        {
            "heading": "C.0.5 SUBSTRUCTURE GFLOWNETS",
            "text": "Theorem C.9. (Substructure GFlowNets attribute credit to shared substructures). In setting C.3, suppose the GFlowNet is trained as a substructure GFlowNet. Then, at the global minima over learned Markovian policies PB , PF ,\nF (s\u2217) = R(x) +R(x\u2032), (39) F (sk(x) \\ s\u2217) = 0. (40)\nProof. As s\u2217 is the only k-length substring in both x, x\u2032, all trajectories ending in x, x\u2032 sampled by the substructure-aware guide distribution must include s\u2217. When PB reaches a global minima in matching the guide distribution, it will also have this property, because this property is Markov: for any sk+1 of length k + 1 connected to s\u2217, have PB(s\u2217|sk+1) = 1. When PF reaches a global minima, it therefore must assign all flow from R(x), R(x\u2032) through s\u2217.\nRemark 15. In practice, it may be preferable to not assign all credit for R(x), R(x\u2032) to a single substring s\u2217; in some situations, this may correspond to overfitting. Other credit assignments are easily achievable by other guide distribution designs, or mixing the substructure GFlowNet guide distribution with a uniform guide distribution."
        }
    ],
    "title": "Towards Understanding and Improving GFlowNet Training",
    "year": 2023
}