{
    "abstractText": "Active, non-parametric peak detection is considered. As a use case, active source localization is examined and an uncertainty-based sampling scheme algorithm to effectively localize the peak from a few energy measurements is designed. It is shown that under very mild conditions, the source localization error with m actively chosen energy measurements scales as O(log m/m). Numerically, it is shown that in low-sample regimes, the proposed method enjoys superior performance on several types of data and outperforms the state-of-the-art passive source localization approaches and in the low sample regime, can outperform greedy methods as well.",
    "authors": [
        {
            "affiliations": [],
            "name": "Praneeth Narayanamurthy"
        }
    ],
    "id": "SP:bd44f356fc4f87ff2e6b1dc521a1dae7b8c046ac",
    "references": [
        {
            "authors": [
                "M. Cobos",
                "F. Antonacci",
                "A. Alexandridis",
                "A. Mouchtaris",
                "B. Lee"
            ],
            "title": "A survey of sound source localization methods in wireless acoustic sensor networks",
            "venue": "Wireless Communications and Mobile Computing, vol. 2017, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "P.-A. Grumiaux",
                "S. Kiti\u0107",
                "L. Girin",
                "A. Gu\u00e9rin"
            ],
            "title": "A survey of sound source localization with deep learning methods",
            "venue": "arXiv preprint arXiv:2109.03465, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Cheng",
                "C. Wu",
                "Y. Zhang",
                "H. Wu",
                "M. Li",
                "C. Maple"
            ],
            "title": "A survey of localization in wireless sensor network",
            "venue": "International Journal of Distributed Sensor Networks, vol. 8, no. 12, p. 962523, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M.A. Jatoi",
                "N. Kamel",
                "A.S. Malik",
                "I. Faye",
                "T. Begum"
            ],
            "title": "A survey of methods used for source localization using eeg signals",
            "venue": "Biomedical Signal Processing and Control, vol. 11, pp. 42\u201352, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "H.C. So"
            ],
            "title": "Source localization: Algorithms and analysis",
            "venue": "Handbook of Position Location: Theory, Practice, and Advances, pp. 25\u201366, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "A. Beck",
                "P. Stoica",
                "J. Li"
            ],
            "title": "Exact and approximate solutions of source localization problems",
            "venue": "IEEE Transactions on signal processing, vol. 56, no. 5, pp. 1770\u20131778, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "C. Meesookho",
                "U. Mitra",
                "S. Narayanan"
            ],
            "title": "On energy-based acoustic source localization for sensor networks",
            "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 1, pp. 365\u2013377, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J. Chen",
                "U. Mitra"
            ],
            "title": "Unimodality-constrained matrix factorization for non-parametric source localization",
            "venue": "IEEE Transactions on Signal Processing, vol. 67, no. 9, pp. 2371\u20132386, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. Bhanu"
            ],
            "title": "Automatic target recognition: State of the art survey",
            "venue": "IEEE transactions on aerospace and electronic systems, no. 4, pp. 364\u2013379, 1986.",
            "year": 1986
        },
        {
            "authors": [
                "S. Sun",
                "A.P. Petropulu",
                "W.U. Bajwa"
            ],
            "title": "Target estimation in colocated mimo radar via matrix completion",
            "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 4144\u20134148.",
            "year": 2013
        },
        {
            "authors": [
                "J. Haupt",
                "R.M. Castro",
                "R. Nowak"
            ],
            "title": "Distilled sensing: Adaptive sampling for sparse detection and estimation",
            "venue": "IEEE Transactions on Information Theory, vol. 57, no. 9, pp. 6222\u20136235, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "S. Choudhary",
                "U. Mitra"
            ],
            "title": "Analysis of target detection via matrix completion",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 3771\u20133775.",
            "year": 2015
        },
        {
            "authors": [
                "S. Mak",
                "Y. Xie"
            ],
            "title": "Active matrix completion with uncertainty quantification",
            "venue": "arXiv preprint arXiv:1706.08037, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. Yang",
                "J. Fang",
                "H. Duan",
                "H. Li",
                "B. Zeng"
            ],
            "title": "Fast low-rank bayesian matrix completion with hierarchical gaussian prior models",
            "venue": "IEEE Transactions on Signal Processing, vol. 66, no. 11, pp. 2804\u2013 2817, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P. Alquier"
            ],
            "title": "A bayesian approach for noisy matrix completion: Optimal rate under general sampling distribution",
            "venue": "Electronic Journal of Statistics, vol. 9, no. 1, pp. 823\u2013841, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Bhargava",
                "R. Ganti",
                "R. Nowak"
            ],
            "title": "Active positive semidefinite matrix completion: Algorithms, theory and applications",
            "venue": "Artificial Intelligence and Statistics. PMLR, 2017, pp. 1349\u20131357.",
            "year": 2017
        },
        {
            "authors": [
                "X. Zhao",
                "W. Zhang",
                "J. Wang"
            ],
            "title": "Interactive collaborative filtering",
            "venue": "Proceedings of the 22nd ACM international conference on Information and Knowledge Management, 2013, pp. 1411\u20131420. 9",
            "year": 2013
        },
        {
            "authors": [
                "J. Kawale",
                "H.H. Bui",
                "B. Kveton",
                "L. Tran-Thanh",
                "S. Chawla"
            ],
            "title": "Efficient thompson sampling for online matrix-factorization recommendation",
            "venue": "Advances in neural information processing systems, 2015, pp. 1297\u20131305.",
            "year": 2015
        },
        {
            "authors": [
                "B. Kveton",
                "C. Szepesv\u00e1ri",
                "A. Rao",
                "Z. Wen",
                "Y. Abbasi-Yadkori",
                "S. Muthukrishnan"
            ],
            "title": "Stochastic low-rank bandits",
            "venue": "arXiv preprint arXiv:1712.04644, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Mokhasunavisu",
                "U. Mitra"
            ],
            "title": "Non-parametric active target localization: Exploiting unimodality and separability",
            "venue": "2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2017, pp. 346\u2013353.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Chen",
                "J. Fan",
                "C. Ma",
                "Y. Yan"
            ],
            "title": "Inference and uncertainty quantification for noisy matrix completion",
            "venue": "Proceedings of the National Academy of Sciences, vol. 116, no. 46, pp. 22 931\u201322937, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J.A. Tropp"
            ],
            "title": "User-friendly tail bounds for sums of random matrices",
            "venue": "Foundations of computational mathematics, vol. 12, no. 4, pp. 389\u2013434, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "J.-Y. Audibert",
                "R. Munos",
                "C. Szepesv\u00e1ri"
            ],
            "title": "Exploration\u2013exploitation tradeoff using variance estimates in multi-armed bandits",
            "venue": "Theoretical Computer Science, vol. 410, no. 19, pp. 1876\u20131902, 2009.",
            "year": 1876
        },
        {
            "authors": [
                "P.-\u00c5. Wedin"
            ],
            "title": "Perturbation bounds in connection with singular value decomposition",
            "venue": "BIT Numerical Mathematics, vol. 12, no. 1, pp. 99\u2013111, 1972. 10",
            "year": 1972
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 5.\n02 37\n6v 1\n[ cs\n.I T\n] 5\nM ay\n2 02"
        },
        {
            "heading": "1 Introduction",
            "text": "Herein, we treat the problem of maximizing an unimodal function that is imperfectly sampled. An important instantiation of this problem is peak detection or peak finding which is prevalent in many signal processing, computer vision and machine learning applications. In particular, we are motivated by the problem of source localization in the absence of a parametric model for the source signal. This problem is relevant to acoustics, [1, 2], wireless networks [3], and medical imaging [4] etc.\nClassical source detection and localization approaches require environmental models and/or energy models [5\u20137]; in many applications (e.g. underwater and gas-source localization) such models are challenging to come by. Herein, we leverage prior work [8] which specifically exploited the unimodality of localization signals enabling a non-parametric approach. While the challenge of model knowledge is obviated in [8], randomized sampling still requires a measurable number of samples. Herein, we focus on active sampling with the goal of strongly reducing the sample complexity. Active single source target localization has been well-studied [9\u201312]. These works, however, impose restrictive signal assumptions: separability of energy fields, full energy measurements and so on.\nIn this work, we propose Uncertainty-Based Active Peak Detection (UBAD) to solve a much more general form of the problem to achieve active source localization. We adopt the overall algorithmic framework of [8] based on unimodal matrix completion. We enhance [8] by exploiting methods from active matrix completion and multi-armed bandits.\nA Bayesian formulation for matrix completion [13] derives maximum-entropy based sampling schemes given the posterior distributions. Error bounds for uniform random sampling under different priors are computed in [14,15]. In contrast, a non-Bayesian formulation is undertaken in [16] using Nystro\u0308m sampling for positive semi-definite matrices. However, none of these works are readily applicable to the current problem setting since we consider a non-parametric, and extremely generic problem formulation. Low-rank matrix completion in a bandits setting [17, 18] also assumes that rewards are sampled from a prior distribution as in the Bayesian case. A seemingly promising work for our framework is that of [19] where an elimination method is employed to find the maximum element of a low rank matrix; unfortunately, it is assumed that the singular vectors are a convex combination of some underlying latent factors. We observe that [20] shows that although greedy search space reduction techniques are highly efficient for peak detection in the absence of noise, such methods fail in the presence of noise. Thus, we will leverage the idea of optimism in the face of uncertainty from multi-armed bandits [21] for our active sampling scheme.\nThe contributions of this work are:\n1. We propose UBAD, a novel uncertainty-based algorithm to solve the problem of non-parametric, active peak detection.\n2. We show that as long as the energy function is non-increasing, our proposed method has favourable error bounds that hold under high generality.\n3. We validate our theoretical claims through extensive numerical results and demonstrate that the proposed method outperforms existing (non-adaptive, adaptive but greedy) source localization algorithms"
        },
        {
            "heading": "1.1 Notation",
            "text": "We use the shorthand notation, [k] := {1, 2, \u00b7 \u00b7 \u00b7 , k}. We use lower (x) and upper (M) case boldface letters to denote vectors and matrices respectively. We index the i-th entry of a vector as xi and i, j-th entry of a matrix as Mi,j respectively. Given a matrix, M \u2208 Rm\u00d7n, and a set S \u2286 [m] \u00d7 [n], MS \u2208 Rm\u00d7n sets the entries outside S to zero. For vectors (matrices), we use \u2016 \u00b7 \u2016 to denote the 2- (induced 2-) norm unless specified otherwise. Throughout the paper we use c, C to denote (possibly different) constants in each use."
        },
        {
            "heading": "2 Problem Setting, Algorithm, and Main Result",
            "text": "We next define the problem setting, describe the algorithm, and provide our main theoretical results."
        },
        {
            "heading": "2.1 Problem Setting",
            "text": "In this work, we consider the setting of a single source located at s\u2217 \u2208 R2 (unknown) inside a target area, A with area with width L. We discretize the L\u00d7 L target area1 A = [\u2212L/2, L/2]\u00d7 [\u2212L/2, L/2] into n\u00d7 n equally spaced grid points. Thus, the center of the (i, j)-grid square, is given by xi,j := ( L(2i\u22121) 2n , L(2j\u22121) 2n )\u22ba . We assume that the true energy measurement at a point xi,j \u2208 A is given by some unknown, non-negative and monotonically non-increasing function, h : R2 \u00d7 R2 \u2192 R. We also assume that there can be additional measurement noise. Thus, the energy measurement at the center of the (i, j)-th grid cell is given as\nYi,j = h(xi,j , s \u2217) + zi,j , (1)\nwhere, s\u2217 := xi\u2217,j\u2217 is the true source/peak location, the function h(\u00b7, \u00b7) is the energy field and zi,j i.i.d\u223c N (0, \u03c32n) models measurement noise. In matrix form, the signal model can be expressed as\nY := H(s\u2217) +Z. (2)\nWe notice that owing to the uniform discretization of the target area and the monotonic assumption on the energy field, h(\u00b7, \u00b7), the matrix H(s\u2217) is unimodal. We use the following definition of matrix unimodality in this work.\nDefinition 2.1 (Unimodality). A matrix M is said to be unimodal with the mode at (i\u2217, j\u2217) if M1,j \u2264 M2,j \u00b7 \u00b7 \u00b7 \u2264Mi\u2217,j \u2265Mi\u2217+1,j \u2265 \u00b7 \u00b7 \u00b7 \u2265Mn,j for all j \u2208 [n] and Mi,1 \u2264Mi,2 \u00b7 \u00b7 \u00b7 \u2264Mi,j\u2217 \u2265Mi,j\u2217+1 \u00b7 \u00b7 \u00b7 \u2265Mi,n for all i \u2208 [n].\nThe goal of this work is to efficiently estimate s\u2217 by actively querying noisy energy measurements, Yi,j informed by prior measurements.\n1We assume a centered and symmetric area for the sake of notational simplicity. All our results apply to general spaces as well."
        },
        {
            "heading": "2.2 UBAD Algorithm",
            "text": "We propose an uncertainty-based active sampling approach, Uncertainty-Based Active Peak Detection (UBAD). For the initialization/exploration phase, we use the method of Latin Squares [22]. For the peak detection, we adapt the method of [8] that transforms the peak detection problem into a unimodality constrained matrix completion operation. We first explain the two sampling schemes followed by the peak detection step in detail.\nInitial Sampling: We draw the initial set of samples, \u2126init \u2282 [n]\u00d7 [n] with |\u2126init| = n through2 Latin Squares. Thus, by construction, we sample the energy measurement in each row and column exactly once. We show that one is equally likely to sample each row/column. Intuitively, (in the single peak setting) this method gives us sufficiently good information about the location of the peak. We quantify this notion in Lemma 2.3 provided in Sec. 2.3.\nActive Sampling: For the active (sequential) sampling stage, we devise a uncertainty-based sampling scheme. Specifically, under standard MC assumptions (incoherence of the matrix, randomly selected samples, and a large enough number of observations) [23, Theorem 2] derives distributional guarantees on the output of any MC algorithm. More precisely, assume that Y SVD = U\u03a3yV\n\u22ba be a rank-r matrix. Then, as long as we observe O(nr5 polylog(n)) measurements, with probability at least 1 \u2212 n\u22123, the output of a MC algorithm satisfies\nY\u0302i,j \u223c N (Yi,j , C \u221a r/n(\u2016U (i)\u201622 + \u2016V (j)\u201622),\nwhere \u2016U (i)\u2016 denotes the i-th row of U . There are two key challenges to be addressed here: (a) we consider the setting of low-samples and so, we do not observe a sufficient number of samples; (b) we do not have access to the true incoherence values (that determine the variance of the estimates). The first point requires a more thorough investigation into derivation of uncertainty bounds which we will consider as part of future work whereas the second challenge is addressed using the estimate of the incoherence instead of the actual incoherence.\nPeak Detection: The remaining part of the algorithm is similar to that of the passive source localization problem studied in [8]. More specifically, we complete the energy matrix followed by estimating the location of the peak as the largest entry of the completed matrix. The complete pseudocode is provided in Algorithm 1."
        },
        {
            "heading": "2.3 Main Result and Proof Sketch",
            "text": "Before showing the main result, we provide a few preliminaries. Let the rank-1 SVD of the energy matrix be given by Y := \u03c32yuv\n\u22ba with \u2016u\u20162 = \u2016v\u20162 = 1. Define the maximum value of Y , b := maxi,j Yi,j . We define the row (column) differences, as \u2206uk|l := Yi\u2217,l \u2212 Yk,l and \u2206vl|k := Yk,j\u2217 \u2212 Yk,l.\nTheorem 2.2 (Sequential Sampling). Assume that the measurements satisfy (1). Then, with probability at least 0.9, the source localization error for the sequential sampling satisfies\nE\n[\n1\nm\nm \u2211\nt=1\n\u2016s\u0302\u03c4 \u2212 s\u2217\u201622\n]\n\u2264 C n \u2211\nk,l=1\n\u03b3uk,l (\u2206uk|l) 2 \u00b7 \u03b3vk,l (\u2206vl|k) 2 \u00b7 \u2016ck,l \u2212 c\u2217\u20162 log2 m m\nwhere, \u03b3uk,l := uk + 2b\u2206 u k|l and \u03b3 v k,l := vl + 2b\u2206 v l|k; ck,l := (k, l) \u22ba and c\u2217 := (i\u2217, j\u2217)\u22ba.\nProof Sketch: There are three main proof components: initialization, demonstration of maximum entropy sampling, and bounding the error of the sequential sampling stage. First consider the initialization step:\n2For the sake of simplicity, in this paper we consider only square matrices after discretization. Rectangular matrices can be handled by a simply stacking multiple smaller Latin Squares along the appropriate dimension.\nAlgorithm 1 Uncertainty based Active Peak Detection (UBAD) Require: Y \u2208 Rn\u00d7n (energy matrix), m (# sequential samples) 1: Init: \u2126\u2190Latin-Squares(n) 2: [u\u03030, \u03c3\u03032, y\u03030]\u2190 SVD1(Y\u2126) 3: u\u03020 \u2190 \u03c3\u0303u\u03030, v\u03020 \u2190 \u03c3\u0303v\u03030 4: for t \u2208 [m] do 5: Y\u0302 t\u22121 \u2190 u\u0302t\u22121(v\u0302t\u22121)\u22ba 6: s\u0302t = (x\u0302\nt\u22121, y\u0302t\u22121)\u2190 argmax i,j |Y\u0302 t\u22121i,j |\n7: (it, jt)\u2190 arg max (i,j)\u2208\u2126c |Y\u0302 t\u22121i,j |+ (u\u0302t\u22121i )2 + (v\u0302t\u22121j )2 8: \u2126\u2190 \u2126 \u222a (it, jt) \u22b2 Update index set 9: Query next sample, Yit,jt , update Y\u2126\n10: Solve Y\u0302 t \u2190 arg min M\u2208Rn\u00d7n \u2016M \u2212 Y\u2126\u2016\u2217 11: Update u\u0302t(v\u0302t)\u22ba SV D\u2190 Y\u0302 t 12: end for Ensure: s\u0302m\nLemma 2.3 (Latin Squares Initialization). Consider the n\u00d7 n rank 1 matrix Y . Let \u2126init be the output of Latin-Squares. Then, with probability at least 1\u2212 n\u221210, the error bound for the matrix is\n\u2016Y \u2212 Y\u2126init\u20162 \u2264 1.01(1\u2212 1/n)\u03c32y\u2016u\u20161\u2016v\u20162 (3)\nThe proof relies on carefully decomposing the residual matrix, Y \u2212 Y\u2126init into a sum of independent sub-Gaussian random matrices, followed by the application of the Matrix Bernstein inequality [24]. To the best of our knowledge, there are no existing results that consider just n observations that are drawn through Latin-Squares based sampling. The Latin-squares sampling, in particular, is challenging to deal with since the entries of \u2126init are not independent.\nEssentially, Lemma 2.3 serves as a proxy for a matrix completion step after the initial sampling stage and allows us to apply the distributional guarantees from [23]. Specifically, [23, Theorem 2] tells us that as long we sample O(n polylog n) entries of the rank-1 matrix Y , uniformly at random, the output of any matrix completion algorithm satisfies\nY\u0302i,j \u223c N (Yi,j , 1\u221a n (|ui|2 + |vj |22)), (4)\nwith probability at least 1\u2212 n\u22123. In this paper, we continue with the following assumption instead 3\nAssumption 2.4 (Uncertainty Based Sampling). Under the conditions of Theorem 2.2, choosing the next sample, (it, jt) at the t-th iteration chooses the location with maximum entropy. Furthermore, (4) holds with high probability even after replacing u (and v) with u\u0302t (and v\u0302t).\nWe now consider the correctness of the sequential sampling step. The proof relies on a careful application of the regret bounds for the multi-armed Bandit problem wherein each arm has an unknown (and possibly distinct) variance [25]. Observe that the matrix completion step (Line 9 of UBAD) induces mutual independence in the estimates, Y\u0302 t across time. This directly follows from the fact that matrix completion algorithms typically begin with a random Gaussian initialization, followed by a refinement procedure. Additionally, we observe from (4) that for any given t, each row (and column) is nearly statistically independent4 of all others. The last key insight is that selecting the t-th sequential sample, (it, jt) is equivalent to selecting\n3Although [23] assumes an \u01eb-accurate recovery, following the matrix completion step, we conjecture that the upper bound of Lemma 2.3 suffices to invoke [23], albeit with larger residual terms and lower probability of success.\n4This stems from the variance expression plus the fact that we do not impose any stochastic assumption on Y .\nan optimal row, it and a column jt, which is further equivalent to playing two instances of the multi-armed Bandit problem \u2013 one to select a row and the other to select a column. To see that at any given t, the two Bandit problems are independent, notice that The above reason justifies the application of the regret bound, [25, Theorem 1]. The remainder of the proof follows from careful algebraic manipulation. We provide the proof in the Appendix.\nThis general result can be simplified for the special cases of Gaussian and Laplacian fields.\nCorollary 2.5 (Special Cases). 1. If the energy field is considered to be a Gaussian function with variance parameter \u03c3, the error, E [ \u2211m\n\u03c4=1 \u2016s\u0302\u03c4 \u2212 s\u2217\u201622 ] is bounded as\n\u2211\nk,l\nC\u03c32 log2 m \u2016ck,l \u2212 c\u2217\u20162\nexp(\u2212 \u2016ck,l\u2212c\u2217\u201622n\u03c32 ) (5)\n2. If the energy field is considered to be a Laplacian function with scale parameter \u03b3, the average error, E [ \u2211m\n\u03c4=1 \u2016s\u0302\u03c4 \u2212 s\u2217\u201622 ] is bounded by\n\u2211\nk,l\nC\u03b3 log2 m \u2016ck,l \u2212 c\u2217\u20162\nexp(\u2212 \u2016ck,l\u2212c\u2217\u20162n\u03b3 ) (6)\nDiscussion: Notice from Corollary 2.5 that for both Gaussian and Laplacian fields, the upper bound is positively correlated with the signal spread (\u03c32 for Gaussians and \u03bb for Laplacians). The reason is that the maximum-entropy-based sampling ensures that the peak is sampled with high probability for the low spread scenario; this trend is validated in our numerical experiments (see Fig. 2). Furthermore, our error upper bounds are higher for the Gaussian energy fields (it is not easy to exactly compare the two settings)5. This has been observed in previous work [8, 12] as well. We corroborate this trend through experiments in (see Fig 1)."
        },
        {
            "heading": "3 Numerical Results",
            "text": "All experiments are averaged over 100 independent trials. The codes are available at https://github.com/praneethmurthy/active-localization.\nEffectiveness of Active Sampling Strategy. We first illustrate that the proposed method indeed serves as an effective sampling strategy for the source localization problem. We generate the data as follows: we set the target area, A := [\u2212L/2, L/2]\u00d7 [\u2212L/2, L/2] with L = 5. We discretize A into a n \u00d7 n equally spaced grid with n = 100. We consider a single source placed at s\u2217 := (2, 3)\u22ba. For the energy emitted by the\n5this follows since the summands scale as x2/ exp(\u2212x2) vs x2/ exp(\u2212x)\nsource, in this paper, we consider two cases: (a) Gaussian energy field, i.e., h(x, s\u2217) = C exp(\u2212\u2016s\u2217\u2212x\u201622/2\u03c32) with variance \u03c32 and (b) Laplacian energy field, i.e., h(x, s\u2217) = C exp(\u2212\u2016s\u2217 \u2212 x\u20161/\u03bb) with scale parameter \u03bb. For the first experiment, we consider the Gaussian setting with \u03c32 = 1, and do not consider additional measurement noise.\nWe compare the proposed method with the state-of-the-art passive source localization approach [8] and a greedy baseline that directly exploits the unimodality property of the energy matrix. Concretely, the greedy baseline is essentially Algorithm 1 without considering the last two terms in Line 7. We implement [8] and in each sequential sampling stage, we draw a sample uniformly at random\nWe implement UBAD as follows. We start with a Latin Squares initialization that queries the energy measurement at n = 100 locations. We employ the same method for the greedy baseline as well as the proposed approach. We set the total number of sequential samples, m = 50 and plot the localization error of all methods with respect to the number of sequential samples observed. The results are shown in Fig. 1. Notice that both the Greedy, and the proposed methods perform significantly better than [8]. Furthermore, the proposed approach performs better than the Greedy approach.\nEvaluating Model Parameters. Next, we illustrate the effect of varying the parameter of the energy field. In particular, we consider the Laplacian setting, and vary the scale parameter. We generate the data exactly as done in the previous experiment. We implement the baseline, [8], and UBAD with m = 100, and illustrate the results in Fig. 2. We observe that (i) even in the Laplacian setting UBAD outperforms the baseline; (ii) as the scale parameter increases, i.e., the steepness of the field reduces, the localization increases. This is in accordance with Corollary 2.5. We observed similar trends for the Gaussian field as well.\nComparison with respect to Additional noise. Finally, we investigate the robustness of UBAD to additional noise. We generate the data as done in the first experiment, but add independent Gaussian noise with zero-mean and variance \u03c32n. We implement the passive baseline [8], the greedy method, and UBAD. We notice that when noise is very high, the passive method is better, but in low-noise regimes, UBAD is better. Furthermore, UBAD is more robust to noise than the greedy approach as expected."
        },
        {
            "heading": "4 Conclusions and Future Work",
            "text": "In this work, we studied the problem of non-parametric active single source localization. We proposed UBAD, an active sampling algorithm that enjoys a error bound that scales as O(log2 m/m) for m sequential samples. Experimentally, we showed that the proposed method performs well across several different types of data and is robust to noise. As part of future work, we will consider (i) dealing with multiple sources; (ii) development of complete convergence guarantees for the proposed method; and (iii) development of a more efficient algorithm that does not involve explicit matrix completion."
        },
        {
            "heading": "A Proof Sketch: Details",
            "text": "In this section, we provide the proof of the main result, Theorem 2.2. To this end, we first prove a generalized version of the initialization result, Lemma 2.3, followed by the sequential sampling result.\nLemma A.1 (Latin Squares Initialization). Consider the true energy matrix, Y . With probability at least 1\u2212 n\u221210\n1. the error bound for the matrix is\n\u2016Y \u2212 Y\u2126init\u20162 \u2264 1.01(1\u2212 1/n)\u2016u\u20161\u2016v\u20162 (7)\n2. the error bound for the subspaces is\n(u\u22bau\u03030) 2 + (v\u22bav\u03030) 2 \u2264 2\u2016Y \u2212 Y\u2126\u2016 2 2\n(\u2016Y \u2016 \u2212 \u2016Y \u2212 Y\u2126\u2016)2 (8)\nwhere u\u03030 and v\u03030 are the top-1 left, and right singular vectors of Y\u2126 repsectively.\nProof of Lemma A.1. Proof of Item 1: Define u\u0304 := \u03c3yu and v\u0304 := \u03c3yv. For simplicity, we drop the subscript init in this proof. Notice that\n\u2016Y \u2212 Y\u2126\u20162 = \u2016u\u0304v\u22ba \u2212 (u\u0304v\u22ba)\u2126\u20162 =\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\ni,j\n\u03c6i,jeiu\u0304iv\u0304je \u22ba\nj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nwhere, ei is the i-th canonical basis vector in R n and we use \u03c6i,j1 if (i, j) \u2208 \u2126c and 0 otherwise. Although these matrices are not independent, notice that for a fixed i, the \u03c6i,j are independent Bernoulli r.v.\u2019s. For a fixed i, we can apply Matrix Bernstein [24]. Let Xi,j := \u03c6i,jeiu\u0304iv\u0304je \u22ba j . We have\n\u2016E[Xi,j ]\u2016 = E[\u03c6i,j ]\u2016eiu\u0304iv\u0304jej\u2016 = (1\u2212 1/n) \u00b7 u\u0304iv\u0304j =\u21d2 max\nj \u2016E[Xi,j ]\u2016 \u2264 (1\u2212 1/n)u\u0304iv\u0304j\u2217 := Ri\nWe can also bound the \u201cvariance parameter\u201d \u2225\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\nE[Xi,jX \u22ba i,j ]\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 = ( 1\u2212 1 n ) u\u03042i \u2016v\u0304\u20162 (9)\nwhere the last line follows from the observation that the matrix is a matrix with just one non-zero element at the (i, i)-location. Furthermore, we have\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\nE[X\u22bai,jXi,j]\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 = ( 1\u2212 1 n ) u\u03042i v\u0304 2 j\u2217 (10)\nwhere the last line follows from the observation that the spectral norm of a diagonal matrix is the value of the largest element, and the unimodality property of v\u0304. From these, we get,\nmax\n\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\nE[Xi,jX \u22ba i,j ]\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\nE[X\u22bai,jXi,j ]\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225    = ( 1\u2212 1 n ) u\u03042i \u2016v\u0304\u20162 := \u03c32i\nApplying the matrix Bernstein for a fixed i, we have\nPr\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\nXi,j\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2265 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\nE[Xi,j ]\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 + s   \u2264 2n exp ( \u2212s2/2 \u03c32i +Ris/3 )\nnow, picking s := \u221a\n0.01\u00b712 logn 2(1\u22121/n)u\u0304i (1\u2212 1/n)u\u0304i\u2016v\u0304\u2016 gives us that\nPr\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\n\u03c6i,jeiu\u0304iv\u0304je \u22ba\nj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2265 1.01(1\u2212 1/n)u\u0304i\u2016v\u0304\u2016   \u2264 n\u221211\nA union bound over all i concludes the proof. Proof of Item 2: The proof follows form the result of Item 1 followed by an application of Wedin\u2019s Theorem [26]. Define\nY\u2126 SVD =\nn \u2211\ni=1\n\u02dc\u03c30,iu\u03030,iu\u0303 \u22ba 0,i = \u03c30,1u\u03030,1v\u0303 \u22ba 0,1 + n \u2211\ni=2\n\u02dc\u03c30,iu\u03030,iu\u0303 \u22ba 0,i := u\u03030\u03c3\u03030v\u0303 \u22ba 0 + U\u03030,\u22a5\u03a3\u03030,\u22a5V\u0303 \u22ba 0,\u22a5\nobserve that the deviation parameter, \u03b4 := min{min1\u2264i\u2264r,r\u2264j\u2264n2 |\u03c3i \u2212 \u03c3\u0303j |,min1\u2264i\u2264r \u03c3i} = min{\u03c32y \u2212 \u2016\u03a30,\u22a5\u2016, \u03c32y} = \u03c32y \u2212 \u2016\u03a30,\u22a5\u20162 > 0. Thus,\n(u\u22bau\u03030) 2 + (v\u22bav\u03030) 2 \u2264 \u2016u \u22ba(Y \u2212 Y\u2126)\u201622 + \u2016v\u22ba(Y \u2212 Y\u2126)\u201622\n\u03b42 (11)\nusing triangle inequality on the numerator terms and, Weyl\u2019s inequality completes the proof.\nProof of Theorem 2.2. First, for the sake of simplicity, assume that the left singular value estimates have the same distribution at all time t, i.e, u\u0302t i.i.d\u223c N (\u00b5u,\u03a3u) with \u03a3u diagonal.\nDefine T uk (m) denote the number of times the k-th row is selected afterm sequential samples. [25, Theorem 2] tells us that the (expected) number of times the \u201cwrong row\u201d is selected can be bounded as\nE[T\u0302 uk (m)] \u2264 C ( (\u03a3u)k,k + 2b(\u2206 u k)\n(\u2206uk) 2\n)\nlogm (12)\nwhere b is the bound on the energy field and \u2206uk := \u00b5 \u2217 u \u2212 (\u00b5u)k is the sub-optimality gap of the k-th row. Similarly, assuming that the right singular value estimates have the same distribution at all time t, i.e, v\u0302t i.i.d\u223c N (\u00b5v,\u03a3v) with \u03a3v diagonal, the number of the l-th colum is selected after m sequential samples can be bounded as\nE[T\u0302 vl (m)] \u2264 C ( (\u03a3v)l,l + 2b\u2206 v l\n(\u2206vl ) 2\n)\nlogm (13)\nwhere \u2206vl := \u00b5 \u2217 v \u2212 (\u00b5v)l is the sub-optimality gap of the l-th column. Now consider the expected error function,\nE\n[\nm \u2211\n\u03c4=1\n\u2016s\u0302\u03c4 \u2212 s\u2217\u2016 ] = E\n\n\nm \u2211\n\u03c4=1\n\u2211\nk,l\n1{i\u03c4=k,j\u03c4=l}(|k \u2212 i\u2217|2 + |l\u2212 j\u2217|2)\n\n\n(a) = \u2211\nk,l\nE[T uk (m)] E[T v l (m)](|k \u2212 i\u2217|2 + |l\u2212 j\u2217|2)\nwhere (a) follows from the definition of T uk (m), T v l (m), re-ordering the summation, and using the nearindependence result [23] to decompose the expected value completes the proof."
        }
    ],
    "title": "Uncertainty-Based Non-Parametric Active Peak Detection",
    "year": 2022
}