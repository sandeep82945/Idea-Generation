{
    "abstractText": "We introduce a method to identify speakers by computing with high-dimensional random vectors. Its strengths are simplicity and speed. With only 1.02k active parameters and a 128-minute pass through the training data we achieve Top-1 and Top-5 scores of 31% and 52% on the VoxCeleb1 dataset of 1,251 speakers. This is in contrast to CNN models requiring several million parameters and orders of magnitude higher computational complexity for only a 2\u00d7 gain in discriminative power as measured in mutual information. An additional 92 seconds of training with Generalized Learning Vector Quantization (GLVQ) raises the scores to 48% and 67%. A trained classifier classifies 1 second of speech in 5.7 ms. All processing was done on standard CPU-based machines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ping-Chen Huang"
        },
        {
            "affiliations": [],
            "name": "Denis Kleyko"
        },
        {
            "affiliations": [],
            "name": "Jan M. Rabaey"
        },
        {
            "affiliations": [],
            "name": "Bruno A. Olshausen"
        },
        {
            "affiliations": [],
            "name": "Pentti Kanerva"
        }
    ],
    "id": "SP:31c377db5a9ebc48fc232a90555c7a4f7b12a086",
    "references": [
        {
            "authors": [
                "A. Nagrani"
            ],
            "title": "VoxCeleb: A large-scale speaker identification dataset",
            "venue": "arXiv:1706.08612, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.S. Chung"
            ],
            "title": "VoxCeleb2: Deep speaker recognition",
            "venue": "arXiv:1806.05622, 2018.",
            "year": 1806
        },
        {
            "authors": [
                "S.-H. Kim",
                "Y.-H. Park"
            ],
            "title": "Adaptive convolutional neural network for text-independent speaker recognition",
            "venue": "Conference of the International Speech Communication Association (INTER- SPEECH), 2021, pp. 641\u2013645.",
            "year": 2021
        },
        {
            "authors": [
                "W.M. Campbell"
            ],
            "title": "Support vector machines using gmm supervectors for speaker verification",
            "venue": "IEEE signal processing letters, vol. 13, no. 5, pp. 308\u2013311, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "P. Kenny"
            ],
            "title": "Joint factor analysis of speaker and session variability: Theory and algorithms",
            "venue": "CRIM, Montreal,(Report) CRIM-06/08- 13, vol. 14, no. 28-29, p. 2, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "N. Dehak"
            ],
            "title": "Support Vector Machines versus fast scoring in the low-dimensional total variability space for speaker verification",
            "venue": "Annual Conference of the International Speech Communication Association, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "P. Kanerva"
            ],
            "title": "Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors",
            "venue": "Cognitive computation, vol. 1, no. 2, pp. 139\u2013 159, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "J. Wong"
            ],
            "title": "Negative capacitance and hyperdimensional computing for unconventional low-power computing",
            "venue": "Ph.D. dissertation, UC Berkeley, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "E. Osipov"
            ],
            "title": "HyperSeed: Unsupervised Learning with Vector Symbolic Architectures",
            "venue": "arXiv:2110.08343, pp. 1\u201312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Kleyko"
            ],
            "title": "A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations",
            "venue": "arXiv:2111.06077, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Joshi"
            ],
            "title": "Language geometry using random indexing",
            "venue": "QI Symposium, 2016, pp. 265\u2013274.",
            "year": 2016
        },
        {
            "authors": [
                "P. Alonso"
            ],
            "title": "HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing enabled Embedding of n-gram Statistics",
            "venue": "IJCNN, 2021, pp. 1\u20139.",
            "year": 2021
        },
        {
            "authors": [
                "A. Rahimi"
            ],
            "title": "Efficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of exg signals",
            "venue": "Proceedings of the IEEE, vol. 107, no. 1, pp. 123\u2013143, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Moin"
            ],
            "title": "A wearable biosensing system with in-sensor adaptive machine learning for hand gesture recognition",
            "venue": "Nature Electronics, vol. 4, no. 1, pp. 54\u201363, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Zhou"
            ],
            "title": "Memory-efficient, limb position-aware hand gesture recognition using hyperdimensional computing",
            "venue": "arXiv:2103.05267, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Ge",
                "K.K. Parhi"
            ],
            "title": "Seizure detection using power spectral density via hyperdimensional computing",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 7858\u20137862.",
            "year": 2021
        },
        {
            "authors": [
                "M. Imani"
            ],
            "title": "VoiceHD: Hyperdimensional Computing for Efficient Speech Recognition",
            "venue": "IEEE ICRC, 2017, pp. 1\u20138.",
            "year": 2017
        },
        {
            "authors": [
                "D. Kleyko"
            ],
            "title": "A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges",
            "venue": "arXiv:2112.15424, pp. 1\u201336, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T.A. Plate"
            ],
            "title": "Distributed representations and nested compositional structure",
            "venue": "Ph.D. dissertation, University of Toronto, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "M.A. Kohler"
            ],
            "title": "Phonetic speaker recognition",
            "venue": "Asilomar Conference on Signals, Systems & Computers, vol. 2, 2001, pp. 1557\u20131561.",
            "year": 2001
        },
        {
            "authors": [
                "A. Burrello"
            ],
            "title": "Hyperdimensional computing with local binary patterns: One-shot learning of seizure onset and identification of ictogenic brain regions using short-time ieeg recordings",
            "venue": "IEEE Transactions on Biomedical Engineering, vol. 67, no. 2, pp. 601\u2013613, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "O. R\u00e4s\u00e4nen"
            ],
            "title": "Generating Hyperdimensional Distributed Representations from Continuous Valued Multivariate Sensory Input",
            "venue": "Annual Meeting of the Cognitive Science Society (CogSci), 2015, pp. 1943\u20131948.",
            "year": 2015
        },
        {
            "authors": [
                "D. Kleyko"
            ],
            "title": "Classification and Recall with Binary Hyperdimensional Computing: Tradeoffs in Choice of Density and Mapping Characteristic",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 12, pp. 5880\u20135898, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "L. Ge",
                "K.K. Parhi"
            ],
            "title": "Classification using Hyperdimensional Computing: A Review",
            "venue": "IEEE Circuits and Systems Magazine, vol. 20, no. 2, pp. 30\u201347, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Rosato"
            ],
            "title": "Hyperdimensional Computing for Efficient Distributed Classification with Randomized Neural Networks",
            "venue": "IJCNN, 2021, pp. 1\u201310.",
            "year": 2021
        },
        {
            "authors": [
                "J. Karlgren",
                "P. Kanerva"
            ],
            "title": "Semantics in High-Dimensional Space",
            "venue": "Frontiers in Artificial Intelligence, vol. 4, pp. 1\u20136, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D.A. Rachkovskij"
            ],
            "title": "Linear Classifiers based on Binary Distributed Representations",
            "venue": "Information Theories and Applications, vol. 14, no. 3, pp. 270\u2013274, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "A. Rahimi"
            ],
            "title": "Hyperdimensional Biosignal Processing: A Case Study for EMG-based Hand Gesture Recognition",
            "venue": "IEEE ICRC, 2016, pp. 1\u20138.",
            "year": 2016
        },
        {
            "authors": [
                "D. Kleyko"
            ],
            "title": "Density encoding enables resource-efficient randomly connected neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 8, pp. 3777\u2013 3783, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Diao"
            ],
            "title": "Generalized Learning Vector Quantization for Classification in Randomized Neural Networks and Hyperdimensional Computing",
            "venue": "IJCNN, 2021, pp. 1\u20139.",
            "year": 2021
        },
        {
            "authors": [
                "A. Sato",
                "K. Yamada"
            ],
            "title": "Generalized Learning Vector Quantization",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), 1996, pp. 423\u2013429.",
            "year": 1996
        }
    ],
    "sections": [
        {
            "text": "with high-dimensional random vectors. Its strengths are simplicity and speed. With only 1.02k active parameters and a 128-minute pass through the training data we achieve Top-1 and Top-5 scores of 31% and 52% on the VoxCeleb1 dataset of 1,251 speakers. This is in contrast to CNN models requiring several million parameters and orders of magnitude higher computational complexity for only a 2\u00d7 gain in discriminative power as measured in mutual information. An additional 92 seconds of training with Generalized Learning Vector Quantization (GLVQ) raises the scores to 48% and 67%. A trained classifier classifies 1 second of speech in 5.7 ms. All processing was done on standard CPU-based machines.\nIndex Terms: speaker recognition, formant, hyperdimensional algebra, random projection, orthogonality, one-shot learning"
        },
        {
            "heading": "1. Introduction",
            "text": "With the emergence of Internet-of-Things devices, speaker recognition at the edge is desirable as it can enable smart environments, cyber-physical security, and robotic control, etc. However, speaker recognition is now done mostly in the cloud due to the constrained resources and battery capacity of small devices that are unable to run complex models. Adapting to new speakers calls for lightweight and efficient algorithms suitable for on-device and online learning.\nDevelopments in deep neural networks have led to end-toend speaker recognition systems that achieve high accuracy on noisy and uncontrolled speech data [1], [2], [3]. Although neural networks have the ability to deal with noisy real-world data, they are expensive to train due to iterative back-propagation using gradient descent. Moreover, the whole network may need to be re-trained when adding new speakers. Non-neural-network approaches include the traditional Gaussian Mixture Model\u2013 Support Vector Machine [4], and the more recent Joint Factor Analysis [5] or the i-vector approaches [6]. However, training these models usually requires running the Expectation Maximization (EM) algorithm iteratively, which can also be computationally intensive for large datasets.\nWe propose a speaker-recognition approach based on computing with high-dimensional (HD) vectors, also called \u201cHyperdimensional\u201d [7]. By mapping data to nearly orthogonal vectors in a high-dimensional space and computing with simple yet powerful operations on vectors, the intrinsic structure of the data can be revealed in a manner that is effective for classification [8, 9]. HD computing has provided an efficient way to analyze various types of data [10] and to achieve fast, online and incremental learning in dealing with text [11, 12], multimodal bio-signals [13\u201316], classifying spoken letters [17], and\nothers [18]. This work extends the application of HD computing to speaker recognition.\nWe start by describing the idea of computing with hypervectors, and the operations that are used to encode speech (Section 2). The proposed speech encoder aims to capture the pronunciation variations between speakers into a speaker profile hypervector (Section 3). The profile is computed in three steps. First, the formants in a time slice are encoded into a hypervector, to capture the variation of the signal over frequencies. Then hypervectors for consecutive time slices are encoded into an N -gram hypervector, to capture the variation of the signal over time. Finally the N -gram hypervectors of a speech sample are added together, to form a profile hypervector that summarizes the course of the power spectrum over time. Ways to improve this basic algorithm using GLVQ are also explored.\nA metric is introduced to evaluate the efficiency of speaker identification systems in terms of the training energy per 1-bit of information gain. This work has achieved a highly competitive energy efficiency due to its small number of active parameters during operation and one-shot learning."
        },
        {
            "heading": "2. Computing with Hypervectors",
            "text": "HD computing originates from Holographic Reduced Representations in the early 1990s [19, 20]. For speech processing, it provides a formulaic way to encode the frequency and temporal structure of a spectrogram into a fixed-dimensional vector.\n\u201cHypervectors\u201d refer to high-dimensional (D > 1,000) seed vectors and to vectors made from them with three operations: addition (+), multiplication (\u2217), and permutation (\u03c1). The seed vectors are chosen at random to represent basic entities\u2014 they are like atoms from which everything else is built. Here they represent differences of spectral power in adjacent frequency bins of a time slice. We use random bipolar vectors (of +1s and \u22121s) as seeds. Addition and multiplication happen coordinate-wise, and permutations reorder (shuffle) hypervector coordinates. The similarity of vectors is measured with the cosine, which equals 0 when the vectors are orthogonal\u2014in a high-dimensional space nearly all pairs of vectors are approximately orthogonal. Computing with vectors is like traditional computing with numbers, except that addition and multiplication now operate on vectors, and no arithmetic operation corresponds to the permutation of coordinates.\nUnlike most machine-learning methods that require iterative training, HD computing offers one-shot and online learning. Learning happens in a single pass over samples from known speakers. The pass produces a D-dimensional profile hypervector\u2014a class prototype\u2014for each speaker. Profiles for test samples are made with the same algorithm and are identified with the most similar speaker profiles. Therefore, the model need not be retrained when speakers are added.\nar X\niv :2\n20 8.\n13 28\n5v 1\n[ cs\n.S D\n] 2\n8 A\nug 2\n02 2"
        },
        {
            "heading": "3. Encoding Speech",
            "text": "Learning the statistics of a signal over time is particularly natural and efficient with hypervectors. The proposed encoder aims to capture the unique pronounciation variation between speakers, similar to the phone N -gram-based modeling [21]. In this approach, a profile hypervector is designed to learn the unique course of the formants over time for each speaker.\nIn a spectrogram, typically up to 4 formants stand out at any moment of time. Taking a spectrum a slice at a time, for example the upper left plot in Figure 1, a formant can be identified by rising power to the left of it and falling to the right. Therefore, formants can be located by comparing the power in adjacent bins. A simple local binary pattern (LBP) encoding [22] is used here to encode the locations of the formants in a time slice. From the first bin to the second bin, the LBP encoder looks at whether the power increases or remains the same, or decreases, and reports 1 or 0. In the VoxCeleb dataset, audio is sampled at 16 kHz, and so the first 40 bins of a power spectrum computed over a 5-ms window cover frequencies from 0 to 8,000 Hz. The power in 40 bins gives rise to 39 differences between neighboring bins, resulting in a 39-bit LBP.\nThe hypervector St for the spectrum at time t summarizes the output of the LBP encoder. It is made from bipolar seed vectors that represent the 0s and the 1s of the LBP. There are a total of 78 seed vectors to choose from, corresponding to power going up or down at each bin. For example, Li[1] or Li[0] represents the power in the (i+1)-th bin greater or less than that in the i-th bin. Then according to the output of the LBP encoder, 39 seed vectors are selected and added together. Finally, the resulting sum vector is transformed to a bipolar vector of 1s and \u22121s by thresholding it at zero:\nSt = \u03b8 ( 39\u2211 i=0 Li[1; 0] ) (1)\nwhere\n\u03b8(x) = { 1 if x > 0 \u22121 if x < 0\n(2)\nis applied coordinate-wise. Only looking at whether the power is increasing or decreasing has the advantage of not being affected by the 1/f power characteristics and the loudness of speech. It is important to note that this encoding yields similar hypervectors for similarly located formants.\nHypervectors St for individual spectra are combined into spectra over time by encoding them inN -grams and adding into a profile hypervector. Empirically, we found that trigrams and tetragrams work the best. Sampling of spectrum slices at 20- ms intervals means that these N -grams represent 60\u201380 ms of speech, or approximately the length of a phoneme.\nA trigram vector is made by permuting the vector for the first spectrum twice, permuting the vector for the second spectrum once, taking the third as is, and multiplying the three vectors coordinate-wise, i.e.:\ntrigram[t] = \u03c12St\u22122 \u2217 \u03c1St\u22121 \u2217 St (3)\nwhere \u03c1 denotes permuting the vector once, and \u03c12 denotes permuting the vector twice. Permutation was implemented by random shuffling the indices of coordinates of the vector.\nFinally, the profile hypervector is formed by summing the trigram vectors over time within an utterance, or across multiple utterances. In VoxCeleb dataset, utterances have been collected for each speaker in different contexts with different recording\nquality and background noise. As it will become clear in Section 4.5, two kinds of profile hypervectors were generated for each speaker: (1) for each context/video/subfolder and (2) one across all the speaker\u2019s contexts. For speaker s, his/her context profile hypervector Vs,c for context c is the sum of all trigrams from all utterances in that context, i.e. Vs,c = \u2211 t trigrams,c[t],\nand the final profile hypervector for speaker s is Vs = \u2211 cVs,c"
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experimental Setup",
            "text": "We used the VoxCeleb dataset [1] to develop and test the algorithm. It consists of speech from 1,251 speakers collected from YouTube videos. Each speaker has a folder, divided into a number of subfolders that contain the audio files. The subfolders come from different videos and are referred to as different \u201ccontexts.\u201d The speech files in the subfolders are called \u201cutterances.\u201d Following the same procedure as in [1], we reserved one subfolder/context for testing that had the fewest utterances but at least five. Each speaker is trained with 904 seconds of speech and tested with 64 seconds, on the average. Top-1, Top-5, and Top-10 accuracies were calculated."
        },
        {
            "heading": "4.2. Input Features",
            "text": "Spectrograms are computed using a 5-ms Hann window and a step size of 20 ms. A short window\u20145 ms vs, the commonly used 25 ms\u2014simplifies the LBP encoding of the formants by smoothing over the multiples of the fundamental frequency and its overtones. Spectrum slices are sampled at 20-ms\nintervals such that a trigram or a tetragram represents 60\u201380 ms of speech, approximately the length of a phoneme."
        },
        {
            "heading": "4.3. Encoder Design",
            "text": "Before training and testing on the entire dataset, a few parameters of the encoder need to be determined, such as the dimension of the hypervectors, which N -gram to use, and the number of frequency bins to encode. The dimension of the hypervectors D was chosen to be 1,024. To determine which N -gram to use, uni-gram to penta-gram were used in the encoder to train and test on 40 speakers\u2019 data, which is sufficient to indicate the results for the entire dataset. The results suggested that tetragrams and trigrams are comparable and perform better than the others. Therefore, trigrams were chosen for the encoder. Similarly, different number of frequency bins (26, 32, and 40) were encoded on 40 speakers\u2019 data, and results suggested that encoding the full number of bins (40) performs best. For the proposed approach, the training and testing on 40 speakers\u2019 data take roughly 5 minutes on an Apple M1 processor, so one can quickly test design parameters."
        },
        {
            "heading": "4.3.1. Weighting",
            "text": "Voice activity detection is usually necessary so that desirable features are extracted only from speech segments. The algorithm so far treated a period of silence the same as speech. To counter the lack of information in silence or a weak signal, we use the total energy in the spectrum to weight its hypervector before including it in the N -gram. The total energy of a spectrum slice at time t is Et = \u2211 f |xf |\n2. Thus the weighted trigram at time t is\n(\u03c12St\u22122 \u2217 \u03c1St\u22121 \u2217 St) \u00b7 (Et\u22122Et\u22121Et)\u03b1 (4)\nwhere the exponent \u03b1 was determined empirically as described in Section 4.3. We found that weighting with the 0.3 power of energy works well with trigrams. Figures 2a and 2b show the correlation matrices of the first 20 speakers\u2019 profile hypervectors before and after applying the weights. It can be seen that the correlations drops after applying the weighting."
        },
        {
            "heading": "4.3.2. Normalized Weights",
            "text": "Performance may be further improved by normalizing the weights to discount large variations in the power of speech segments across different contexts (due to lack of control over recording conditions in the VoxCeleb dataset). As shown in Figure 3, the average power per frequency bin over an utterance for the same speaker can vary over 15 dB. To avoid favoring contexts with louder voices or even noise, the weight for the hypervector of each spectrum slice is scaled by the ratio between the desired maximum bin power and a speaker\u2019s maximum bin power averaged over the utterance where the hypervector at time t is computed from:\nct = Ptarget\nPmax, utterance (5)\nwhere Ptarget is the targeted max bin power set for all utterances to be adjusted to and can be an arbitrary constant. We set it to the average of the largest bin power over the first 40 speakers. Pmax, utterance is the maximum bin power of the utterance being considered and is computed during training. This essentially makes all utterances\u2019 maximum bin power equal, so the weights no longer favor contexts with higher spectral power. Applying\nthis normalizing ratio leads to an increase of 4.3% and 7.3% in the Top-1 and Top-5 accuracies from the weighted case."
        },
        {
            "heading": "4.4. Refinement of profile hypervectors with",
            "text": "Learning Vector Quantization\nThe profile hypervector Vs that sums a speaker\u2019s context hypervectors corresponds to centroid-based classification, which is commonly used in speech and signal processing (e.g., [23\u201325]) due to its simplicity, although it does not guarantee the most accurate classification [26, 27]. Of the various classifiers that can take hypervectors as input (e.g., [17, 28\u201331]), we used the Generalized Learning Vector Quantization (GLVQ) [32], since it is natural to use context hypervectors to initialize prototypes and then refine them iteratively [31]. In each iteration, the classifier uses one misclassified context hypervector Vs,c from one speaker to update the speaker\u2019s profile vector Vs (\u201cprototype\u201d) as well as the profile vector of the nearest (the most similar) speaker. In this manner, the classification accuracy improves after each iteration."
        },
        {
            "heading": "4.5. Results",
            "text": "Table 1 summarizes the identification accuracies on the entire dataset using the methods described above. The baseline is based on a very simple encoder, and any reasonable feature engineering keeps improving the score. Most importantly, training and testing on the entire dataset require only 1.02k active parameters and take only 135.6 minutes (training: 128 minutes; testing: 7.6 minutes) on a regular CPU-based Linux machine (Intel Xeon\u00ae CPU @ 2.40GHz) with a maximum usage of 5 cores during the program. The classification speed is 5.7 ms per 1-second of speech.\nThe GLVQ classifier with a single prototype per speaker is used to obtain refined profile vectors for speakers from the context hypervectors. After every epoch (i.e., a pass over all training samples) the classification accuracies were evaluated. Figure 4 shows the results for Top-1, Top-5, and Top-10 test accuracies. Epoch 0 corresponds to the accuracy of the centroid-based classification. For example, in just two epochs Top-1 accuracy increased from 0.313 to 0.385, and the accuracy started to saturate after approximately 15 epochs reaching 0.479, 0.674, and 0.755 as Top-1, Top-5, and Top-10 accuracies, respectively. Running GLVQ for 30 epochs took only 92 seconds on a regular CPU-based laptop.\nTable 2 compares this work to the state-of-the-art nonneural network and neural network approaches. Although identification accuracy has been the only metric reported for most methods, the cost that comes with it is also an important factor\nMethod Top-1 Top-5 Active Parameters1 Stored Mutual Training Classification during training Parameters Info. Method (Time) Speed\ni-vectors/SVM [1] 0.490 0.566 not reported 1M2 4.04 bits iterative EM+SGD3 not reported\ni-vectors/PLDA/SVM [1] 0.608 0.756 not reported 0.5M4 5.29 bits iterative EM+SGD not reported\nCNN [1] 0.805 0.921 67M 67M 7.57 bits iterative SGD not reported\nACNN [3] 0.855 0.953 4.69M 4.69M 8.20 bits iterative SGD not reported\nThis work 0.479 0.674 HD: 1.02k HD: 1.28M 3.93 bits HD: one-shot (128 min) 5.7 ms perGLVQ: 2.05k GLVQ: 1.28M GLVQ: SGD (92 sec) 1-sec test sample 1 Number of parameters updated for a single pass of one data sample. 2 Assuming two 400-dimensional vectors [1] were stored for each speaker\u2019s SVM. 3 Generally i-vector extractor is trained with the EM algorithm, and SVM is trained with the SGD algorithm. 4 Assuming two 200-dimensional vectors [1] were stored for each speaker\u2019s SVM. 5 SVM: Support Vector Machine; PLDA: Probabilistic Linear Discriminant Analysis; CNN: Convolutional Neural Network; ACNN: Adaptive CNN;\nEM: Expectation Maximization; SGD: Stochastic Gradient Descent.\nthat cannot be ignored. Therefore, we proposed a metric to evaluate the efficiency of a design: the energy to train a model per 1-bit of information gain, i.e.:\nEtrain I = (# active paramaters during training) \u00b7 ttrain I (6)\nwhere ttrain is the training time, and I is the information gain (in bits) from the speaker identification system. 1 Although neural networks achieve 4 more bits of information gain, the en-\n1I can be estimated from the Top-1 accuracy p as I = H(Y ) \u2212 H(Y |X) = log21251 \u2212 ( p \u00b7 log2 1p + (1\u2212 p) \u00b7 log2 1250 1\u2212p ) , where X and Y are the input speaker id and output speaker id of the identification system, assuming every speaker is equally likely to appear at the input and has the same probability p to get identified at the output. If not identified, a speaker gets misclassified to any one of the rest 1250 speakers equally likely.\nergy to train the network is much larger than 2 times, as they take more than 4500\u00d7 active parameters to train iteratively over an unspecified number of epochs. For i-vector approaches, they also require iterative training for i-vector extraction and the perspeaker SVM training. Due to the limited information reported from other works, we are not able to quantify their efficiency. Considering relatively few active parameters used during training and the one-shot learning algorithm, we believe that our approach leads to highly energy-efficient systems for speech processing."
        },
        {
            "heading": "5. Discussion",
            "text": "In this work, we have studied the application of a new computing paradigm to the encoding of speech. With a simple encoding scheme and reasonable feature engineering, it has achieved highly competitive efficiency for its information gain. The results obtained so far are solely based on making use of one acoustic feature (formants) and their course over a short time. There are many more acoustic features yet to be considered, such as the pitch and cepstral coefficients. HD computing is especially suited for encoding a combination of features and producing a fixed-dimensional representation for them [27]. Therefore, its identification accuracy is expected to keep improving when combined with other acoustic features, with a modest increase in computing time and memory use. This work can help to originate a simpler, more energy-efficient machine learning for speech processing."
        },
        {
            "heading": "6. Acknowledgements",
            "text": "PCH was supported by NSF ECCS-2147640. PCH, DK, JMP, BAO, and PK were supported in part by the DARPA\u2019s AIE (HyDDENN Project). DK, BAO, and PK were also supported in part by AFOSR FA9550-19-1-0241. DK was supported by the EU\u2019s MSCA Fellowship (839179)."
        },
        {
            "heading": "7. References",
            "text": "[1] A. Nagrani et al., \u201cVoxCeleb: A large-scale speaker identification\ndataset,\u201d arXiv:1706.08612, 2017.\n[2] J. S. Chung et al., \u201cVoxCeleb2: Deep speaker recognition,\u201d arXiv:1806.05622, 2018.\n[3] S.-H. Kim and Y.-H. Park, \u201cAdaptive convolutional neural network for text-independent speaker recognition,\u201d in Conference of the International Speech Communication Association (INTERSPEECH), 2021, pp. 641\u2013645.\n[4] W. M. Campbell et al., \u201cSupport vector machines using gmm supervectors for speaker verification,\u201d IEEE signal processing letters, vol. 13, no. 5, pp. 308\u2013311, 2006.\n[5] P. Kenny, \u201cJoint factor analysis of speaker and session variability: Theory and algorithms,\u201d CRIM, Montreal,(Report) CRIM-06/0813, vol. 14, no. 28-29, p. 2, 2005.\n[6] N. Dehak et al., \u201cSupport Vector Machines versus fast scoring in the low-dimensional total variability space for speaker verification,\u201d in Annual Conference of the International Speech Communication Association, 2009.\n[7] P. Kanerva, \u201cHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\u201d Cognitive computation, vol. 1, no. 2, pp. 139\u2013 159, 2009.\n[8] J. Wong, \u201cNegative capacitance and hyperdimensional computing for unconventional low-power computing,\u201d Ph.D. dissertation, UC Berkeley, 2018.\n[9] E. Osipov et al., \u201cHyperSeed: Unsupervised Learning with Vector Symbolic Architectures,\u201d arXiv:2110.08343, pp. 1\u201312, 2021.\n[10] D. Kleyko et al., \u201cA Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations,\u201d arXiv:2111.06077, 2021.\n[11] A. Joshi et al., \u201cLanguage geometry using random indexing,\u201d in QI Symposium, 2016, pp. 265\u2013274.\n[12] P. Alonso et al., \u201cHyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing enabled Embedding of n-gram Statistics,\u201d in IJCNN, 2021, pp. 1\u20139.\n[13] A. Rahimi et al., \u201cEfficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of exg signals,\u201d Proceedings of the IEEE, vol. 107, no. 1, pp. 123\u2013143, 2018.\n[14] A. Moin et al., \u201cA wearable biosensing system with in-sensor adaptive machine learning for hand gesture recognition,\u201d Nature Electronics, vol. 4, no. 1, pp. 54\u201363, 2021.\n[15] A. Zhou et al., \u201cMemory-efficient, limb position-aware hand gesture recognition using hyperdimensional computing,\u201d arXiv:2103.05267, 2021.\n[16] L. Ge and K. K. Parhi, \u201cSeizure detection using power spectral density via hyperdimensional computing,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 7858\u20137862.\n[17] M. Imani et al., \u201cVoiceHD: Hyperdimensional Computing for Efficient Speech Recognition,\u201d in IEEE ICRC, 2017, pp. 1\u20138.\n[18] D. Kleyko et al., \u201cA Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges ,\u201d arXiv:2112.15424, pp. 1\u201336, 2021.\n[19] T. A. Plate, \u201cDistributed representations and nested compositional structure,\u201d Ph.D. dissertation, University of Toronto, 1994.\n[20] \u2014\u2014, Holographic Reduced Representation. Stanford, CA: CSLI Publications, 2003.\n[21] M. A. Kohler et al., \u201cPhonetic speaker recognition,\u201d in Asilomar Conference on Signals, Systems & Computers, vol. 2, 2001, pp. 1557\u20131561. [22] A. Burrello et al., \u201cHyperdimensional computing with local binary patterns: One-shot learning of seizure onset and identification of ictogenic brain regions using short-time ieeg recordings,\u201d IEEE Transactions on Biomedical Engineering, vol. 67, no. 2, pp. 601\u2013613, 2019.\n[23] O. Ra\u0308sa\u0308nen, \u201cGenerating Hyperdimensional Distributed Representations from Continuous Valued Multivariate Sensory Input,\u201d in Annual Meeting of the Cognitive Science Society (CogSci), 2015, pp. 1943\u20131948.\n[24] D. Kleyko et al., \u201cClassification and Recall with Binary Hyperdimensional Computing: Tradeoffs in Choice of Density and Mapping Characteristic,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 12, pp. 5880\u20135898, 2018.\n[25] L. Ge and K. K. Parhi, \u201cClassification using Hyperdimensional Computing: A Review,\u201d IEEE Circuits and Systems Magazine, vol. 20, no. 2, pp. 30\u201347, 2020.\n[26] A. Rosato et al., \u201cHyperdimensional Computing for Efficient Distributed Classification with Randomized Neural Networks,\u201d in IJCNN, 2021, pp. 1\u201310.\n[27] J. Karlgren and P. Kanerva, \u201cSemantics in High-Dimensional Space,\u201d Frontiers in Artificial Intelligence, vol. 4, pp. 1\u20136, 2021.\n[28] D. A. Rachkovskij, \u201cLinear Classifiers based on Binary Distributed Representations,\u201d Information Theories and Applications, vol. 14, no. 3, pp. 270\u2013274, 2007.\n[29] A. Rahimi et al., \u201cHyperdimensional Biosignal Processing: A Case Study for EMG-based Hand Gesture Recognition,\u201d in IEEE ICRC, 2016, pp. 1\u20138.\n[30] D. Kleyko et al., \u201cDensity encoding enables resource-efficient randomly connected neural networks,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 8, pp. 3777\u2013 3783, 2021.\n[31] C. Diao et al., \u201cGeneralized Learning Vector Quantization for Classification in Randomized Neural Networks and Hyperdimensional Computing,\u201d in IJCNN, 2021, pp. 1\u20139.\n[32] A. Sato and K. Yamada, \u201cGeneralized Learning Vector Quantization,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 1996, pp. 423\u2013429."
        }
    ],
    "title": "Computing with Hypervectors for Efficient Speaker Identification",
    "year": 2022
}