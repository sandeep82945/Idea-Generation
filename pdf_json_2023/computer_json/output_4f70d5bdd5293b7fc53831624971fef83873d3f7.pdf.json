{
    "abstractText": "In this paper, we address the problem of video temporal sentence localization, which aims to localize a target moment from videos according to a given language query. We observe that existing models suffer from a sheer performance drop when dealing with simple phrases contained in the sentence. It reveals the limitation that existing models only capture the annotation bias of the datasets but lack sufficient understanding of the semantic phrases in the query. To address this problem, we propose a phrase-level Temporal Relationship Mining (TRM) framework employing the temporal relationship relevant to the phrase and the whole sentence to have a better understanding of each semantic entity in the sentence. Specifically, we use phrase-level predictions to refine the sentencelevel prediction, and use Multiple Instance Learning to improve the quality of phrase-level predictions. We also exploit the consistency and exclusiveness constraints of phrase-level and sentence-level predictions to regularize the training process, thus alleviating the ambiguity of each phrase prediction. The proposed approach sheds light on how machines can understand detailed phrases in a sentence and their compositions in their generality rather than learning the annotation biases. Experiments on the ActivityNet Captions and CharadesSTA datasets show the effectiveness of our method on both phrase and sentence temporal localization and enable better model interpretability and generalization when dealing with unseen compositions of seen concepts. Code can be found at https://github.com/minghangz/TRM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Minghang Zheng"
        },
        {
            "affiliations": [],
            "name": "Sizhe Li"
        },
        {
            "affiliations": [],
            "name": "Qingchao Chen"
        },
        {
            "affiliations": [],
            "name": "Yuxin Peng"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        }
    ],
    "id": "SP:b534d3c902852e275ebe9995fd07c8ffad2f0473",
    "references": [
        {
            "authors": [
                "S. Chen",
                "Y.-G. Jiang"
            ],
            "title": "Semantic Proposal for Activity Localization in Videos via Sentence Query",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01): 8199\u20138206.",
            "year": 2019
        },
        {
            "authors": [
                "X. Ding",
                "N. Wang",
                "S. Zhang",
                "D. Cheng",
                "X. Li",
                "Z. Huang",
                "M. Tang",
                "X. Gao"
            ],
            "title": "Support-Set Based CrossSupervision for Video Grounding",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11553\u2013 11562.",
            "year": 2021
        },
        {
            "authors": [
                "J. Gao",
                "C. Sun",
                "Z. Yang",
                "R. Nevatia"
            ],
            "title": "Tall: Temporal activity localization via language query",
            "venue": "Proceedings of the IEEE international conference on computer vision, 5267\u20135275.",
            "year": 2017
        },
        {
            "authors": [
                "J. Gao",
                "X. Sun",
                "M. Xu",
                "X. Zhou",
                "B. Ghanem"
            ],
            "title": "Relation-aware Video Reading Comprehension for Temporal Language Grounding",
            "venue": "ArXiv, abs/2110.05717.",
            "year": 2021
        },
        {
            "authors": [
                "J. Gao",
                "C. Xu"
            ],
            "title": "Fast Video Moment Retrieval",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), 1503\u20131512.",
            "year": 2021
        },
        {
            "authors": [
                "J. Huang",
                "H. Jin",
                "S. Gong",
                "Y. Liu"
            ],
            "title": "Video Activity Localisation with Uncertainties in Temporal Boundary",
            "venue": "European Conference on Computer Vision, 724\u2013740. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "J. Huang",
                "Y. Liu",
                "S. Gong",
                "H. Jin"
            ],
            "title": "Crosssentence temporal and semantic relations in video activity localisation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 7199\u20137208.",
            "year": 2021
        },
        {
            "authors": [
                "R. Krishna",
                "K. Hata",
                "F. Ren",
                "L. Fei-Fei",
                "J.C. Niebles"
            ],
            "title": "Dense-Captioning Events in Videos",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2017
        },
        {
            "authors": [
                "J. Li",
                "J. Xie",
                "L. Qian",
                "L. Zhu",
                "S. Tang",
                "F. Wu",
                "Y. Yang",
                "Y. Zhuang",
                "X.E. Wang"
            ],
            "title": "Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning",
            "venue": "CoRR, abs/2203.13049.",
            "year": 2022
        },
        {
            "authors": [
                "S. Li",
                "C. Li",
                "M. Zheng",
                "Y. Liu"
            ],
            "title": "Phrase-level Prediction for Video Temporal Localization",
            "venue": "International Conference on Multimedia Retrieval (ICMR), 360\u2013 368.",
            "year": 2022
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2980\u20132988.",
            "year": 2017
        },
        {
            "authors": [
                "B. Liu",
                "S. Yeung",
                "E. Chou",
                "D.-A. Huang",
                "L. Fei-Fei",
                "J.C. Niebles"
            ],
            "title": "Temporal modular networks for retrieving complex compositional activities in videos",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), 552\u2013568.",
            "year": 2018
        },
        {
            "authors": [
                "D. Liu",
                "X. Qu",
                "X. Di",
                "Y. Cheng",
                "Z. Xu",
                "P. Zhou"
            ],
            "title": "Memory-Guided Semantic Learning Network for Temporal Sentence Grounding",
            "venue": "arXiv preprint arXiv:2201.00454.",
            "year": 2022
        },
        {
            "authors": [
                "D. Liu",
                "X. Qu",
                "J. Dong",
                "P. Zhou",
                "Y. Cheng",
                "W. Wei",
                "Z. Xu",
                "Y. Xie"
            ],
            "title": "Context-aware Biaffine Localizing Network for Temporal Sentence Grounding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11235\u201311244.",
            "year": 2021
        },
        {
            "authors": [
                "D. Liu",
                "X. Qu",
                "P. Zhou",
                "Y. Liu"
            ],
            "title": "Exploring Motion and Appearance Information for Temporal Sentence Grounding",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "J. Mun",
                "M. Cho",
                "B. Han"
            ],
            "title": "Local-global videotext interactions for temporal grounding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10810\u201310819.",
            "year": 2020
        },
        {
            "authors": [
                "A. v. d. Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "M. Otani",
                "Y. Nakashima",
                "E. Rahtu",
                "J. Heikkil\u00e4"
            ],
            "title": "Uncovering Hidden Challenges in Query-Based Video Moment Retrieval",
            "venue": "31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event, UK, September 7-10, 2020. BMVA Press.",
            "year": 2020
        },
        {
            "authors": [
                "X. Qu",
                "P. Tang",
                "Z. Zou",
                "Y. Cheng",
                "J. Dong",
                "P. Zhou",
                "Z. Xu"
            ],
            "title": "Fine-grained iterative attention network for temporal language localization in videos",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, 4280\u20134288.",
            "year": 2020
        },
        {
            "authors": [
                "C. Rodriguez-Opazo",
                "E. Marrese-Taylor",
                "B. Fernando",
                "H. Li",
                "S. Gould"
            ],
            "title": "DORi: Discovering Object Relationships for Moment Localization of a Natural Language Query in a Video",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 1079\u2013",
            "year": 2021
        },
        {
            "authors": [
                "A. Rohrbach",
                "M. Rohrbach",
                "R. Hu",
                "T. Darrell",
                "B. Schiele"
            ],
            "title": "Grounding of Textual Phrases in Images by Reconstruction",
            "venue": "Leibe, B.; Matas, J.; Sebe, N.; and Welling, M., eds., Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October",
            "year": 2016
        },
        {
            "authors": [
                "H. Ryu",
                "S. Kang",
                "H. Kang",
                "C.D. Yoo"
            ],
            "title": "Semantic Grouping Network for Video Captioning",
            "venue": "ThirtyFifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on",
            "year": 2021
        },
        {
            "authors": [
                "V. Sanh",
                "L. Debut",
                "J. Chaumond",
                "T. Wolf"
            ],
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "P. Shi",
                "J.J. Lin"
            ],
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "venue": "ArXiv, abs/1904.05255.",
            "year": 2019
        },
        {
            "authors": [
                "G.A. Sigurdsson",
                "G. Varol",
                "X. Wang",
                "A. Farhadi",
                "I. Laptev",
                "A.K. Gupta"
            ],
            "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding",
            "venue": "ArXiv, abs/1604.01753.",
            "year": 2016
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556.",
            "year": 2014
        },
        {
            "authors": [
                "X. Song",
                "L. Jiao",
                "S. Yang",
                "X. Zhang",
                "F. Shang"
            ],
            "title": "Sparse coding and classifier ensemble based multi-instance learning for image categorization",
            "venue": "Signal Processing, 93(1): 1\u201311.",
            "year": 2013
        },
        {
            "authors": [
                "D. Tran",
                "L. Bourdev",
                "R. Fergus",
                "L. Torresani",
                "M. Paluri"
            ],
            "title": "Learning spatiotemporal features with 3d convolutional networks",
            "venue": "Proceedings of the IEEE international conference on computer vision, 4489\u20134497.",
            "year": 2015
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "H. Wang",
                "Z.-J. Zha",
                "L. Li",
                "D. Liu",
                "J. Luo"
            ],
            "title": "Structured Multi-Level Interaction Network for Video Moment Localization via Language Query",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7026\u20137035.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "L. Wang",
                "T. Wu",
                "T. Li",
                "G. Wu"
            ],
            "title": "Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding",
            "venue": "CoRR, abs/2109.04872.",
            "year": 2021
        },
        {
            "authors": [
                "T. Wolf",
                "L. Debut",
                "V. Sanh",
                "J. Chaumond",
                "C. Delangue",
                "A. Moi",
                "P. Cistac",
                "T. Rault",
                "R. Louf",
                "M Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "arXiv preprint arXiv:1910.03771",
            "year": 2019
        },
        {
            "authors": [
                "S. Xiao",
                "L. Chen",
                "S. Zhang",
                "W. Ji",
                "J. Shao",
                "L. Ye",
                "J. Xiao"
            ],
            "title": "Boundary Proposal Network for Two-Stage Natural Language Video Localization",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 2986\u20132994.",
            "year": 2021
        },
        {
            "authors": [
                "H. Xu",
                "S. Venugopalan",
                "V. Ramanishka",
                "M. Rohrbach",
                "K. Saenko"
            ],
            "title": "A multi-scale multiple instance video description network",
            "venue": "arXiv preprint arXiv:1505.05914.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Xu",
                "J.-Y. Zhu",
                "E.I.-C. Chang",
                "M. Lai",
                "Z. Tu"
            ],
            "title": "Weakly supervised histopathology cancer image segmentation and classification",
            "venue": "Medical Image Analysis, 18(3): 591\u2013 604.",
            "year": 2014
        },
        {
            "authors": [
                "W. Yang",
                "T. Zhang",
                "Y. Zhang",
                "F. Wu"
            ],
            "title": "Local correspondence network for weakly supervised temporal sentence grounding",
            "venue": "IEEE Transactions on Image Processing, 30: 3252\u20133262.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Yuan",
                "X. Lan",
                "L. Chen",
                "W. Liu",
                "X. Wang",
                "W. Zhu"
            ],
            "title": "A Closer Look at Temporal Sentence Grounding in Videos: Datasets and Metrics",
            "venue": "CoRR, abs/2101.09028.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Yuan",
                "X. Liang",
                "X. Wang",
                "D.-Y. Yeung",
                "A. Gupta"
            ],
            "title": "Temporal Dynamic Graph LSTM for Action-driven Video Object Detection",
            "venue": "ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "R. Zeng",
                "H. Xu",
                "W. bing Huang",
                "P. Chen",
                "M. Tan",
                "C. Gan"
            ],
            "title": "Dense Regression Network for Video Grounding",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "D. Zhang",
                "X. Dai",
                "X.E. Wang",
                "Y. fang Wang",
                "L.S. Davis"
            ],
            "title": "MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhang",
                "A. Sun",
                "W. Jing",
                "L. Zhen",
                "J.T. Zhou",
                "R.S.M. Goh"
            ],
            "title": "Parallel Attention Network with Sequence Matching for Video Grounding",
            "venue": "FINDINGS.",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhang",
                "A. Sun",
                "W. Jing",
                "J.T. Zhou"
            ],
            "title": "Spanbased localizing network for natural language video localization",
            "venue": "arXiv preprint arXiv:2004.13931.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Peng"
            ],
            "title": "Hierarchical Vision-Language Alignment for Video Captioning",
            "venue": "Kompatsiaris, I.; Huet, B.; Mezaris, V.; Gurrin, C.; Cheng, W.; and Vrochidis, S., eds., MultiMedia Modeling - 25th International Conference, MMM 2019, Thessaloniki, Greece, January 8-11, 2019, Pro-",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhang",
                "H. Peng",
                "J. Fu",
                "J. Luo"
            ],
            "title": "Learning 2D Temporal Adjacent Networks forMoment Localization with Natural Language",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zhang",
                "X. Han",
                "X. Song",
                "Y. Yan",
                "L. Nie"
            ],
            "title": "Multi-Modal Interaction Graph Convolutional Network for Temporal Language Localization in Videos",
            "venue": "IEEE Transactions on Image Processing, 30: 8265\u20138277.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhao",
                "Z. Zhao",
                "Z. Zhang",
                "Z. Lin"
            ],
            "title": "Cascaded Prediction Network via Segment Tree for Temporal Video Grounding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4197\u20134206.",
            "year": 2021
        },
        {
            "authors": [
                "M. Zheng",
                "Y. Huang",
                "Q. Chen",
                "Y. Liu"
            ],
            "title": "Weakly supervised video moment localization with contrastive negative sample mining",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 1, 3.",
            "year": 2022
        },
        {
            "authors": [
                "M. Zheng",
                "Y. Huang",
                "Q. Chen",
                "Y. Peng",
                "Y. Liu"
            ],
            "title": "Weakly Supervised Temporal Sentence Grounding With Gaussian-Based Contrastive Proposal Learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15555\u201315564.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhou",
                "C. Zhang",
                "Y. Luo",
                "Y. Chen",
                "C. Hu"
            ],
            "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8445\u2013 8454.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Video temporal sentence localization has become an important research problem due to its potential for a wide range of practical applications, requiring intelligent systems to identify the start and end timestamps of segments (i.e., moments) with respect to any given language queries in an untrimmed video. Using free-form natural language as queries allows users to freely search for interesting content without being restricted to pre-defined classes, which makes sentence localization have greater application potential. The model is expected to understand the visual and language concepts and their compositions to achieve robust performance.\n*Corresponding author Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nFully supervised approaches have made steady progress in the last decades when the queries are complete sentences. However, human-generated queries \u2018in the wild\u2019 vary a lot in terms of specificity, we expect models to deal with both complete sentences (the query marked in green in Fig. 1(a)) and short phrases (the query marked in blue in Fig. 1(a)) to be competent for real-world applications. However, we empirically observe that even the most recent open-source models learned by using sentence annotations lack the capability to deal with the phrase-level query, as shown in Fig. 1(b). We evaluate the existing method (Wang et al. 2021b) on the\nCharades-STA dataset, and observe a sheer drop in prediction accuracy: IoU@0.3 is dropped by 22.07% and 27.35% when dealing with simpler verb queries and noun queries.\nUsually, a word or group of words forms a syntactic constituent with a single grammatical function (ie. verb, subject, or object), representing a more straightforward semantic meaning than sentences (no need to understand their compositions). The typical failure in much more straightforward scenarios reveals the following problems. First, existing models tend to capture the annotation bias in the benchmark but lack sufficient understanding of the intrinsic relationship between simple visual and language concepts. Consequently, existing models may easily fail when the unrealistic assumption of the in-distribution test setting does not hold, i.e., incapable of generalizing to novel combinations of visual entities and text, which is also revealed by (Otani et al. 2020; Yuan et al. 2021; Li et al. 2022a). Second, the models\u2019 interpretability and robustness are questioned since they fail to deal with simple (atomic) concepts, even though they achieve decent results in sentence-level prediction tasks. This may hinder the application of these methods in real scenarios.\nMotivated by the above observations, we attempt to take phrase-level prediction into consideration of temporal localization models\u2019 designation. To avoid the high annotation cost and subjective annotation bias of fine-grained phrases, we propose phrase-level Temporal Relationship Mining (TRM) framework to improve the phrase temporal localization using sentence-level supervision only. The two key ideas underpinning this framework are as follows. First, inspired by the successful application of Multiple Instance Learning (MIL) to weakly supervised temporal sentence localization, we train the model to discriminate between matched and unmatched video-phrase pairs without phrase-level annotations. Second, in order to consider the constraints of sentence-level annotations on phrase-level predictions, we exploit the temporal localization relationship relevant to the phrase and the whole sentence and follow the two design principles -consistency and exclusiveness. Specifically, consistency requires every phrase-level prediction should share a period with the annotated sentence-level ground truth. As shown in Fig 1(a), all predictions of the phrases \u201dputs on\u201d, \u201dgloves\u201d, \u201dclean\u201d and \u201dsnow\u201d should overlap with the sentence ground truth annotation (in green). Exclusiveness requires that every period not intersect the sentence ground truth (as shown in red boxes in Fig 1(a)) is at least excluded from one phrase-level prediction (not intersect at least one phrase prediction). Combining the above two key ideas, the performance of our model on phrase level prediction has been significantly improved (18.62% improvement for verb phrases and 45.12% improvement for noun phrases in Fig. 1(b)).\nOur contributions are summarized as follows: (1) We highlight the importance of phrases in video temporal localization and exploit the temporal relationship relevant to phrases and the whole sentence. (2) We propose phrase-level Temporal Relationship Mining (TRM) framework to investigate phrase-level prediction using sentence-level supervision only, which proposes the consistency and exclusive-\nness constraints to regularize the training process. (3) Experiments on Charades-STA and ActivityNet Captions demonstrate our method\u2019s ability to improve phrase-level performance while performance in sentence-level settings remains stable, achieving better generalization performance."
        },
        {
            "heading": "Related Work",
            "text": ""
        },
        {
            "heading": "Temporal Sentence Localization",
            "text": "Since being proposed by TALL (Gao et al. 2017), the task has drawn wide attention. Most previous methods either generate candidate proposals and rank them using multimodal features (Zhang et al. 2020b), or use multi-modal features to generate timestamp predictions directly (Zhang et al. 2020a). Recent works have started to consider finegrained vision and language information. For example, for vision information, DORi (Rodriguez-Opazo et al. 2021) and MARN (Liu et al. 2022b) consider the features of objects within the video and improve models\u2019 performance. Correspondingly, for language features. LGI (Mun, Cho, and Han 2020) generates sub-query features to implicitly consider fine-grained text features and boost sentence localization performance. MMN (Wang et al. 2021b) trains the model to distinguish matched and unmatched videosentence pairs collected from both intra-video and intervideo. MGSL-Net (Liu et al. 2022a), which uses memory to reinforce uncommon samples in the training process. EMB (Huang et al. 2022) constructs elastic boundaries to handle the uncertainties in temporal boundary. VISA (Li et al. 2022a) considers the distribution of different entities and conducts the Charades-CG and ActivityNet-CG dataset splits to test the compositional generalization, where the novel composition of seen phrases will appear in the test split. However, we found that existing approaches perform poorly when using simpler phrases as queries, suggesting that they do not really understand the intrinsic connection between vision and language. In this paper, we propose a unified framework dealing with both sentence and phrase queries simultaneously and improve the performance.\nMultiple Instance Learning Multiple Instance Learning (MIL) has been widely applied in computer vision, such as content-based image retrieval (Song et al. 2013), object localization, and segmentation (Xu et al. 2015), computer-aided diagnosis and detection (Xu et al. 2014), etc. Although (Huang et al. 2021; Yang et al. 2021; Huang et al. 2021; Zheng et al. 2022a,b) use MIL to solve the weakly supervised temporal sentence localization, where only videos and natural language queries are available during training, no previous work has tried to use it to solve the phrase-level video temporal localization problem.\nMoreover, directly regarding phrase-level prediction as a weakly supervised task and introducing MIL ignores the constraint of sentence-level annotations on phrase-level predictions. Thus, we exploit the relationship between phraselevel predictions and sentence-level annotations and put forward the assumptions of consistency and exclusivity."
        },
        {
            "heading": "Phrase in Vision-Language Tasks",
            "text": "Phrase-level features can provide models with more finegrained text representations and have wide applications in vision-language tasks, such as video grounding (Mun, Cho, and Han 2020; Rohrbach et al. 2016), video captioning (Ryu et al. 2021; Zhang and Peng 2019), etc. LGI (Mun, Cho, and Han 2020) first exploits sub-query features. However, it simply fuses them in an early stage to obtain fine-grained sentence features, neither locating the phrases directly nor considering the relationship between the localization results of phrases and sentence. This results of LGI still perform poorly when encountering phrases as queries, as shown in Tab. 2. For the first time, PLPNet (Li et al. 2022b) directly considers the problem of locating a phrase and improves the performance of phrase-level localization through contrastive learning. However, it has no extra constraints on phrase-level predictions and sentence-level predictions, dismissing the intrinsic connection between the video periods related to a sentence and its phrases. In this paper, we propose a unified framework to deal with both sentence and phrase queries simultaneously and improve the performance of both. We introduce constraints from the perspective of prediction results so that the TRM model can directly supervise the predicted phrase-level timestamps without extra phrase-level annotations. To our best knowledge, we are the first to investigate the temporal relationship between phrase-level prediction and sentence-level prediction explicitly. This setting is more in line with real-world application scenarios and enables the model to generalize to unseen combinations of seen phrases.\nMethod"
        },
        {
            "heading": "Overview",
            "text": "Fig. 2 illustrates the overall architecture of our proposed TRM framework. We first extract video representation and generate a 2D Temporal Map (Zhang et al. 2020b). Meanwhile, the query encoder generates phrases and extracts text features for both phrases and sentences. To represent the similarity between the text and each video proposal, we generate score maps using the 2D temporal map and the text feature for sentences and all phrases. Due to the lack of phraselevel annotation, we explored the consistency and exclusiveness relationship between phrases and sentences as the loss function to regularize the training process and improve the accuracy of phrase score maps. Since the phrase-level score maps can provide more fine-grained information for the sentence, we use them to refine the sentence score map with a weighted sum option as well, and the weight of each phrase represent its importance. Finally, we optimize the refined sentence score map with an IoU regression loss and a contrastive learning loss."
        },
        {
            "heading": "Model Architecture",
            "text": "Video Encoder The video encoder aims at extracting video features and generating a 2D temporal map for similarity learning. We extract features from the input video and encode them as a 2D temporal adjacent feature map following MMN (Wang et al. 2021b). For an input video, we first split it into small video clips, each containing equal\nframes. Then we extract the clip-level visual feature with a pre-trained CNN model. We can obtain N clip-level features {fVi }Ni=1 \u2208 RN\u00d7d, where N is the number of clips and d is the feature dimension. Then, we build up the 2D proposal feature map FV \u2208 RN\u00d7N\u00d7d following MMN (Wang et al. 2021b), where proposal FVi,j represents the video candidate starting from the i-th clip and ending with the j-th clip.\nQuery Encoder The query encoder aims to generate finegrained phrases for a sentence and extract both sentence and phrase-level text features. More specifically, given a query sentence S, we first parse Np phrases [p1, p2, ..., pNp ] using pre-trained SRLBERT(Shi and Lin 2019). SRLBERT assigns semantic role labels to each word in the sentence, while we only keep the semantic roles with more than 1000 occurrences in the training set as phrases. Then, we use a pre-trained DistilBERT (Sanh et al. 2019) model following MMN (Wang et al. 2021b) to extract the features of sentences and phrases at the same time. Phrases provide finegrained information to the sentence, and the sentence provides global information to phrases. Therefore, we further interact sentence and phrase features through a single-layer transformer encoder (Vaswani et al. 2017). The final sentence feature and phrase features are represented as fS \u2208 Rd and fP \u2208 RNp\u00d7d respectively.\nSimilarity Learning Module To learn the semantic relevance of each sentence and phrase with each temporal proposal, we generate score maps for both sentence and phrases according to the similarity of text and video features. In order to improve the quality of phrase score maps, we propose two assumptions of consistency and exclusivity to constrain the phrase score maps. Since phrases provide finer-grained semantic information for sentences, we use the phrase score maps to refine the sentence score map so that it can summarize the attentional information for each phrase. We use a weighted sum option over the phrase score maps and leverage phrase weights to describe the importance of different phrases. Finally, we optimize the refined sentence score map with an IoU regression loss and a contrastive learning loss.\nScore Map Generation. For the sentence, we perform 1 \u00d7 1 convolution operation on visual feature map F and perform a linear projection on text features fS respectively to project the features of two modalities into the same dimension dH . The final representations of sentence features fSiou \u2208 Rd H and visual features FViou \u2208 RN\u00d7N\u00d7d H are:\nfSiou = FCiou(f S), FViou = Conviou(F V ) (1)\nwhere FC(\u00b7) is a fully connected network and Conv(\u00b7) is an 1 \u00d7 1 convolution. Then we regard the cosine similarity of fSiou and F V iou as sentence-level score map: S\ns = FV Tiou f S iou \u2208 RN\u00d7N , in which Ssi,j represents the similarity score between the sentence and the proposal from the i-th video clip to the j-th video clip.\nTemporal Relation Mining. In previous works (Wang et al. 2021b; Zhang et al. 2020b), the sentence score map is directly used to predict the timestamps. However, it dismisses the fine-grained phrases inside the query, and has poor performance when the query is a single phrase. To solve\nthis problem, we build phrase score maps and mine the temporal relationship between the phrases and the sentence. Due to the lack of phrase-level annotation data, we impose constraints between the phrase score maps for training purposes. We have the following two hypotheses considering the relationship between phrases and sentences:\n1. consistency: For paired sentences and videos, every phrase-level prediction should share a period with the annotated sentence-level ground truth. For unpaired sentences and videos, at least one phrase-level prediction does not share a period with the annotated ground truth.\n2. exclusiveness: Each frame outside the ground truth is not contained in at least one phrase-level prediction result.\nIn detail, we first obtain the text feature fPi,iou \u2208 Rd H for the i-th phrase through Eq (1). Then we regard the cosine similarity as moments\u2019 estimation score map Sp of each\nphrase: Spi = F V T iou f P i,iou \u2208 RN\u00d7N . Inspired by Multiple Instance Learning, we also randomly sample unmatched phrases in a batch and compute their score map S\u0302p. Based on the degree of intersection with the sentence ground truth, we divide all proposals into two subsets. As shown in the left half of Fig. 3, all the proposals in Area I have an IoU with the ground-truth moment large than a certain threshold \u03b8, while the opposite is true for all proposals in Area II.\nOur consistency loss ensures that each phrase-level prediction should be located in Aera I, which is illustrated in Fig.3. That is: for each phrase score map, the max score (marked by black) in Area I should be 1. Our consistency loss also requires that for a negative sentence, there should be at least one phrase that mismatches any proposal in Area I, which is represented in Fig.3 as Nneg\nmin i=1 max A1\nSi \u2192 0. The\nconsistency loss can be described as follows:\nLcon = Np max i=1 (Lf ( max (s,t)\u2208A1 Spi [s, t], 1))+\nNp\nmin i=1 (Lf ( max (s,t)\u2208A1\nS\u0302pi [s, t], 0))\n(2)\nwhere Lf is the focal loss (Lin et al. 2017) to balance the positive and negative samples, A1 represents Area I, and A2 represents Area II.\nOur exclusiveness loss requires that each proposal in Area II should mismatch at least one phrase of the query sentence. That is: as shown in Fig. 3, at least one of the phrase\u2019s scores should be 0 (i.e. the minimum score marked by green should be 0) for all the proposals in Area II. The exclusiveness loss can be described as follows:\nLex = 1 |A2| \u2211\n(s,t)\u2208A2\nLf ( Np\nmin i=1\n(Spi [s, t]), 0) (3)\nSentence Score Map Refinement.. Since the phrase-level score maps can provide more fine-grained information for the sentence, we use them to refine the original sentence score map Ss \u2208 RN\u00d7N . We gain the final sentence score map S \u2208 RN\u00d7N by aggregating the score maps of the sentence and all of its phrases, which is shown as follows:\n\u03b1 = softmax(MLPsatt([p1, p2, ..., pNp ])) (4) S = Ss + \u2211\n\u03b1iS p i \u2208 R N\u00d7N (5)\nwhere \u03b1 \u2208 RNp is the phrase weights that describe the importance of different phrases, MLPsatt denotes a multilayer perception with a output layer of 1-dimension.\nTo supervise the sentence score map, we apply the binary cross entropy loss to regress the IoU score of each proposal. Following (Zhang et al. 2020b), we adopt a scaled IoU value yi as the supervision scale, but not a hard binary score. Then the binary cross entropy loss can be expressed as\nLiou = \u2212 1\nC C\u2211 i=1 (yilogSi + (1\u2212 yi)log(1\u2212 Si)), (6)\nwhere C is the number of proposals. Sentence-level Contrastive Learning. Following MMN (Wang et al. 2021b), we also use contrastive learning to provide more supervised signals to the model. We collect positive and negative sentence-video pairs within and between videos, and use noise contrastive estimation (Oord, Li, and Vinyals 2018) to estimate two conditional distributions p(s|v) and p(v|s). The former represents the probability that a sentence s matches the video v when giving v, and the latter represents the probability that a video v matches the sentence s when giving s. We adopt the contrastive loss to help capture better information between modalities as follows:\nLcont = \u2212( \u2211 s\u2208S logp(vs|s) + \u2211 v\u2208V logp(sv|v)) (7)\nwhere S,V are the sets of training sentences and video in a batch, vs is the video that matches the sentence s, and sv is the sentence that matches the video v."
        },
        {
            "heading": "Training and Inference",
            "text": "Training The total loss of our model is as follows.\nL = Liou + Lcont + Lcon + Lex (8) Given the lack of phrase-level annotations, we can still optimize the understanding of phrases during training with the constraints between the whole sentence and phrases.\nInference At the inference time, when given a sentence query, we can obtain the refined score maps S through Eq(5) to make predictions. When given a single phrase query, we can treat it as a sentence (as the text encoders for phrase and sentence are shared). In this case, the score maps of the sentence and phrase are the same and both can be used to output phrase predictions.\nExperiments"
        },
        {
            "heading": "Dataset",
            "text": "Charades-STA Charades-STA (Gao et al. 2017) originates from Charades (Sigurdsson et al. 2016) dataset, containing indoor videos with sentence queries and corresponding annotations. There are 12,408 and 3,720 video-query pairs for training and testing respectively. Our sentence-level results are reported on the test split.\nActivityNet Captions ActivityNet Captions (Krishna et al. 2017) contains 20K videos, with 37,417/17,505/17,031 video-query pairs in the train /val 1/val 2 split. We adopt standard splits and report the sentence-level results on the val 2 split."
        },
        {
            "heading": "Experiment Settings",
            "text": "Evaluation Metric. Following (Gao et al. 2017), we adopt the \u201cR@1, IoU = m\u201d and mIoU (the mean average IoU) metrics to evaluate the model\u2019s performance. Specifically, this metric evaluates the percentage of predicted moments that have the temporal Intersection over Union (IoU) larger than the threshold m, and m is set to {0.3, 0.5, 0.7}. Evaluation for phrase. When evaluating the performance of phrases, we use a single phrase rather than a complete sentence as the query, in which case the score map of the sentence and phrase is the same and both can be used to output predictions. Due to the lack of phrase-level annotations, we adopt the action annotation used for the Temporal Action Localization task and use the action names as the query phrases. Although we only tested with verbs, our model can handle arbitrary phrases. To prove this, we also use the object annotations on the Charades-STA dataset provided by (Yuan et al. 2017). We collect the common noun phrases in the sentences, and get the time of the first appearance and the last disappearance of the object in the object annotation as the noised noun phrase ground truth timestamps. We report the evaluation results of our model when using noun phrases as queries in the ablation section. It is worth noting that we only use the phrase-level annotations for evaluating the model\u2019s performance on phrases, and avoid using them in the training process. So our experiment setting is fair compared with others.\nImplementation Details. For the 2D temporal feature map encoder, we use exactly the same settings with 2DTAN (Zhang et al. 2020b) and MMN (Wang et al. 2021b) for fair comparisons. We use the VGG (Simonyan and Zisserman 2014) features for the Charades-STA dataset and C3D features (Tran et al. 2015) for the ActivityNet Captions dataset, and the number of sampled clips N is 16 for Charades-STA and 64 for ActivityNet Captions. For the text encoder, we use the HuggingFace (Wolf et al. 2019) implementation of DistilBERT (Sanh et al. 2019) with pretrained model following MMN (Wang et al. 2021b). We use AdamW (Loshchilov and Hutter 2017) optimizer with learning rate 1 \u00d7 10\u22124 and batch size 12 for Charades, learning rate 1 \u00d7 10\u22124 and batch size 20 for ActivityNet Captions. The learning rate of DistilBERT is 1/10 of our main model."
        },
        {
            "heading": "Comparison with Other Methods",
            "text": "This part compares state-of-the-art models and TRM\u2019s ability to deal with sentence-level and phrase-level prediction. On both Charades-STA and ActivityNet Captions datasets,\nwe use sentences and verb phrases (obtained from action labels used for the temporal action localization task) as queries respectively. We reproduce some of the open-source methods to test the performance of phrase-level localization. For fair comparison, all methods use C3D (Tran et al. 2015) features on ActivityNet Captions and VGG (Simonyan and Zisserman 2014) features on Charades-STA.\nAs shown in Tab. 1, TRM achieves comparable results when using completed sentences as queries and achieves an absolute advantage when using verb phrases as queries. All the existing methods we reproduced have a sheer drop when using phrases as queries. This reveals that existing models lack sufficient understanding of the intrinsic relationship between simple visual and language concepts. As shown in Tab. 2, on ActivityNet Captions, our sentence prediction is 1.92% higher than baseline MMN (IoU=0.7) and achieves comparable results with MGSL-Net.\nAs shown in Tab. 3, we test the compositional generalization of our method on ActivityNet-CG (Li et al. 2022a) dataset. VISA (Li et al. 2022a) re-splits the ActivityNet\ndatasets and constructs the ActivityNet-CG datasets. The test-trivial split has the same distribution as the training set, the novel-composition split includes unseen compositions of seen phrases, and the novel-word split includes unseen words. We achieve the best performance on all the splits, which proves that learning phrase prediction helps generalize to novel phrase compositions and novel words."
        },
        {
            "heading": "Ablation Studies",
            "text": "In this section, we conduct ablative experiments on the Charades-STA dataset to analyze the necessity of phraselevel information and phrase-level constraints.\nAs shown in Tab. 4, comparing the first and second rows, we find that simply introducing fine-grained phrase features without considering the relationship between phrase and sentence-level predictions has limited performance improvement for phrase prediction. From the third row, we see that consistency loss can greatly improve the performance of phrase prediction. From the fourth row, it can be seen that training with only exclusiveness loss has a negative impact on the model. This is because only the exclusivity loss is incomplete because the all-zero scores map of phrases is a set of trivial solutions. From the fifth row, we can see that the consistency loss and exclusiveness loss together can further improve the performance of both sentences and phrases. The results show that exploiting the consistency and exclusiveness constraints of phrase-level predictions and sentencelevel predictions can regularize the training process, thus alleviating the ambiguity of each phrase localization."
        },
        {
            "heading": "Qualitative Results",
            "text": "In Fig. 4, we visualize an example on Charades-STA Dataset. As we see, our prediction for the sentence matches the ground truth (in green) well. Also, TRM understands that the entire sentence consists of three phrases: \u2018drinking\u2019, \u2018some coffee\u2019, and \u2018walks\u2019. All the predictions satisfy\nQuery: A person walks in a doorway drinking some coffee.\nour constraints of consistency and exclusiveness. This shows TRM understands the intrinsic relationship between simple visual and language concepts."
        },
        {
            "heading": "Conclusion",
            "text": "In this work, we propose the phrase-level Temporal Relationship Mining (TRM) framework considering both phrase and sentence queries, making the first attempt to mine the phrase-proposal relation in the temporal localization task. We develop a method to constrain phrase-level prediction in training, tackling the lack of phrase-level annotation. We propose the consistency and exclusiveness constraints of phrase-level and sentence-level predictions to regularize the training process, thus alleviating the ambiguity of each phrase prediction. Experimental results on Charades-STA and ActivityNet Captions indicate that our model surpasses other models in phrase-level prediction while sentence-level results remain stable, demonstrating our model\u2019s competence, interpretability, and generalization performance."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the grants from the National Natural Science Foundation of China (61925201, 62132001, U21B2025, 62201014), Zhejiang Lab (NO. 2022NB0AB05), the National Key R&D Program of China (2021YFF0901502) and CAAI-Huawei MindSpore Open Fund."
        }
    ],
    "title": "Phrase-Level Temporal Relationship Mining for Temporal Sentence Localization",
    "year": 2023
}