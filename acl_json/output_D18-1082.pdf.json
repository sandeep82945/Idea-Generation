{
    "abstractText": "How to generate relevant and informative responses is one of the core topics in response generation area. Following the task formulation of machine translation, previous works mainly consider response generation task as a mapping from a source sentence to a target sentence. To realize this mapping, existing works tend to design intuitive but complex models. However, the relevant information existed in large dialogue corpus is mainly overlooked. In this paper, we propose Sequence to Sequence with Prototype Memory Network (S2SPMN) to exploit the relevant information provided by the large dialogue corpus to enhance response generation. Specifically, we devise two simple approaches in S2SPMN to select the relevant information (named prototypes) from the dialogue corpus. These prototypes are then saved into prototype memory network (PMN). Furthermore, a hierarchical attention mechanism is devised to extract the semantic information from the PMN to assist the response generation process. Empirical studies indicate the advantage of our model over several classical and strong baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaxin Pei"
        },
        {
            "affiliations": [],
            "name": "Chenliang Li"
        }
    ],
    "id": "SP:df337b2e7f503b1455a771e88574d4a4d6de02e5",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "CoRR, abs/1409.0473.",
            "year": 2014
        },
        {
            "authors": [
                "Hongshen Chen",
                "Zhaochun Ren",
                "Jiliang Tang",
                "Yihong Eric Zhao",
                "Dawei Yin."
            ],
            "title": "Hierarchical variational memory network for dialogue generation",
            "venue": "Proceedings of the 2018 World Wide Web Conference on World Wide Web, pages 1653\u20131662.",
            "year": 2018
        },
        {
            "authors": [
                "Shizhu He",
                "Cao Liu",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning",
            "venue": "ACL, pages 199\u2013208.",
            "year": 2017
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Xinyu Hua",
                "Lu Wang."
            ],
            "title": "Neural argument generation augmented with externally retrieved evidence",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Min-Yen Kan",
                "Xiangnan He",
                "Wenqiang Lei",
                "Xisen Jin",
                "Zhaochun Ren",
                "Dawei Yin."
            ],
            "title": "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Florian Kreyssig",
                "I\u00f1igo Casanueva",
                "Pawel Budzianowski",
                "Milica Gasic."
            ],
            "title": "Neural user simulation for corpus-based policy optimisation for spoken dialogue systems",
            "venue": "SIGDIAL Conference.",
            "year": 2018
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Yuanxin Xiang",
                "Yuwei Wang",
                "Qian Zhong",
                "Meichun Liu",
                "Min-Yen Kan."
            ],
            "title": "Linguistic properties matter for implicit discourse relation recognition: Combining semantic interaction, topic continuity and attribution",
            "venue": "AAAI.",
            "year": 2018
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "NAACL, pages 110\u2013119.",
            "year": 2016
        },
        {
            "authors": [
                "Jiwei Li",
                "Will Monroe",
                "Alan Ritter",
                "Dan Jurafsky",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Deep reinforcement learning for dialogue generation",
            "venue": "EMNLP, pages 1192\u20131202.",
            "year": 2016
        },
        {
            "authors": [
                "Jiwei Li",
                "Will Monroe",
                "Tianlin Shi",
                "S\u00e9bastien Jean",
                "Alan Ritter",
                "Dan Jurafsky."
            ],
            "title": "Adversarial learning for neural dialogue generation",
            "venue": "EMNLP, pages 2157\u20132169.",
            "year": 2017
        },
        {
            "authors": [
                "Wenjie Li",
                "Furu Wei",
                "Sujian Li",
                "Ziqiang Cao."
            ],
            "title": "Retrieve, rerank and rewrite: Soft template based neural summarization",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Qun Liu",
                "Yang Feng",
                "Hongshen Chen",
                "Zhaochun Ren",
                "Dawei Yin",
                "Shuman Liu."
            ],
            "title": "Knowledge diffusion for neural dialogue generation",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Lili Mou",
                "Yiping Song",
                "Rui Yan",
                "Ge Li",
                "Lu Zhang",
                "Zhi Jin."
            ],
            "title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation",
            "venue": "COLING, pages 3349\u20133358.",
            "year": 2016
        },
        {
            "authors": [
                "Nanyun Peng",
                "Marjan Ghazvininejad",
                "Jonathan May",
                "Kevin Knight."
            ],
            "title": "Towards controllable story generation",
            "venue": "Proceedings of the First Workshop on Storytelling, pages 43\u201349.",
            "year": 2018
        },
        {
            "authors": [
                "Minghui Qiu",
                "Feng-Lin Li",
                "Siyu Wang",
                "Xing Gao",
                "Yan Chen",
                "Weipeng Zhao",
                "Haiqing Chen",
                "Jun Huang",
                "Wei Chu."
            ],
            "title": "Alime chat: A sequence to sequence and rerank based chatbot engine",
            "venue": "ACL, pages 498\u2013503.",
            "year": 2017
        },
        {
            "authors": [
                "Alan Ritter",
                "Colin Cherry",
                "William B Dolan."
            ],
            "title": "Data-driven response generation in social media",
            "venue": "EMNLP, pages 583\u2013593.",
            "year": 2011
        },
        {
            "authors": [
                "Lifeng Shang",
                "Zhengdong Lu",
                "Hang Li."
            ],
            "title": "Neural responding machine for short-text conversation",
            "venue": "ACL, pages 1577\u20131586.",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "NIPS, pages 3104\u20133112.",
            "year": 2014
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Quoc V. Le."
            ],
            "title": "A neural conversational model",
            "venue": "CoRR, abs/1506.05869.",
            "year": 2015
        },
        {
            "authors": [
                "Lu Wang",
                "Wang Ling."
            ],
            "title": "Neural network-based abstract generation for opinions and arguments",
            "venue": "HLT-NAACL.",
            "year": 2016
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "Milica Gasic",
                "Nikola Mrksic",
                "Lina Maria Rojas-Barahona",
                "Pei hao Su",
                "Stefan Ultes",
                "David Vandyke",
                "Steve J. Young."
            ],
            "title": "A network-based end-to-end trainable task-oriented dialogue system",
            "venue": "EACL.",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "Milica Gasic",
                "Nikola Mrksic",
                "Lina Maria Rojas-Barahona",
                "Pei hao Su",
                "David Vandyke",
                "Steve J. Young."
            ],
            "title": "Multi-domain neural network language generation for spoken dialogue systems",
            "venue": "HLT-NAACL.",
            "year": 2016
        },
        {
            "authors": [
                "Yu Wu",
                "Wei Wu",
                "Dejian Yang",
                "Can Xu",
                "Zhoujun Li",
                "Ming Zhou."
            ],
            "title": "Neural response generation with dynamic vocabularies",
            "venue": "AAAI.",
            "year": 2018
        },
        {
            "authors": [
                "Chen Xing",
                "Wei Wu",
                "Yu Wu",
                "Jie Liu",
                "Yalou Huang",
                "Ming Zhou",
                "Wei-Ying Ma."
            ],
            "title": "Topic aware neural response generation",
            "venue": "AAAI, pages 3351\u2013 3357.",
            "year": 2017
        },
        {
            "authors": [
                "Jiyuan Zhang",
                "Yang Feng",
                "Dong Wang",
                "Yang Wang",
                "Andrew Abel",
                "Shiyue Zhang",
                "Andi Zhang."
            ],
            "title": "Flexible and creative chinese poetry generation using neural memory",
            "venue": "ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Jiangtong Li",
                "Pengfei Zhu",
                "Hai Zhao",
                "Gongshen Liu."
            ],
            "title": "Modeling multiturn conversation with deep utterance aggregation",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018),",
            "year": 2018
        },
        {
            "authors": [
                "Hao Zhou",
                "Minlie Huang",
                "Tianyang Zhang",
                "Xiaoyan Zhu",
                "Bing Liu."
            ],
            "title": "Emotional chatting machine: emotional conversation generation with internal and external memory",
            "venue": "arXiv preprint arXiv:1704.01074.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 745\u2013750 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n745"
        },
        {
            "heading": "1 Introduction",
            "text": "Dialogue systems, or say, chatbots are usually considered as the future of human-computer interaction and extensive works have been done in this area (Wen et al., 2016; Qiu et al., 2017; Wen et al., 2017; Kreyssig et al., 2018).\nAs one of the main approaches for dialogue system design, response generation has attracted more and more attention from research community. Neural networks based models like Seq2Seq architecture (Vinyals and Le, 2015; Shang et al., 2015) are proven to be effective to generate valid responses for a dialogue system. However, as revealed in many previous works (Li et al., 2016a;\n\u2217*Chenliang Li is the Corresponding Author\nWu et al., 2018), \u201dsafe reply\u201d is still an open problem and lots of efforts are made to generate more informative responses (Li et al., 2016a; Mou et al., 2016; Li et al., 2016b; Qiu et al., 2017; Li et al., 2017; Zhao et al.; He et al., 2017; Zhou et al., 2017; Liu et al., 2018; Chen et al., 2018).\nNote that in this paper when we say response generation, we focus on single turn chit-chat for that other tasks like multi-turn (Zhang et al., 2018) or goal-oriented (Kan et al., 2018)generation could be partly considered as the extensions of single-turn generation.\nThough existing works mentioned above are helpful in some ways, they all follow the task formulation proposed by (Ritter et al., 2011), which considers response generation (RG) task as a mapping from a source sentence to a target sentence like machine translation (MT). This task formulation ignores the natural difference between MT and RG: MT deals with sentence pairs of the same meanings while RG needs to realize the meaning transformation from a source post to the target response. In this sense, the meaning transformation is more difficult than machine translation. Hence, many researchers have designed more and more complex models. However, given a target post, the relevant information covered by the dialogue corpus is usually overlooked. It is intuitive that the responses for a similar post would provide more contextual information to guide the response generation. To this end, we are interested in exploiting the relevant responses in the training set as soft prototypes to assist the response generation.\nSpecifically, in this paper, we propose Sequence to Sequence with Prototype Memory Network (named S2SPMN). We introduce two Prototype Memory Networks (PMNs) to store the relevant responses extracted from the dialogue corpus: static PMN and dynamic PMN. Tested on a widely used benchmark dataset, the proposed\nPage 1 of 1\n8/28/2018file:///C:/Users/Li%20Jing/Desktop/PMN3.svg\nS2SPMN produces more informative responses than the standard and strong baselines. To the best of our knowledge, it is the first work leveraging prototype information in dialogue corpus in response generation area.\nThe contributions of this paper could be summarized as follows:\n(1) We propose S2SPMN, a simple yet effective response generation model which could leverage relevant information in dialogue corpus to assist response generation.\n(2) Empirical studies indicate the superiority of proposed S2SPMN over other methods."
        },
        {
            "heading": "2 Architecture",
            "text": ""
        },
        {
            "heading": "2.1 Problem Definition",
            "text": "Given a dialogue dataset \u0393 = {Xi, Yi}Ni=1, where Yi is the response for a post Xi, we aim to train a model with \u0393 such that the model can generate an accurate and informative response for a new post X \u2032. Here, we propose to exploit the relevant information provided by \u0393. Let T \u2032 = (r1, r2, ..., rm) refers to the prototype memory network constructed for post X \u2032, where ri is the i-th relevant response (named prototype) extracted from dialogue dataset \u0393. The goal is to derive the model to generate the response Y \u2032: p(Y \u2032|X \u2032) = p(Y \u2032|T \u2032, X \u2032).\nIn following sections, we firstly introduce the generation framework with hierarchical attention mechanism assuming PMN is constructed. Then we will introduce two kinds of PMNs: static PMN and dynamic PMN."
        },
        {
            "heading": "2.2 Sequence-to-Sequence with Prototype Memory Network",
            "text": "S2SPMN is built with a Seq2Seq encoder-decoder framework (Sutskever et al., 2014) with the attention mechanism (Bahdanau et al., 2014). We use\nLSTM (Hochreiter and Schmidhuber, 1997) to materialize both encoder and decoder. The hidden state at t-th encoding step is generated from previous hidden state ht\u22121 and current input xt as follows:\nht = lstm(xt, ht\u22121) (1)\nFor decoder, at i-th timestep, si is the decoder\u2019s hidden state and pi is the probability distribution of candidate words .\nsi = lstm(yi\u22121, si\u22121, ci, oi) (2)\npi = softmax(MLP (si, yt\u22121, oi, ci)) (3)\nwhere MLP () is a one-layer perception, oi is the hierarchical attention over entire prototype memory network which will be formalized in following sections. ci is the summarization for the post regarding to the hidden state si\u22121:\nci = T\u2211\nj=1 \u03b1ijhj , \u03b1ij = exp(eij)\u2211T k=1 exp(eik) (4)\neij = v T 1MLP (si\u22121, hj) (5)\nwhere v1 is the attention parameter."
        },
        {
            "heading": "2.3 Prototype Memory Network",
            "text": "Given a post X \u2032, a set of responses are selected from training set as prototypes and are then saved into the Prototype Memory Network(PMN). We propose two kinds of Prototype Memory Networks.\nStatic PMN: For static PMN(SPMN), we randomly select m responses before training starts and the entire PMN remains unchanged during the training process. That is, we use the same prototypes for all the post-response pairs.\nDynamic PMN: In dynamic PMN(DPMN), prototypes are selected by retrieving the most relevant posts. We calculate the cosine similarity with TF-IDF weighting scheme between the given post and all the posts in training set. We consider top-m posts and put the associated responses into DPMN. This means that the prototypes are characteristic for each post-response pair.\nIn both SPMN and DPMN, m is a predefined hyper-parameter controlling the size of the PMN. Each prototype is represented with the concatenation of word embeddings. We perform zero padding for both SPMN and DPMN with a pseudo\nword1, making the length for the representation of each prototype be the same. Here we denote the prototype memory network as PMN = {r1, r2, ..., rm}, in which rm is the representation of m-th prototype and m is the size of the PMN. And rm = {wm,1, wm,2, ..., wm,l} where wm,i is the embedding of i-th word, and l is the maximum allowable length for a prototype.\nFor both SPMN and DPMN, we select responses rather than posts although sometimes they have similar vocabularies and syntactic structure. We believe that using responses as prototypes could help with the meaning transformation from post to response. In DPMN, all the retrieved prototypes could be considered as responses to the target post. It is intuitive that the generated response would have similar representation to these prototypes."
        },
        {
            "heading": "2.4 Hierarchical Attention Mechanism",
            "text": "We use a two-stage hierarchical attention mechanism to extract useful information in PMN and integrate it into the decoding process. The first stage is a sentence level attention over entire PMN to generate the abstractive prototype r\u0302i at each timestep:\nr\u0302i = M\u2211 j=1 \u03b2ijrj , \u03b2ij = exp(fij)\u2211M k=1 exp(fik) (6)\nfij = v T 2MLP (si\u22121, rj) (7)\nwhere v2 is the attention parameter. The second stage is a word level attention oi over the generated r\u0302i = {w\u03021, w\u03022, ..., w\u0302l} and is calculated as follows:\noi = l\u2211\nj=1 \u03b3ijw\u0302j , \u03b3ij = exp(gij)\u2211l k=1 exp(gik) (8)\ngij = v T 3MLP (si\u22121, w\u0302j) (9)\nwhere v3 is the attention parameter."
        },
        {
            "heading": "3 Experiment",
            "text": ""
        },
        {
            "heading": "3.1 Experiment Setup",
            "text": "We use a subset of STC dataset (Shang et al., 2015) crawled from Weibo, the largest social media in China. The vocabulary size is set to be 8, 000 for computational efficiency and words out of vocabulary are replaced by the symbol \u201dunk\u201d.\n1The embedding of the pseudo word is a zero vector.\nWe remove sentences longer than 25 words or containing more than 2 unk symbols. After preprocessing step, we have 315, 980 post-response pairs in training set, 3, 510 pairs in validation set and 300 in test set.\nIn our model, we use one-layer LSTM and the hidden size is set to be 600 in both encoder and decoder. For all the words used in our model, the embedding size is 300. Mini-batch learning is used and batch size is set as 64. We use simple SGD for optimization and the initial learning rate is set to be 0.2."
        },
        {
            "heading": "3.2 Evaluation Metrics",
            "text": "We use two automatic evaluation metrics including Perplexity and Distinct. Human evaluation is also conducted as the only gold standard for response generation is human judgement.\nPerplexity: Following (Vinyals and Le, 2015) and (Xing et al., 2017), we use perplexity as one of our automatic evaluation metrics. Perplexity could measure the holistic condition of model learning. A lower perplexity score indicates better generalization performance. Perplexity on both validation set (PPL-V) and test set (PPL-T) are presented in table 2.\nDistinct-1, Distinct-2: Distinct-1 and distinct2 calculate the ratios of distinct unigrams and bigrams in the generated responses respectively (Li et al., 2016a; Xing et al., 2017; Wu et al., 2018). The higher score suggests that the generated response is more diverse and informative. Here, we report the distinct-1 and distinct-2 scores on entire test set.\nHuman Annatation: We further recruit human annotators to judge the quality of the generated answers for all the qa-pairs in test set. Responses generated by all the methods are pooled and randomly shuffled for each annotator. A score between 0 and 2 is assigned to each generated answer based on the following criteria:\n+2: the answer is natural and relevant to the question.\n+1: the answer can be used as a reply, but is not informative enough (e.g. \u201c\u6211\u4e5f\u662f\u201d (me too), \u201c\u4e0d \u77e5\u9053\u201d (I don\u2019t know)).\n+0: the answer is irrelevant and unclear in meaning (e.g. too many grammatical errors to understand)."
        },
        {
            "heading": "3.3 Results Comparation",
            "text": "We use a standard baseline and a strong baseline for comparison.\nS2SA: The standard Seq2Seq model with an attention mechanism (Vinyals and Le, 2015).\nTAS2S: One of the existing state-of-the-art neural models based on Seq2Seq architecture. The topical words relevant to the post are considered via an attention mechanism when decoding (Xing et al., 2017).\nAs for our models, we use SPMN to denote the generating method with static prototype memory networks and DPMN with dynamic prototype memory networks. The numbers following model names are the size of PMN.\nAutomatic Evaluation: Table 1 shows the automatic evaluation results. We see that both SPMN and DPMN obtain huge improvements over the two baselines in terms of PPL-V and PPL-T. Also, we observe that SPMN1000 outperforms SPMN500 in all the four automatic metrics. Note that each post has the same prototypes provided by SPMN. This is reasonable that the relevant response is more likely to be covered by storing more prototypes in SPMN. As for the DPMN, we can see that DPMN achieves the best performance with only 100 prototypes in terms of PPL-T, compared with the other 4 methods. This suggests that using a retrieval mechanism to incorporate the relevant responses brings more useful information for better response generation. Note that S2SA outperforms the others in terms of distinct-1 and distinct-2. Further human evaluation indicates that many responses generated by S2SA are irrelevant and meaningless, which could inevitably increase the distinct scores.\nHuman Annotation: Table 2 shows human annotation results. It is clear that our models (SPMN500, SPMN1000, DPMN100) generate much more informative and valid responses and much less meaningless or \u201csafe\u201d responses than baseline models (S2SA, TAS2S). Specifically, SPMN500, SPMN1000 and DPMN100 all\noutperform S2SA and TAS2S by producing more informative and valid responses. Also, we can find that DPMN still outperforms SPMN500 and SPMN1000 with only 100 relevant responses, which is consistent with the observation made in automatic evaluation (in terms of PPL-V and PPLT).\nCase Study Table 3 shows several cases generated by different models. Note that the size of training set and vocabulary used in our experiments are relatively small compared to millions of qa-pairs used in other works (Xing et al., 2017; Wu et al., 2018), so it\u2019s reasonable that bad cases sometimes occur in results of baselines. However, our models, no matter the static one or the dynamic one, could generate amazing responses which are not only grammatical and informative, but also have some emotional expressions like the use of punctuation and repetition."
        },
        {
            "heading": "4 Related Work",
            "text": ""
        },
        {
            "heading": "4.1 Natural language generation",
            "text": "How to generate grammatical and interesting sentences in different situations is one of the core topics in natural language processing area. Extensive works are proposed to generate poems (Zhang et al., 2017), abstracts (Wang and Ling, 2016), arguments (Hua and Wang, 2018), stories (Peng et al., 2018) and so on. Although existing approaches are useful in some ways, it\u2019s still difficult to generate natural sentences from scratch and integrating retrieved results has recently become a new fashion in this area. Hua and Wang (2018) proposed an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Li et al. (2018) devised a RetrieveRerank-Rewrite model for abstractive summarization which uses retrieved results as soft template to assist the decoding process."
        },
        {
            "heading": "4.2 Response generation",
            "text": "Hand-craft rules, retrieval and generation are three main solutions for conversational AI and generation is the most interesting one in current research community. Li et al. (2016a; 2016b; 2017) proposed a series of works in solving the \u201dsafe reply\u201d problem using different approaches like redefining the objective function or leveraging GAN. Xing et al. (2017) considered topic coherence issue by incorporating topical words. Dynamically restricting the target vocabulary is also an interesting idea and Wu et al. (2018) proposed to filter irrelevant words while achieving better computational efficiency . He et al. (2017) introduced copy mechanism to simulate people\u2019s behaviors in real conversations and the proposed model could copy useful words from source sentences. Zhou et al. (2017) indicated that emotion is quite important in real dialogues thus an emotional chatting machine was devised to generate emotional responses. Liu et al. (2018) proposed a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we propose S2SPMN, a simple yet effective response generation model by exploiting relevant information contained in large dialogue dataset. Empirical studies indicate that simply selecting responses from training set as prototypes and integrating them into the generation process could dramatically improve the quality of generated responses. Moreover, our model is very flex-\nible and could be adapted to any other Seq2Seq based generation methods. Most importantly, we claim the intrinsic difference between RG and MT and propose a new way to define response generation.\nAs the first work trying to help with the meaning transformation between source and target, we have obtained the encouraging progress. However, we know that there are still many directions to enrich the proposed framework. In future work, we would like to devise more sophisticated solutions to bridge the semantic gap in RG and explore linguistic patterns in conversations like what has been done in discourse analysis (Lei et al., 2018) ."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was supported by National Natural Science Foundation of China (No. 61502344, No. 61872278), Natural Scientific Research Program of Wuhan University (No. 2042017kf0225). Chenliang Li is the corresponding author."
        }
    ],
    "title": "S2SPMN:A Simple and Effective Framework for Response Generation with Relevant Information",
    "year": 2018
}