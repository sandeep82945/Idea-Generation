{
    "abstractText": "Beam search optimization (Wiseman and Rush, 2016) resolves many issues in neural machine translation. However, this method lacks principled stopping criteria and does not learn how to stop during training, and the model naturally prefers longer hypotheses during the testing time in practice since they use the raw score instead of the probability-based score. We propose a novel ranking method which enables an optimal beam search stopping criteria. We further introduce a structured prediction loss function which penalizes suboptimal finished candidates produced by beam search during training. Experiments of neural machine translation on both synthetic data and real languages (German\u2192English and Chinese\u2192English) demonstrate our proposed methods lead to better length and BLEU score.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mingbo Ma"
        },
        {
            "affiliations": [],
            "name": "Renjie Zheng"
        },
        {
            "affiliations": [],
            "name": "Liang Huang"
        }
    ],
    "id": "SP:516ebf91520725b85dca68e3b9848b01b902cb84",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473.",
            "year": 2014
        },
        {
            "authors": [
                "Samy Bengio",
                "Oriol Vinyals",
                "Navdeep Jaitly",
                "Noam Shazeer."
            ],
            "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
            "venue": "NIPS.",
            "year": 2015
        },
        {
            "authors": [
                "Samy Bengio",
                "Oriol Vinyals",
                "Navdeep Jaitly",
                "Noam Shazeer."
            ],
            "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
            "venue": "Advances in Neural Information Processing Systems, pages 1171\u20131179.",
            "year": 2015
        },
        {
            "authors": [
                "Mauro Cettolo",
                "Jan Niehues",
                "Sebastian St\u00fcker",
                "Luisa Bentivogli",
                "Marcello Federico."
            ],
            "title": "Report on the 11th iwslt evaluation campaign, iwslt 2014",
            "venue": "Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam.",
            "year": 2014
        },
        {
            "authors": [
                "John Duchi",
                "Elad Hazan",
                "Yoram Singer."
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of Machine Learning Research, 12(Jul):2121\u20132159.",
            "year": 2011
        },
        {
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Denis Yarats",
                "Yann N. Dauphin."
            ],
            "title": "Convolutional sequence to sequence learning",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Aus-",
            "year": 2017
        },
        {
            "authors": [
                "Liang Huang",
                "Kai Zhao",
                "Mingbo Ma."
            ],
            "title": "When to finish? optimal beam search for neural text generation (modulo beam size)",
            "venue": "EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "G. Klein",
                "Y. Kim",
                "Y. Deng",
                "J. Senellart",
                "A.M. Rush."
            ],
            "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
            "venue": "ArXiv e-prints.",
            "year": 2017
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando Pereira."
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "Proceedings of ICML.",
            "year": 2001
        },
        {
            "authors": [
                "Mingbo Ma."
            ],
            "title": "Structured neural models for natural language processing",
            "venue": "Ph.D thesis, Oregon State University.",
            "year": 2018
        },
        {
            "authors": [
                "Mingbo Ma",
                "Liang Huang",
                "Bing Xiang",
                "Bowen Zhou."
            ],
            "title": "Group sparse cnns for question classification with answer sets",
            "venue": "ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Mingbo Ma",
                "Liang Huang",
                "Hao Xiong",
                "Kaibo Liu",
                "Chuanqiang Zhang",
                "Zhongjun He",
                "Hairong Liu",
                "Xing Li",
                "Haifeng Wang."
            ],
            "title": "Stacl: Simultaneous translation with integrated anticipation and controllable latency",
            "venue": "ArXiv, abs/1810.08398.",
            "year": 2018
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks. ICLR",
            "year": 2016
        },
        {
            "authors": [
                "Steven J Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel."
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7008\u20137024.",
            "year": 2017
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "arXiv preprint arXiv:1508.07909.",
            "year": 2015
        },
        {
            "authors": [
                "Shiqi Shen",
                "Yong Cheng",
                "Zhongjun He",
                "Wei He",
                "Hua Wu",
                "Maosong Sun",
                "Yang Liu."
            ],
            "title": "Minimum risk training for neural machine translation",
            "venue": "Proceedings of ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in neural information processing systems, pages 3104\u20133112.",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30.",
            "year": 2017
        },
        {
            "authors": [
                "Arun Venkatraman",
                "Martial Hebert",
                "J. Andrew Bagnell."
            ],
            "title": "Improving multi-step prediction of learned time series models",
            "venue": "AAAI.",
            "year": 2015
        },
        {
            "authors": [
                "Sam Wiseman",
                "Alexander M Rush."
            ],
            "title": "Sequence-to-sequence learning as beam-search optimization",
            "venue": "Proceedings of EMNLP.",
            "year": 2016
        },
        {
            "authors": [
                "Yilin Yang",
                "Liang Huang",
                "Mingbo Ma."
            ],
            "title": "Breaking the beam search curse: A study of (re-) scoring methods and stopping criteria for neural machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
            "year": 2018
        },
        {
            "authors": [
                "Renjie Zheng",
                "Mingbo Ma",
                "Liang Huang."
            ],
            "title": "Multi-reference training with pseudo-references for neural translation and text generation",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Renjie Zheng",
                "Yilin Yang",
                "Mingbo Ma",
                "Liang Huang."
            ],
            "title": "Ensemble sequence level training for multimodal mt: Osu-baidu wmt18 multimodal machine translation system report",
            "venue": "WMT.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Proceedings of NAACL-HLT 2019, pages 1884\u20131889 Minneapolis, Minnesota, June 2 - June 7, 2019. c\u00a92019 Association for Computational Linguistics\n1884"
        },
        {
            "heading": "1 Introduction",
            "text": "Sequence-to-sequence (seq2seq) models based on RNNs (Sutskever et al., 2014; Bahdanau et al., 2014), CNNs (Gehring et al., 2017) and selfattention (Vaswani et al., 2017) have achieved great successes in Neural Machine Translation (NMT). The above family of models encode the source sentence and predict the next word in an autoregressive fashion at each decoding time step. The classical \u201ccross-entropy\u201d training objective of seq2seq models is to maximize the likelihood of each word in the translation reference given the source sentence and all previous words in that reference. This word-level loss ensures efficient and scalable training of seq2seq models.\nHowever, this word-level training objective suffers from a few crucial limitations, namely the label bias, the exposure bias, and the loss-evaluation mismatch (Lafferty et al., 2001; Bengio et al.,\n\u2217 Equal contribution\n2015a; Venkatraman et al., 2015). In addition, more importantly, at decoding time, beam search is universally adopted to improve the search quality, while training is fundamentally local and greedy. Several researchers have proposed different approaches to alleviate above problems, such as reinforcement learning-based methods (Ranzato et al., 2016; Rennie et al., 2017; Zheng et al., 2018b), training with alternative references (Shen et al., 2016; Zheng et al., 2018a). Recently, Wiseman and Rush (2016) attempt to address these issues with a structured training method, Beam Search Optimization (BSO). While BSO outperforms other proposed methods on German-toEnglish translation, it also brings a different set of problems as partially discussed in (Ma, 2018) which we present with details below.\n1There are two types of \u201clength ratios\u201d in this paper: (a) target to reference ratio (|y|/|y\u2217|), which is used in BLEU, and (b) target to source ratio (|y|/|x|). By default, the term \u201clength ratio\u201d in this paper refers to the former.\nBSO relies on unnormalized raw scores instead of locally-normalized probabilities to get rid of the label bias problem. However, since the raw score can be either positive or negative, the optimal stopping criteria (Huang et al., 2017) no longer holds, e.g., one extra decoding step would increase the entire unfinished hypothesis\u2019s model score when we have positive word score. This leads to two consequences: we do not know when to stop the beam search and it could return overlength translations (Fig. 1) or underlength translations (Fig. 3) in practice. As shown in Fig. 1, the BLEU score of BSO drops significantly when beam size gets larger as a result of overlong translations (as evidenced by length ratios larger than 1). Furthermore, BSO performs poorly (shown in Section 4) on hard translation pairs, e.g., Chinese\u2192English (Zh\u2192En) translation, when the target / source ratio is more diverse (Table 1).\nTo overcome the above issues, we propose to use the sigmoid function instead of the raw score at each time step to rank candidates. In this way, the model still has probability properties to hold optimal stopping criteria without label bias effects. Moreover, we also encourage the model to generate the hypothesis which is more similar to gold reference in length. Compared with length rewardbased methods (Huang et al., 2017; Yang et al., 2018), our model does not need to tune the predicted length and per-word reward. Experiments on both synthetic and real language translations (De\u2192En and Zh\u2192En) demonstrate significant improvements in BLEU score over strong baselines and other methods."
        },
        {
            "heading": "2 Preliminaries: NMT and BSO",
            "text": "Here we briefly review the conventional NMT and BSO (Wiseman and Rush, 2016) to set up the notations. For simplicity, we choose to use RNNbased model but our methods can be easily applied to other designs of seq2seq model as well.\nRegardless of the particular design of different seq2seq models, generally speaking, the decoder always has the following form:\np(y | x) = \u220f|y|\nt=1 p(yt | x, y<t) (1)\nwhere x \u2208 RN\u00d7D represents the D-dimension hidden states from encoder with N words and y<t denotes the gold prefix (y1, ..., y(t\u22121)) before t. The conventional NMT model is locally trained to maximize the above probability.\nInstead of maximizing each gold word\u2019s probability, BSO tries to promote the non-probabilistic scores of gold sequence within a certain beam size b. BSO removes the softmax layer and directly uses the raw score after hidden-to-vocabulary layer, and the non-probabilistic scoring function fx(yt | y<t) represents the score of word yt given gold prefix y<t and x. Similarly, fx(y\u0302bt | y\u0302b<t) is the bth sequence with beam size b at time step t. Then, we have the following loss function to penalize the bth candidate and promote gold sequence:\nL = |y|\u2211 t=1 \u2206(y\u0302b\u2264t)(1 + fx(y\u0302 b t | y\u0302b<t)\u2212 fx(yt | y<t))+ (2)\nwhere \u2206(y\u0302b\u2264t) is defined as (1\u2212BLEU(y\u0302b\u2264t,y\u2264t)) which scales the loss according to BLEU score between gold and bth hypothesis in the beam. The notation (\u00b7)+ represents a max function between any value and 0, i.e., z+ = max(0, z).\nWhen Eq. 1 equals to 0 at time step t, then the gold sequence\u2019s score is higher than the last hypothesis in the beam by 1, and a positive number otherwise. Finally, at the end of beam search (t = |y|), BSO requires the score of y exceed the score of the highest incorrect hypothesis by 1.\nNote that the above non-probabilistic score function fx(\u00b7) is not bounded as probabilistic score in conventional NMT. In practice, when we have positive word score, then the unfinished candidates always get higher model scores with one extra decoding step and the optimal stopping criteria 2 (Huang et al., 2017) is no longer hold. BSO implements a similar \u201cshrinking beam\u201d strategy which duplicates top unfinished candidate to replace finished hypotheses and terminates the beam search when there are only </eos> in the beam. Non-probabilistic score function works well in parsing and Statical MT where we know when to stop beam search. However, in the NMT scenario, without optimal stopping criteria, we don\u2019t know when to stop beam search."
        },
        {
            "heading": "3 Learning to Stop",
            "text": "We propose two major improvements to BSO."
        },
        {
            "heading": "3.1 Sigmoid Scoring Function",
            "text": "As mentioned in Section 2, BSO relies on raw score function to eliminate label bias effects.\n2Beam search stops when the score of the top unfinished hypothesis is lower than any finished hypothesis, or the </eos> is the highest score candidate in the beam.\nHowever, without using locally-normalized score does not mean that we should stop using the probabilistic value function. Similar with multi-label classification in (Ma et al., 2017), instead of using locally normalized softmax-based score and non-probabilistic raw scores, we propose to use another form of probabilistic scoring function, sigmoid function, which is defined as follows:\ngx(yt | y<t) = (1 + ew\u00b7fx(yt|y<t))\u22121 (3)\nwhere w is a trainable scalar parameter which shifts the return value of fx(yt | y<t) into a nonsaturated value region of sigmoid function. Eq. 3 measures the probability of each word independently which is different from locally-normalized softmax function. Similar to the scenario in multilabel classification, gx(yt | y<t) only promotes the words which are preferred by gold reference and does not degrade other words. Eq. 3 enables the model to keep the probability nature of scoring function without introducing label bias effects. After the model regain probability-based scoring function, the optimal stopping criteria can be used in testing time decoding."
        },
        {
            "heading": "3.2 Early Stopping Penalties",
            "text": "Similar to Eq. 1, testing time decoder multiplies the new word\u2019s probabilistic score with prefix\u2019s score when there is a new word appends to an unfinished hypothesis. Though the new word\u2019s probabilistic score is upper bounded by 1, in practice, the score usually far less than one. As described in (Huang et al., 2017; Yang et al., 2018), decoder always prefers short sentence when we use the probabilistic score function.\nTo overcome the above so-called \u201cbeam search curse\u201d, we propose to penalize early-stopped hypothesis within the beam during training. The procedure during training is illustrated in Fig. 2.\nDifferent from BSO, to penalize the underlength finished translation hypotheses, we include additional violations when there is an </eos> within the beam before the gold reference finishes and we force the score of that </eos> lower than the b + 1 candidate by a margin. This underlength translation violation is formally defined as follows:\nLs = |y|\u2211 t=1 b\u2211 j=1 1(y\u0302jt = </eos>) \u00b7Q(y\u0302 j t , y\u0302 b+1 t ) , Q(y\u0302jt , y\u0302 b+1 t ) = (1 + fx(y\u0302 j t | y\u0302 j <t)\u2212 fx(y\u0302 b+1 t | y\u0302 b+1 <t )) + (4)\nwhere notation 1 is identification function which only equals to 1 when ith candidate in beam y\u0302jt is </eos>, e.g. in Fig. 2. We only have non-zero loss when the model score of underlength translation candidates are greater than the b+ 1 candidate by a margin. In this way, we penalize all the short hypotheses during training time. Note that during both training and testing time, the decoder stops beam search when it satisfies the optimal stopping criteria (Huang et al., 2017). Therefore, we do not need to penalize the overlength translations since we have already promoted the gold reference to the top of the beam at time step |y| during training."
        },
        {
            "heading": "4 Experiments",
            "text": "We showcase the performance comparisons over three different datasets. We implement seq2seq model, BSO and our proposed model based on PyTorch-based OpenNMT (Klein et al., 2017). We use a two-layer bidirectional LSTM as the encoder and a two layer LSTM as the decoder. We train Seq2seq model for 20 epochs to minimize perplexity on the training dataset, with a batch size of 64, word embedding size of 512, the learning rate of 0.1, learning rate decay of 0.5 and dropout rate of 0.2. Following Wiseman and Rush (2016), we then train BSO and our model based on the previous Seq2seq model with the learning rate of\n0.01 and learning rate decay of 0.75, batch size of 40. Note that our pretrained model is softmaxbased, and we only replace the softmax layer with the sigmoid layer for later training for simplicity. The performance will have another boost when our pretrained model is sigmoid-based. We use Adagrad (Duchi et al., 2011) as the optimizer.\nIn Zh\u2192En task, we employ BPE (Sennrich et al., 2015) which reduces the source and target language vocabulary sizes to 18k and 10k. Following BSO, we set the decoding beam size smaller than the training beam size by 1."
        },
        {
            "heading": "4.1 Synthetic Task",
            "text": "Table 1 shows the statistics of source sentence length and the ratio between target and source sentences. The synthetic dataset is a simple translation task which generates target sentences from this grammar: {a \u2192 x, b \u2192 x x, c \u2192 x x x, d \u2192 x x x x, e\u2192 x x x x x}. For example:\n1. source sentence [b c a] will generate the target sentence [x x x x x x] (2 x from b, 3 x from c and 1 x from a).\n2. source sentence [a, b, c, d, e] will be translated into [x x x x x x x x x x x x x x x] in target side (1 x from a, 2 x from b, 3 x from c, 4 x from d and 5 x from e).\nThis dataset is designed to evaluate the length prediction ability of different models. Fig. 3 shows the length ratio of different models on the test set. Only our model can predict target sentence length correctly with all beam sizes which shows a better ability to learn target length."
        },
        {
            "heading": "4.2 De\u2192En Translation",
            "text": "The De\u2192En dataset is previously used in BSO and MIXER (Ranzato et al., 2016), which is from IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014) 3.\n3The test set of De\u2192En involves some mismatched source-reference pairs. We have cleaned this test set and report the statistics based on the cleaned version.\nTable 2 shows the BLEU score and length ratio of different models on dev-set. Similar to seq2seq, our proposed model achieves better BLEU score with larger beam size and outperforms the best BSO b = 4 model with 0.76 BLEU. The ablation study in Table 3 shows that the model produces shorter sentence without scale augment (term \u2206(y\u0302b\u2264t) in Eq. 2) and early stopping loss. The model also performs worse when replacing softmax to sigmoid because of the label bias problem. Fig. 4 shows BLEU score and length ratio of BSO and our models trained with beam size b = 6 with different decoding beam size. Compared with BSO, whose BLEU score degrades dramatically when increasing beam size, our model performs much more stable. Moreover, BSO achieves much better BLEU score with decoding beam b = 3 while trained with b = 6 because of a better\nlength ratio, this is inconsistent with their claim that decoding beam size should smaller than training beam size by 1.\nTable 4 shows better accuracy of our proposed model than not only published test results of BSO (Wiseman and Rush, 2016), DAD (Bengio et al., 2015b) and MIXER (Ranzato et al., 2016), but also our implemented seq2seq and BSO model."
        },
        {
            "heading": "4.3 Zh\u2192En Translation",
            "text": "We also perform experiments on NIST Zh\u2192En\ntranslation dataset. We use the NIST 06 and 08 dataset with 4 references as the validation and test set respectively. Table 1 shows that the characteristic of Zh\u2192En translation is very different from De\u2192En in source length and variance in target/source length ratio.\nWe compare our model with seq2seq, BSO and seq2seq with length reward (Huang et al., 2017) which involves hyper-parameter to solve neural model\u2019s tendency for shorter hypotheses (our proposed method does not require tuning of hyperparameter). Fig. 5 shows that BSO prefers overlength hypotheses in short source sentences and underlength hypotheses when the source sentences are long. This phenomenon degrades the BLEU score in dev-set from Table 5. Our proposed model comparatively achieves better length ratio on almost all source sentence length in dev-set."
        },
        {
            "heading": "5 Future Works and Conclusions",
            "text": "Our proposed methods are general techniques which also can be applied to the Transformer (Vaswani et al., 2017). As part of our future works, we plan to adapt our techniques to the Transformer to further evaluate our model\u2019s performance.\nThere are some scenarios that decoding time beam search is not applicable, such as the simultaneous translation system proposed by Ma et al. (2018) which does not allow for adjusting the committed words, the training time beam search still will be helpful to the greedy decoding performance. We plan to further investigate the performance of testing time greedy decoding with beam search optimization during training.\nWe propose two modifications to BSO to provide better scoring function and under-translation penalties, which improves the accuracy in De-En and Zh-En by 0.8 and 3.7 in BLEU respectively."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported in part by DARPA grant N66001-17-2-4030, and NSF grants IIS-1817231 and IIS-1656051."
        }
    ],
    "title": "Learning to Stop in Structured Prediction for Neural Machine Translation",
    "year": 2019
}