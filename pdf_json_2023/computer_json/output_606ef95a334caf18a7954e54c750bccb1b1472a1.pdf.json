{
    "abstractText": "Decoy-based methods are a popular choice for the statistical validation of peptide detections in tandem mass spectrometry proteomics data. Such methods can achieve a substantial boost in statistical power when coupled with post-processors such as Percolator that use auxiliary features to learn a better-discriminating scoring function. However, we recently showed that Percolator can struggle to control the false discovery rate (FDR) when reporting the list of discovered peptides. To address this problem, we developed a general protocol called \u201cRESET\u201d that is used to determine the list of reported discoveries in a target-decoy competition setup, where each putative discovery is augmented with a list of relevant features. The key idea is that, rather than using the entire set of decoys wins for both training and estimating, a random subset of decoys is chosen for each task. One subset is then used to train a semi-supervised machine learning model, and the other is used for controlling the FDR. We show that, by applying RESET to the Percolator algorithm, we control the FDR both theoretically and empirically, while reporting only a marginally smaller number of discoveries than Percolator. Furthermore, a variant of the protocol that we introduce, which requires generating two decoys per target, maintains theoretical and empirical FDR control, typically reports slightly more discoveries than Percolator, and exhibits less variability than the single-decoy approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jack Freestone"
        },
        {
            "affiliations": [],
            "name": "Lukas K\u00e4ll"
        },
        {
            "affiliations": [],
            "name": "William Stafford Noble"
        },
        {
            "affiliations": [],
            "name": "Uri Keich"
        }
    ],
    "id": "SP:5e6afb54fc54087e10026f359c2d45f73aae23b7",
    "references": [
        {
            "authors": [
                "R.F. Barber",
                "Emmanuel J. Cand\u00e8s"
            ],
            "title": "Controlling the false discovery rate via knockoffs",
            "venue": "The Annals of Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Yulia Danilova",
                "Anastasia Voronkova",
                "Pavel Sulimov",
                "Attila Kert\u00e9sz-Farkas"
            ],
            "title": "Bias in false discovery rate estimation in mass-spectrometry-based peptide identification",
            "venue": "Journal of proteome research,",
            "year": 2019
        },
        {
            "authors": [
                "B. Diament",
                "W.S. Noble"
            ],
            "title": "Faster SEQUEST searching for peptide identification from tandem mass spectra",
            "venue": "Journal of Proteome Research,",
            "year": 2011
        },
        {
            "authors": [
                "J.E. Elias",
                "S.P. Gygi"
            ],
            "title": "Target-decoy search strategy for increased confidence in large-scale protein identifications by mass spectrometry",
            "venue": "Nature Methods,",
            "year": 2007
        },
        {
            "authors": [
                "K. Emery",
                "S. Hasam",
                "W.S. Noble",
                "U. Keich"
            ],
            "title": "Multiple competition-based fdr control and its application to peptide detection",
            "venue": "In International Conference on Research in Computational Molecular Biology,",
            "year": 2020
        },
        {
            "authors": [
                "J. Freestone",
                "W.S. Noble",
                "U. Keich"
            ],
            "title": "Analysis of tandem mass spectrometry data with CONGA: Combining open and narrow searches with group-wise analysis",
            "year": 2023
        },
        {
            "authors": [
                "J. Freestone",
                "W.S. Noble",
                "U. Keich"
            ],
            "title": "Re-investigating the correctness of decoy-based false discovery rate control in proteomics tandem mass spectrometry",
            "venue": "In preparation,",
            "year": 2023
        },
        {
            "authors": [
                "J. Freestone",
                "T. Short",
                "W.S. Noble",
                "U. Keich"
            ],
            "title": "Group-walk: a rigorous approach to groupwise false discovery rate analysis by target-decoy competition",
            "venue": "Bioinformatics, 38(Supplement 2):ii82\u2013ii88,",
            "year": 2022
        },
        {
            "authors": [
                "V. Granholm",
                "J.F. Navarro",
                "W.S. Noble",
                "L. K\u00e4ll"
            ],
            "title": "Determining the calibration of confidence estimation procedures for unique peptides in shotgun proteomics",
            "venue": "Journal of Proteomics,",
            "year": 2013
        },
        {
            "authors": [
                "V. Granholm",
                "W.S. Noble",
                "L. K\u00e4ll"
            ],
            "title": "On using samples of known protein content to assess the statistical calibration of scores assigned to peptide-spectrum matches in shotgun proteomics",
            "venue": "Journal of Proteome Research,",
            "year": 2011
        },
        {
            "authors": [
                "V. Granholm",
                "W.S. Noble",
                "L. K\u00e4ll"
            ],
            "title": "A cross-validation scheme for machine learning algorithms in shotgun proteomics",
            "venue": "BMC Bioinformatics,",
            "year": 2012
        },
        {
            "authors": [
                "K. He",
                "Y. Fu",
                "W.-F. Zeng",
                "L. Luo",
                "H. Chi",
                "C. Liu",
                "L.-Y. Qing",
                "R.-X. Sun",
                "S.-M. He"
            ],
            "title": "A theoretical foundation of the target-decoy search strategy for false discovery rate control in proteomics",
            "year": 2015
        },
        {
            "authors": [
                "L. K\u00e4ll",
                "J. Canterbury",
                "J. Weston",
                "W.S. Noble",
                "M.J. MacCoss"
            ],
            "title": "A semi-supervised machine learning technique for peptide identification from shotgun proteomics",
            "venue": "datasets. Nature Methods,",
            "year": 2007
        },
        {
            "authors": [
                "A. Keller",
                "A.I. Nesvizhskii",
                "E. Kolker",
                "R. Aebersold"
            ],
            "title": "Empirical statistical model to estimate the accuracy of peptide identification made by MS/MS and database search",
            "venue": "Analytical Chemistry,",
            "year": 2002
        },
        {
            "authors": [
                "A. Kertesz-Farkas",
                "F.L. Nii Adoquaye Acquaye",
                "K. Bhimani",
                "J.K. Eng",
                "W.E. Fondrie",
                "C. Grant",
                "M.R. Hoopmann",
                "A. Lin",
                "Y.Y. Lu",
                "R.L. Moritz",
                "M.J. MacCoss",
                "W.S. Noble"
            ],
            "title": "The Crux Toolkit for Analysis of Bottom-Up Tandem Mass Spectrometry Proteomics Data",
            "venue": "Journal of Proteome Research,",
            "year": 2023
        },
        {
            "authors": [
                "J. Klimek",
                "J.S. Eddes",
                "L. Hohmann",
                "J. Jackson",
                "A. Peterson",
                "S. Letarte",
                "P.R. Gafken",
                "J.E. Katz",
                "P. Mallick",
                "H. Lee",
                "A. Schmidt",
                "R. Ossola",
                "J.K. Eng",
                "R. Aebersold",
                "D.B. Martin"
            ],
            "title": "The standard protein mix database: a diverse data set to assist in the production of improved peptide and protein identification software tools",
            "venue": "Journal of Proteome Research,",
            "year": 2008
        },
        {
            "authors": [
                "A. Lin",
                "T. Short",
                "W.S. Noble",
                "U. Keich"
            ],
            "title": "Improving peptide-level mass spectrometry analysis via double competition",
            "venue": "Journal of Proteome Research,",
            "year": 2022
        },
        {
            "authors": [
                "L. Martens",
                "H. Hermjakob",
                "P. Jones",
                "M. Adamsk",
                "C. Taylor",
                "D. States",
                "K. Gevaert",
                "J. Vandekerckhove",
                "R. Apweiler"
            ],
            "title": "PRIDE: The proteomics identifications",
            "venue": "database. Proteomics,",
            "year": 2005
        },
        {
            "authors": [
                "D.H. May",
                "K. Tamura",
                "W.S. Noble"
            ],
            "title": "Detecting modifications in proteomics experiments with Param-Medic",
            "venue": "Journal of Proteome Research,",
            "year": 2019
        },
        {
            "authors": [
                "P. Sulimov",
                "A. Kert\u00e9sz-Farkas"
            ],
            "title": "Tailor: A nonparametric and rapid score calibration method for database search-based peptide identification in shotgun proteomics",
            "venue": "Journal of Proteome Research,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords: proteomics, false discovery rate control, tandem mass spectrometry"
        },
        {
            "heading": "1 Introduction",
            "text": "Tandem mass spectrometry provides a high-throughput method for detecting proteins in a complex sample. The approach involves digesting the proteins to peptides and then running a liquid chromatography tandem mass spectrometry (MS/MS) experiment to generate MS2 fragmentation spectra. Each of those spectra is then searched against a relevant peptide database to find its best matching peptide. Deciding which peptides to report is most commonly done through target-decoy competition (TDC), which involves searching the same spectra against a database of decoy peptides, generated by reversing or randomly shuffling the original (\u201ctarget\u201d) peptides. For each spectrum, the best-scoring target and decoy peptides compete against each other. Thereafter, a relatively simple calculation provides an estimate of the false discovery rate (FDR) among all target peptides that win the competition and whose assigned score exceeds a given threshold [4, 17]. Notably, it has been established that, under natural assumption that are specified below, this TDC procedure theoretically guarantees FDR control [1, 12].\nUnfortunately, applying TDC to database search results can become more complicated when those results are first post-processed by a machine learning algorithm. The general idea of applying a machine learning classifier to re-rank peptide-spectrum matches (PSMs) from a database search was pioneered by the PeptideProphet algorithm [14], which involved training a linear discriminant analysis (LDA) model to classify PSMs on the basis of a hand-curated set of \u201ccorrect\u201d and \u201cincorrect\u201d PSMs. This approach suffered from poor generalizability when the data used to train the model had different characteristics than the data being classified. This problem led to the development of Percolator [13], which is a semi-supervised machine learning algorithm that trains its model using a collection of target and decoy PSMs. The idea is that the decoy PSMs can be labeled \u201cincorrect,\u201d whereas the targets are a mixture of \u201ccorrect\u201d and \u201cincorrect\u201d PSMs. Importantly, because target and decoy PSMs can be generated automatically, no hand curation of labels is necessary, and Percolator can be retrained for every new dataset. This approach guarantees that the training set and test set distributions are similar. The semi-supervised approach has become the de facto standard in the field.\nDespite its popularity, we have recently shown that in practice Percolator can fail to control the FDR [7]. The problem arises because Percolator uses decoys in two fundamentally different ways: first to train a machine learning model to distinguish between correct and incorrect PSMs, and second to estimate FDR using the TDC procedure. Recognizing this challenge, Percolator uses a cross-validation procedure to attempt to prevent information about the target/decoy labels to leak from the training set to the test set [11]. However, we argued that in practice Percolator\u2019s training process can indirectly peek at the target/decoy labels of the test set, and hence the overall procedure may fail to control the FDR.\nIn this paper we offer a new approach for training a semi-supervised machine learning model in a way that does not break the FDR control. We achieve this goal by randomly placing each decoy win into one of two sets, where the first set is used for training the model and the second is reserved for estimating the number of false target discoveries. Accordingly, we refer to our new approach as \u201cRESET\u201d (REScoring via Estimating and Training). We establish that under a natural extension of TDC\u2019s key assumptions RESET rigorously controls the FDR. We also use entrapment experiments to show that integrating RESET with Percolator (\u201cPercolator-RESET\u201d) empirically offers significantly more robust control of the FDR than Percolator does. Interestingly, in spite of its much tighter FDR control, Pecolator-RESET gives up very little in terms of power (i.e., the number of discoveries) when compared with Percolator.\nWe also offer an extension of RESET that considers two decoys for every target. The use of a second decoy reduces RESET\u2019s variability, which is inherent to its random split of the decoy\nset. Moreover, integrating this enhanced version with Percolator delivers a slightly better trained version of Percolator-RESET, allowing the two-decoy version to deliver slightly more discoveries than the original Percolator and the single-decoy Percolator-RESET. Finally, we note that although this paper is concerned with the specific goal of detecting peptides using fragmentation spectra, the two RESET procedures are more generally applicable."
        },
        {
            "heading": "2 Methods",
            "text": ""
        },
        {
            "heading": "2.1 Target-decoy competition",
            "text": "Our implementation of the RESET procedure employs a modification of the standard TDC procedure, so we begin with outlining the four steps of PSM-and-peptide, which is the particular protocol of peptide-level TDC that we build on [17]. First, we compete each target PSM with its corresponding decoy PSM, and we keep only the higher scoring of the two (ties are randomly broken throughout). Second, each peptide (target or decoy) is assigned a score, which is that of the maximal scoring PSM associated with the peptide. A key feature of PSM-and-peptide is that it next extends the target-decoy competition to the peptide level: in the third step, each target peptide competes with its paired decoy, and only the higher scoring of the two is kept, together with a label indicating whether the winning score corresponds to the target peptide (\u201ctarget win\u201d) or the decoy peptide (\u201cdecoy win\u201d).\nFinally, PSM-and-peptide proceeds, as all variants of TDC do, to use the decoy wins to estimate the false discovery rate (FDR) among the corresponding target wins, which it subsequently uses to determine which winning target peptides to report. More specifically, TDC begins this step by sorting the winning peptide scores in decreasing order. For each k, it then uses the number of decoy wins in the top k scores as an estimate of the number of falsely detected/discovered target peptides among the top k winning peptides. This approach is justified by TDC\u2019s first key assumption, namely, that each target peptide that is absent from the sample is equally likely to win the peptide-level competition as its decoy. Dividing this estimated number of falsely discovered target peptides by the complementary number of target wins in the top k winning peptides gives an estimate of the false discovery rate among those top target wins. To rigorously control the FDR at level \u03b1, TDC adds 1 to the number of decoy wins before dividing by the number of target wins, and it looks for the largest k for which this estimate is still \u2264 \u03b1. TDC then uses this largest k to report all the target peptides among the top k peptides. This estimating/controlling step of TDC is further described in SeqStep+ (Algorithm 1 with c = 1/2 here), which in turn is essentially the Selective SeqStep+ procedure of [1]. It was shown that adding to the first key assumption a second one, which requires that the first assumption holds for each target-decoy paired-peptides independently of all other pairs, as well as of the pair\u2019s winning score, TDC rigorously controls the FDR [1, 12].\nImportantly, the same approach will control the FDR for any ordering of the winning peptides that does not peek at the target/decoy win labels [1]. In particular, if we can find a better ordering of the winning scores, that is, one that is likely to rank correct discoveries higher up, then applying the same estimating/controlling step that TDC does (SeqStep+ with c = 1/2) will yield more discoveries while still controlling the FDR."
        },
        {
            "heading": "2.2 Feature-augmented target-decoy competition",
            "text": "Although we focus in this work on the peptide detection problem, we present our method in the following, more generally applicable context. Consider a setup where we have a list of proposed\ndiscoveries, of which only a subset are true (e.g., a list of all the target database peptides). Our goal is to report as many of the discoveries as possible while controlling the FDR, which in this case is the expected proportion of false discoveries (e.g., target peptides that are not in the sample) among the reported ones. As in TDC, we assume that each putative discovery is paired with a competing, artificially constructed, or statistically speaking, null-generated discovery (e.g., the paired decoy peptide). Both discoveries, which we refer to as the target and decoy discoveries, respectively, are scored, where a higher score indicates a discovery that is more likely to be true (e.g., a peptide\u2019s score is that of the maximal PSM associated with it). As an aside, note that in the context of variable selection, where this setup is also applicable, the decoys are called knockoffs [1].\nDeparting now from the canonical TDC setup, we introduce a \u201cfeature-augmented\u201d setting, in which we assume that we can associate with each discovery a list of features that may help us distinguish between correct and incorrect discoveries. For example, in the peptide detection problem each target or decoy peptide that has at least one PSM associated with it can inherit the features of its maximally scoring PSM, such as the ones that Percolator uses: the precursor charge state, peptide length, etc. Peptides with no associated PSMs either lose their paired competition or are dropped if neither of the paired peptides is associated with a PSM.\nImportantly, the features should not be able to help us distinguish between the target and its decoy when the target discovery is incorrect. Indeed, a feature like this would compromise our approach, similar to how it can foil Percolator\u2019s FDR control [2, 10]. In the feature-augmented setting, we also assume that the following extended analogue of TDC\u2019s key assumptions hold. First, each false target discovery is equally likely to be the higher scoring when competing with its paired decoy. Second, this happens independently of all the winning scores, all features, and the labels of all other discoveries.\nThe same comment made earlier about TDC applies to feature-augmented TDC: applying TDC\u2019s SeqStep+ to any ordering of the discoveries that does not peek at the target-decoy win labels controls the FDR. We next describe two new procedures that take advantage of this fact by offering an improved ordering in this general context."
        },
        {
            "heading": "2.3 The RESET (REScoring via Estimating and Training) procedure",
            "text": "RESET was developed to take advantage of the additional features to rescore the discoveries so that correct discoveries will generally rank higher. While originally developed to integrate with the Percolator algorithm, RESET is generally compatible with any semi-supervised machine learning approach that takes advantage of the target/decoy labels in a similar fashion. RESET is described through the following main steps, with further details given in Algorithm 2.\n1. RESET begins with a TDC-like competition step: from each pair of target and decoy discoveries only the maximal one is kept, together with a label indicating whether this was a target or a decoy win. RESET also records the vector of features associated with the winning discovery. As an aside, note that there is some flexibility in how the winning score is defined\u2014for example, it can be the absolute value of the difference in the scores [1]\u2014but here we focus on the maximum.\n2. RESET then defines the training decoy set by independently randomly assigning to it each winning decoy with probability s (we used s = 1/2 throughout). We refer to the remaining set of winning discoveries (targets, as well as non-training decoys) as pseudo targets.\n3. Next, RESET applies a user-selected semi-supervised machine learning model whose input includes the following:\n\u2022 a negative set containing the scores and features of the training decoys, and \u2022 a set containing the winning scores and features of the pseudo-targets but without the labels distinguishing between targets and decoys.\nThe output of this step is a rescoring of the training decoys and pseudo-targets, ideally ranking many of the correct pseudo-target discoveries higher than they were ranked originally.\n4. Once the training is complete, the training decoys are thrown out, and the pseudo-targets are ordered according to the learned scores, with their corresponding original target/decoy winning labels revealed. Some of the revealed labels will correspond to decoy wins\u2014we refer to those as estimating decoys\u2014and the others correspond to target wins. RESET applies SeqStep+, as in TDC, but uses the number of estimating decoys in the top k pseudo-target scores to estimate the number of false discoveries among the targets in the same top k scores. The key difference with TDC is that because in RESET we use approximately half of the decoys for training purposes, each estimating decoy estimates more than one false target discovery. To adjust for that difference, we multiply the number of decoys (plus 1) by 2. The rest of SeqStep+ works exactly as in TDC (Algorithm 1 with c1\u2212c = 2, or c = 2/3)."
        },
        {
            "heading": "2.4 Using an extra decoy: an enhancement of RESET",
            "text": "We introduce an enhancement of RESET that makes use of an extra decoy for each target. It goes through the same steps as RESET with the following modifications:\n\u2022 In step 1, the competition now takes place between all sets of triplets: the target and its two decoys. We record a target/decoy label indicating whether the target or one of its two associated decoys is equal to the maximum of the three scores, corresponding to a target or decoy win, respectively (again ties are broken randomly). In addition to the winning score and a target/decoy win label, we keep the vector of features of the winning discovery. This multiple competition method is essentially the same as the max method introduced by Emery et al. [5].\n\u2022 When applying SeqStep+, each estimating decoy now estimates a single false target discovery (as in TDC itself), so no adjustment is made to the observed number of decoy wins (Algorithm 1 with c = 12).\nNotably, in this enhanced version of RESET, each target competes in step 1 with two associated decoys rather than a single one. The sets of of training and estimating decoys are roughly equal-sized for both RESET procedures, and therefore both sets are typically larger in the case of the twodecoy version. While these larger training and estimating sets come at a cost of a reduced number of target wins, keep in mind that these lost targets only win one of their two decoy competitions; hence, they are unlikely to be high scoring enough to eventually make it to the reported discoveries list. At the same time, those larger sets of training and estimating decoys allow us to better train our model, as well as to have a more stable final procedure when defining the discovery list using SeqStep+. This is borne out in practice when we integrate Percolator with the two-decoy RESET below."
        },
        {
            "heading": "2.5 Combining RESET with Percolator",
            "text": ""
        },
        {
            "heading": "2.5.1 Overall approach: Percolator-RESET",
            "text": "RESET allows us to fix Percolator\u2019s lapses in FDR control by integrating the latter\u2019s iterative linear SVM training into the RESET wrapper framework. As a specialization of RESET, Percolator-\nRESET (Algorithm 3) can be described by applying the following adjustments to the steps described in Section 2.3:\n\u2022 In Step 1 of Section 2.3, we integrate the PSM-and-peptide protocol of [17] (Section 2.1 and Algorithm 4a) with the dynamic-level competition we introduced in [6]. We start with the same initial PSM-level competition that PSM-and-peptide employs, recording for each spectrum its best matching peptide in the concatenated target-decoy database while using the primary database search score. For example, in our experiments using the Tide search engine [3], we employ the Tailor similarity score [20].\nAssuming no variable modifications are considered, the subsequent competition occurs, as in the original PSM-and-peptide, at the peptide-level: we define each peptide\u2019s score as the maximal PSM score associated with that peptide, and we let the peptide inherit all the features associated with this maximal scoring PSM (charge, peptide length, delta mass, etc.). Then for each pair of target-decoy peptides, we take the peptide with the higher peptide score, along with the associated feature information. The selected peptide may be a decoy (\u201cdecoy win\u201d) or a target (\u201ctarget win\u201d).\nOne challenge is that, when variable modifications are considered, the target database will contain clusters of target peptides with highly similar theoretical spectra. In this case, none of the canonically constructed decoy databases can offer a proper competition to a target peptide that was identified due to an incorrect match within its cluster. To account for this, the level of competition dynamically switches from the peptide level to what we call the \u201cstem\u201d level (where the stem is the peptide sequence with all modifications removed) [6]. At this level, we consider the maximal scoring peptide (a representative peptide) from each cluster in the target database, and we compete this peptide with the maximal scoring peptide from the associated decoy pairs. We provide further details in Section 2.5.3 and Algorithms 4a-4c.\n\u2022 For Step 3 of Section 2.3, the negative set used to train an SVM is the set of training decoys. We extract a positive set from the pseudo-targets analogously to Percolator by applying SeqStep+ (Algorithm 1), except we use the pseudo-target peptides and the training decoy peptides instead of the target and decoy PSMs as Percolator does. More specifically, all the winning peptides are first ordered by decreasing Tailor scores. Using SeqStep+ we then look for the largest set for which the estimated proportion of false discoveries among the top pseudo-target peptides is still \u2264 \u03b11 (we use the same \u03b11 = 0.01 as in Percolator). Because we only sampled s = 1/2 of the decoy wins to define the training decoys, we need to adjust for that by multiplying the observed number of training decoys (plus 1) by 3. The corresponding set of \u201cdiscovered\u201d pseudo-targets is our positive set for training the SVM. In Section 2.5.4 we explain how we deal with cases where the above defined positive set is empty. Using the positive and negative training sets, and their associated features (including the scores), we train an SVM to rescore each peptide (further details are given in Section 2.5.4).\nWe then iterate the procedure described in the above paragraph but using the SVM score instead of the Tailor score to re-rank the peptides. The SVM is trained this way for a fixed number of iterations (we used total iter = 5)."
        },
        {
            "heading": "2.5.2 Percolator-RESET with two decoys",
            "text": "If we search the spectra against two randomly generated peptide-shuffled decoy databases, then we can employ the two-decoy version of Percolator-RESET, which follows the steps of the enhanced two-decoy RESET procedure with the following adjustments:\n\u2022 In step 1 we generalize the double competition protocol of Section 2.5.1 to account for an extra decoy database as follows. The PSM-level competition now takes place by assigning to each spectrum its best matching peptide across the two searches, which is akin to searching a database made of the target concatenated to both decoy databases. Each peptide is then assigned its maximal Tailor-scoring PSM, and it inherits the maximal PSM\u2019s features (Section 2.5.5 fills in some technical details that we glossed over here).\nAs with the single-decoy case, we next employ dynamic-level competition: if no variable modifications are used, then the second competition takes place between all sets of matched peptide triplets: a target and its two shuffled decoy derivatives. The peptide with the maximal score wins the triplet competition, and the triplet is represented by its winning peptide with its inherited features while the two losing peptides are discarded. If variable modifications are used, then a stem-level competition takes place between all sets of matched cluster triplets: the cluster of target peptides and the two clusters of their corresponding decoys, one from each decoy database. The representative peptide with the maximal score from each cluster wins the triplet competition, and the triplet is represented by its winning representative peptide together with its inherited features while the two losing representative peptides are discarded (see Algorithms 5a-5c.)\n\u2022 The SVM training phase (step 3) is mostly the same as in the single-decoy Percolator-RESET. The positive set is similarly defined by applying SeqStep+ to the pseudo-targets and training decoys using the same \u03b11 = 0.01. However, we now multiply the observed number of training decoy (plus 1) by 2 rather than 3 as in the analogous step in the single-decoy PercolatorRESET."
        },
        {
            "heading": "2.5.3 Dynamic-level competition",
            "text": "We recently pointed out that if variable modifications are specified, the decoy database can no longer provide adequate competition against some of the incorrectly identified target peptides [6]. Specifically, TDC relies on the assumption that a false discovery (a peptide that won its competition but is not in the sample) is equally likely to be a target or a decoy win. However, when variable modifications are specified the database will often contain clusters of highly similar peptides corresponding to variable modifications of the same unmodified (\u201cstem\u201d) form of a single peptide. The problem is that the canonically constructed decoys (by shuffling or reversing the target peptides) cannot account for mismatches within such clusters. For example, consider a spectrum that was generated by P[10]EPTIDE but because of random noise is optimally matched to PEP[10]TIDE. In this case, the decoys cannot offer a real competition to such a mismatch because, typically, the \u201cclosest\u201d decoy to P[10]EPTIDE would still be significantly different from its cluster-neighbor PEP[10]TIDE. This problem can be further exacerbated in a so-called \u201copen\u201d search, in which each peptide is allowed to match to spectra whose associated precursor masses can substanially differ from the mass of the peptide. In an open search setting, in our example P[10]EP[10]TIDE would also belong to the cluster.\nTo address this problem we employ an adaptation of the dynamic-level competition introduced in [6]: when no variable modifications are used, the second competition takes place at the peptide level. On the other hand, when variable modifications are used and the tandem spectra are searched with a narrow precursor tolerance, then the stem level analysis first clusters peptides with the same residue sequence and modification mass. In our above example, this would amount to clustering the peptides P[10]EPTIDE, PEP[10]TIDE, and any other peptidoform of PEPTIDE with a mass modification that totals 10 Da. We then select from each cluster its highest scoring peptide as\nthe cluster\u2019s representative. Finally, the target-decoy pairing can be readily extended to the clusters, and hence to the clusters\u2019 representatives. Therefore, we can execute our second target-decoy competition at this representative level, keeping the higher scoring of the two competing representatives together with its associated features. Note that the competing representatives are paired at the cluster level, and each of them might be paired with other peptides from the cluster of the competing representative. The above procedure also applies when variable modifications are used in combination with an open search, except that the clusters are defined as all peptidoforms of the same stem form (clustering all peptidoforms of PEPTIDE in our example above).\nAn algorithmic description of this procedure is given in Algorithms 4a-4c, and the extension of the procedure when two decoy databases are considers is outlined in Section 2.5.2 and detailed in Algorithms 5a-5c."
        },
        {
            "heading": "2.5.4 SVM training in Percolator-RESET",
            "text": "If the dataset is sufficiently small, it is possible that the positive training set, determined in Step 3 of Sections 2.5.1 and 2.5.2, will be empty. In this case, we first remove the +1 penalty from SeqStep+ (SeqStep in Algorithm 1) and repeat the procedure without it. If the newly defined positive training set is still empty, then we iteratively increase the training FDR threshold from its initial value of \u03b11 = 0.01 by increments of 0.005 until a non-empty positive training set is obtained.\nBecause the positive and negative training sets are expected to be unbalanced, we select the class weights C+, C\u2212 \u2208 {0.1, 1, 10} using a three-fold cross validation procedure, as in Percolator. Here we use the average number of discoveries using SeqStep+ with the +1 penalty and the training FDR threshold \u03b11 := 0.01 as the criterion for selection of those hyperparameters. If none of the class weight combinations yields a positive average number of discoveries (which may happen for small data sets), then we follow the procedure described above, first dropping the +1 penalty, and then iteratively increasing the training FDR threshold by 0.005 while the procedure fails to report any discoveries."
        },
        {
            "heading": "2.5.5 Processing the Tailor scores in Percolator-RESET with two decoys",
            "text": "In practice, we obtain search results for the two-decoy Percolator-RESET by applying the Tide search engine twice, once for each of the two randomly generated peptide-shuffled decoy databases. Specifically, in each search, Tide is given the target database and a paired decoy database, and it searches for the best match in each of the two databases. The best target match for a given spectrum does not vary between the two searches, but that optimal target PSM will typically have different Tailor scores across the two searches. This is because the Tailor score normalizes the raw PSM score with respect to the quantiles of all the target and decoy matches to the given spectrum, and the decoys vary between the two searches [20]. Since we use the Tailor score for doing the peptide-level competition in Percolator-RESET, as well as for ordering the winning peptides to define the initial positive set, we address the aforementioned variation by averaging the two Tailor scores assigned for each target PSM."
        },
        {
            "heading": "2.5.6 Reporting PSMs",
            "text": "Both versions of Percolator-RESET promise theoretical FDR control only at the (representative) peptide, because the PSM level suffers from a multiplicity issue that violates the usual independence assumptions [12, 17]. Understandably, some sort of PSM-level analysis is often desirable. For example, when using an open search, multiple matches to the same stem peptide may correspond to multiple types of modifications of that peptide. Hence, similar to CONGA [6], we additionally\nreport a list of PSMs that are associated with the originally discovered peptides. The process is essentially the same as the one used in CONGA: we first define a set of mass bins for each discovered (stem) peptide by a greedy clustering algorithm that takes into account the specified isolation window (Supplementary Algorithm 6). Then the highest (Tailor) scoring one of all the PSMs with the same peptide that fall into the same mass bin is reported. The list of PSMs is enhanced with a label for each PSM, which indicates whether the PSM scored above the SVM score threshold, giving the user additional confidence in those PSMs."
        },
        {
            "heading": "2.5.7 Availability",
            "text": "Open source, Apache licensed Python implementations of both Percolator-RESET procedures are available at https://github.com/freejstone/Percolator-RESET. Both tools will be implemented as part of an upcoming release of Percolator (v4.0)."
        },
        {
            "heading": "2.6 Datasets",
            "text": ""
        },
        {
            "heading": "2.6.1 Entrapment runs",
            "text": "Entrapment runs were performed using data taken from the standard protein mix database, ISB18 [16]. We used the nine .ms2 spectrum files taken from [17], which were originally sourced from the \u201cMix 7\u201d spectrum files downloaded from https://regis-web.systemsbiology.net/ PublicDatasets/18_Mix/Mix_7/ORBITRAP/ (this excluded the tenth, smallest spectrum file). These .ms2 spectrum files were subsequently used as input for Tide.\nThe in-sample database contained the 18 proteins of the mix and 30 additional hitchhiker proteins downloaded from https://regis-web.systemsbiology.net/PublicDatasets/database. We used the castor plant proteome as the entrapment database, as in [17]. Tide-index was used to virtually digest the in-sample and entrapment database. Four random subsets of varying proportions of the in-sample peptides were added to the entrapment peptides to create four combined target peptide databases. Tide-index was then used to create 100 randomly shuffled decoy database indices for each of the four. Param-medic [19] was used to determine any variable modifications, though none were detected. Any entrapment peptide that was identical (up to any leucine/isoleucine substitution) to an ISB18 peptide was removed from the entrapment database. Table 1 lists the resulting numbers of in-sample and entrapment peptides in each combined target peptide database, along with their in-sample-to-entrapment ratios.\nTide-search was used to search the nine .ms2 spectrum files against each of the combined target and decoy databases with the following settings: for narrow searches we used --auto-precursorwindow warn --auto-mz-bin-width warn --use-tailor-calibration T --pin-output T,\nand for open searches we used --auto-mz-bin-width warn --precursor-window-type mass -- precursor-window 100 --use -tailor-calibration T --pin-output T. The option --concat was set to T for searches subsequently used by Percolator and the single-decoy Percolator-RESET and F for searches subsequently used by the two-decoy Percolator-RESET. We used Tide in Crux v4.1.6809338 [15]."
        },
        {
            "heading": "2.6.2 PRIDE-20 searches",
            "text": "We downloaded 20 high-resolution spectrum files from the Proteomics Identifications Database, PRIDE [18]. Seven of these spectrum files were taken from [8] and were originally obtained by randomly selecting a spectrum file from randomly selected PRIDE projects (submitted no earlier than 2018). The remaining 13 spectrum files were similarly obtained by randomly selecting a single spectrum file from a randomly selected PRIDE project (submitted no earlier than 2019). The sampling was constrained to generate a collection of 10 spectrum files that have modifications and 10 spectrum files for which no modifications were detected, as determined in both cases by Parammedic [19]. The protein FASTA database files were also downloaded from the associated PRIDE projects or, in the case of human data, the UniProt database UP000005640 was used (downloaded 9/11/2021). Table 2 reports the list of the 20 spectrum files and PRIDE projects used.\nFor each of the PRIDE-20 data sets, we used Tide index 10 times to create 10 different decoy peptide databases, each of which was paired with the target to yield 10 target-decoys databases. We used --auto-modifications T to call Param-medic and account for variable modifications as detected. For narrow searches using Tide, we used the following options -- auto-precursor-window warn --auto-mz-bin-width warn --use-tailor-calibration T and for open searches using Tide we used --auto-mz-bin-width warn --precursor-window-type mass --precursor-window 100 --use-tailor-calibration T. The option --concat was set to T for searches subsequently used by Percolator and the single-decoy Percolator-RESET, and to F for searches subsequently used by the two-decoy Percolator-RESET. We constructed pin files using the option --pin-output T, or using the utility function make-pin."
        },
        {
            "heading": "2.6.3 Percolator and Percolator-RESET settings",
            "text": "For each pin file, we used the default number of top 5 PSMs which allows for generating the feature deltLCn (the difference between the XCorr score of the top scoring PSM and the fifth/last ranked PSM, divided by the maximum of the top PSM\u2019s XCorr score and 1). In order to filter out hitchhiking neighbors, or peptides that might offer a fairly good match to a spectrum simply because they share many peaks with the top matching peptide, we subsequently kept only the top PSM for each spectrum. Lastly, we removed the enzInt feature from consideration (technically we set it to 0 for each PSM), as it was shown that this feature can compromise Percolator\u2019s FDR control by allowing it to distinguish between target and decoy PSMs [2]. Overall, the features used for training are deltLCn, deltCn, Xcorr, TailorScore, PepLen, Charge, enzN, enzC, lnNumSP, dM, absdM where Charge is represented by a one-hot vector indicating the charge state of the precursor. An explanation of each feature is given in Supplementary Table 3.\nWhen running Percolator in Crux, we used --only-psms T, --tdc F in the crux percolator command, which reports only the PSMs and does not apply the TDC procedure within Percolator. This allowed us to apply the dynamic-level competition protocol (outlined in Section 2.5.3 and Algorithms 4a-4c) to Percolator\u2019s list of PSMs, using the learned percolator score. Furthermore we varied the option --percolator-seed during the entrapment experiments. Doing this randomizes the cross-validation folds between each call to Percolator.\nOur Python implementation of the two-decoy Percolator-RESET takes in the pin files searched against the target and decoy databases separately. Therefore, when running this version of PercolatorRESET we adjusted some of the features so that they were calculated with respect to the combined target and decoy databases instead. These include lnNumSP (the number of candidate peptides in the combined database for each spectrum), deltCn (the difference between the XCorr score of the two top ranked PSMs with respect to the combined database, divided by maximum of the top PSM XCorr score and 1), and deltLCn (the difference between the XCorr score of the top scoring PSM and the fifth/last ranked PSM with respect to the combined database, divided by the maximum of the top PSM\u2019s XCorr score and 1)."
        },
        {
            "heading": "3 Results",
            "text": ""
        },
        {
            "heading": "3.1 Both versions of RESET control the FDR",
            "text": "There are two complementary ways in which a given procedure can be said to control the FDR: theoretically and empirically. A rigorous proof of theoretical FDR control is highly desirable, but even with such a proof, empirical evidence of FDR control is also useful, as a way to check that the\nassumptions underlying the proof hold in practice. Accordingly, we first prove that both versions of RESET provide theoretical FDR control and then demonstrate this control using real data.\nAny proof of FDR control must start by laying out its assumptions. In our case, the RESET procedures provide theoretical guarantee of FDR control if we make analogous assumptions to ones that form the basis of TDC [1, 12]. Specifically, for the single-decoy RESET, we assume that an incorrect target discovery is equally likely to end up winning the competition with its paired decoy (Step 1 of 2.3) as the decoy is, and moreover this happens independently of the winning scores, W , all features x, and the labels L of all other discoveries. Because the enhanced RESET considers two decoys for each target instead of one, the above assumptions must be slightly adjusted to reflect the fact that for an incorrect target and its two derived decoys, each are equally likely to win their triplet competition. That is, we assume that the probability that an incorrect target discovery will win its triplet competition is 1/3 (or a decoy will win with probability 2/3), and again this happens independently given the same conditions as above.\nThe following theorem, whose proof is given in Supplementary Section A.2, underpins the theoretical basis of FDR control.\nTheorem 1. Under the above assumptions both versions of RESET control the FDR in the featureaugmented TDC setting at the user-specified threshold of \u03b1.\nBecause the RESET procedures are general wrappers for any machine learning model, Theorem 1 trivially holds true for the two Percolator-RESET procedures under the same assumptions.\nTo provide an empirical assessment of FDR control, we carried out a series of \u201centrapment\u201d experiments [9]. In this setup, spectra generated in a controlled experiment are searched against a combined database containing those in-sample peptides plus an irrelevant database of entrapment sequences, typically taken from a species known not to be present in the sample. For our experiments, we used spectra from an 18-protein mixture (ISB18) [16]. Furthermore, as first introduced in [7], to increase the amount of foreign spectra and thus vary the signal-to-noise ratio, the in-sample database is randomly downsampled to use increasingly smaller proportions (100%, 75%, 50%, 25%). The resulting four target databases have a ratio of entrapment to ISB18 peptides ranging from 636:1 to 2539:1. Because the number of entrapment peptides is so much larger than the number of ISB18 peptides, incorrect PSMs will usually involve an entrapment peptide, thereby allowing us to reliably estimate the false discovery proportion (FDP) among the set of discovered peptides. Furthermore, by averaging the estimated FDP over 100 randomly generated decoy databases, along with 2 random draws of the training decoy set for each decoy database, we obtain an estimated FDR for a given FDR threshold. If the FDR control procedure is valid, then this empirical FDR should not significantly exceed the threshold. We conducted the entrapment experiments using both narrow and open searches, as specified in Section 2.6.1.\nThe results of our entrapment experiments (Figures 1A-B) confirm that the empirical FDRs of the Percolator-RESET procedures are typically below their corresponding thresholds. We observe a few exceptions to this trend, the worst (as a percentage) being for the two-decoy PercolatorRESET at an FDR threshold of 0.03, with an empirical FDR of 0.0319 (using 100% of the insample peptides and using an open-search mode). Given that we averaged the result over 100 decoys, this 6% violation does not seem that significant. Moreover, keep in mind that even if we were able to average the FDP over a much larger number of decoys, we still do not average out all the variability in the experiment: the MS/MS runs themselves introduce a random effect that cannot be eliminated just by averaging over multiple drawn decoys. Indeed, accounting for that latter effect would have required repeating these MS/MS experiments many times.\nWe also analyzed Percolator\u2019s behavior in the context of this entrapment setup. The results (Figure 1C), done here with 100 decoy databases, reinforce our previously reported observation\n(based on only 20 decoy databases), that Percolator struggles to control the FDR in these entrapment experiments [7]: the worst empirical FDRs at thresholds 0.01, 0.05 and 0.1 were 0.0195, 0.1068 and 0.1419, respectively. As noted previously, we believe that Percolator\u2019s failure to control the FDR stems from the presence of multiple spectra generated from the same peptide species. Consequently, these essentially identical spectra may appear in both the training and test sets in Percolator\u2019s cross-validation scheme causing the model to overfit and violate FDR control."
        },
        {
            "heading": "3.2 The Percolator-RESET procedures maintain a similar number of discoveries to Percolator",
            "text": "Percolator can fail to control the FDR, and therefore its results should be taken with a grain of salt. However, particularly in light of Percolator\u2019s popularity, it is instructive to compare its apparent statistical power (i.e., the number of peptides discovered at a specified FDR threshold) with that of our two new procedures. To do so, we used a collection of publicly available MS/MS datasets from 20 projects in the Proteomics Identifications Database (PRIDE) [18]. The datasets were essentially selected at random, with the constraint that Param-Medic [19] detected posttranslational modifications in 10 of the 20 files. For each spectrum file, we used Tide-index to create 10 randomly shuffled decoy databases. Then for each spectrum file we used Tide-search to search against each of the target-decoy databases in both narrow and open search mode, resulting in a total of 20\u00d7 10\u00d7 2 searches.\nThe results (Figure 2) demonstrate that both versions of Percolator-RESET provide a comparable number of detected peptides as Percolator on the PRIDE-20 datasets. Indeed, at a 1% FDR threshold the single-decoy Percolator-RESET marginally loses to Percolator: considering the ratio of the number of reported discoveries averaged over the 10 decoy sets, we find its median across the PRIDE-20 dataset (expressed as percentages) is 99.8% in narrow mode and 99% in open mode. At higher FDR thresholds, this difference in number of discoveries shrinks even more with the median average ratio now at 99.9% in both narrow mode and open mode at the 5% FDR threshold. Moreover, the two-decoy Percolator-RESET appears to marginally edge out Percolator, with median average ratios of 100.3% and 100.6% at a 1% FDR threshold and a median greater\nthan 100% for all remaining FDR thresholds. The slight improvement is perhaps not surprising, given that the algorithm utilizes an additional decoy database. Overall, these results suggest that our new procedures deliver better FDR control without a loss in statistical power."
        },
        {
            "heading": "3.3 Percolator-RESET\u2019s discoveries are less variable using two decoy databases instead of one",
            "text": "As suggested in Section 2.5, we hypothesize that the use of an extra decoy database should allow Percolator-RESET to reduce some of the variability we observe in the number of discoveries reported by the single-decoy version. This variability is inherent to the process of randomly splitting the decoy set into training and estimating decoys and, as explained, the two-decoy version typically increases the size of both sets. It follows that the two-decoy Percolator-RESET typically utilizes a larger number of training decoys as the negative set for SVM training, as well as a larger number of estimating decoys for the FDR estimation, thus reducing the overall variability in the reported list of peptides.\nTo test this hypothesis, we conducted two experiments. We began the first by finding, for each PRIDE-20 spectrum file, the sample variance in the number of discoveries reported in 10 applications of the single-decoy Percolator-RESET, each using a different decoy database. We then compared each of those 20 sample variances with corresponding sample variances of 10 applications of the two-decoy Percolator-RESET, each using a distinct pair of decoy databases (each of the 10 available decoy databases was used in exactly two applications). Specifically, for each PRIDE20 spectrum file we calculated the ratio of the two-decoy Percolator-RESET to the single-decoy Percolator-RESET sample variances. Figure 3A shows the quartiles (over the 20 datasets) of these ratios in logarithmic scale, using both narrow and open searches. At the 1% FDR threshold the two-decoy version typically exhibits significantly less variability than the one-decoy version, yielding a median reduction of 49.1% in both the narrow-search and open-search modes.\nIn the second experiment, we focused on the variation in the number of discoveries due to the\nrandom splitting of the decoys into training and estimating decoys. For each PRIDE-20 spectrum file we first pre-selected two decoy databases arbitrarily. Next, we applied the two-decoy PercolatorRESET 20 times, each using a different random split between the training and estimating decoys, and we calculated the sample variance in the number of discoveries over these 20 runs. For the single-decoy version, we applied it 10 times using the first decoy database and 10 times using the second decoy database, randomly splitting the decoys in all those applications. Subsequently, we calculated the pooled sample variance in the number of discoveries reported by the single decoy Percolator-RESET, i.e., the average of the two sample variances calculated with respect to the 10 runs using each of the two databases. Finally, we calculated the ratio of the two-decoy version\u2019s sample variance to the single-decoy version\u2019s pooled sample variance for each PRIDE-20 spectrum file. Figure 3B plots the quartiles of these ratios in logarithmic scale while varying the FDR threshold. Again, we see that the two-decoy version typically exhibits substantially lower variability than the single-decoy Percolator-RESET, with a median reduction of 52.8% and 54.8% at the 1% FDR threshold when using a narrow and open search respectively."
        },
        {
            "heading": "4 Discussion",
            "text": "The Percolator software takes as input tandem mass spectrometry search results and performs two distinct tasks: first, it uses machine learning to automatically re-rank the given set of PSMs, and second, it uses TDC to estimate the FDR in the resulting list. Our previous analysis in [7], as well as our analysis in Section 3.1, demonstrate that Percolator can link these two tasks in a problematic fashion, leading to underestimation of the FDR in the context of peptide detection.\nOur proposed RESET procedures address this problem by introducing a decoy-splitting proce-\ndure, which may be used in conjunction with other semi-supervised machine learning models that rely on a negative and an unlabelled set. We prove that, under mild assumptions analogous to those that TDC relies on, both versions of RESET control the FDR in the feature-augmented TDC setup. We supplement this theoretical result by showing that our two RESET procedures, integrated with a Percolator-like model, appear to effectively control the FDR in the same entrapment experiments that Percolator failed to do so. Importantly, in spite of its tighter FDR control, the power of the single-decoy version of Percolator-RESET is only marginally lower than Percolator\u2019s. Moreover, utilizing a second decoy dataset, the enhanced version of Percolator-RESET is marginally more powerful than Percolator while exhibiting significantly less variability than the single-decoy version in the number of discoveries it reports.\nBy splitting the winning decoys into training and estimating ones, our new procedures do away with Percolator\u2019s cross-validation scheme for FDR control, while still using cross-validation for the selection of hyperparameters. Notably, it is this former cross-validation that seems to be at the heart of Percolator\u2019s potential struggles with FDR control [7]. The cross-validation was deemed necessary at the time, because Percolator uses the entire set of decoy wins as its negative set [11]. Removing the need for cross-validation is also computationally efficient, particularly if our RESET procedures are to be extended to other machine learning models in the future. It would be interesting to compare the impact of different such models on the performance of RESET.\nAnother notable difference between Percolator and both versions of Percolator-RESET is that the former trains its SVM at the PSM level, whereas the latter two train at the peptide level. In fact, it is impossible to train the single-decoy version of Percolator-RESET at the PSM level without violating the assumptions that guarantee its FDR control. Indeed, the same multiplicity problem that makes it impossible to use TDC for PSM-level FDR control applies here: multiple spectra generated by the same peptide species could end up creating nearly identical PSMs that will be shared between the training and estimating decoys. In practice, this means that some sort of over-fitting will occur in this case. In contrast, we can adjust the two-decoy Percolator-RESET so that it can train its SVM at the PSM level while still rigorously controlling the FDR. We plan on looking further into the difference between peptide- and PSM-level training in this case.\nFinally, we stress that dynamic-level competition is relevant beyond Percolator-RESET. Indeed, it is relevant to any application of TDC as it can be essential in validating TDC\u2019s assumption that an incorrect discovery is equally likely to be a target or a decoy win."
        },
        {
            "heading": "A Supplement",
            "text": "A.1 Features used by Percolator-RESET\nA.2 Proof of Theorem 1\nAs noted in Section 2.3, RESET\u2019s discovery list is defined by applying SeqStep+ to the list of pseudo-targets (target wins plus the estimating decoys) ordered by RESET\u2019s learned scores. Accordingly, we can prove Theorem 1 by verifying that the labels of this list of pseudo-targets satisfy the conditions of Theorem 3 of [1]. The latter theorem deals with sequential hypothesis testing, where the null hypotheses Hj , for j = 1, . . . ,m, are sorted in some pre-determined order and it can be restated in our context as follows.\nTheorem 3 (of [1]). Suppose that associated with each hypothesis Hj is a label Lj \u2208 {\u22121, 1} so that for any true null Hj, P (Lj = 1) = c and P (Lj = \u22121) = 1\u2212 c independently of all other Li for i \u0338= j. Then SeqStep+ (Algorithm 1) controls the FDR in the finite sample setting.\nRemark 1. The original formulation of Barber and Cande\u0300s does not refer to any labels, but instead refers to p-values pj that are associated with each hypothesis. Given the p-values we can define the labels as Lj = \u00b11 according to pj \u2264 c or pj > c, and conversely given the labels we can define p-values of pj = c or pj = 1 according to Lj = 1 or Lj = \u22121.\nIt follows that to establish our theorem we only need to analyze the probability distribution of the labels Lj of the true null pseudo-targets winning discoveries, i.e., the incorrect target and estimating decoy winning discoveries defined in steps 1-2 of RESET. For ease of reference, below we refer to the two-decoy version of RESET as RESET+, and to the single-decoy version of RESET as simply RESET. We first remind the reader of the necessary assumptions that we make.\nAssumption 1. For RESET, an incorrect target discovery is equally likely to end up winning the competition with its paired decoy as the decoy is, independently of all the winning scores, W , all the features x, and the labels L of all other discoveries. For RESET+, an incorrect target discovery will win its triplet competition with probability 1/3 and a decoy will win it with probability 2/3, independently of all the winning scores, W , all the features x, and the labels L of all other discoveries.\nLet\n\u2022 N be the set of indices of the true null hypotheses (corresponding to the incorrect target discoveries);\n\u2022 d be the number of decoys per target (d = 1 for RESET and d = 2 for RESET+);\n\u2022 L\u0303j denote whether the jth discovery is a pseudo-target (L\u0303j = 1) or a training decoy (L\u0303j = \u22121).\nBy Assumption 1 and the definition of the training labels L\u0303j , for each j \u2208 N , independently of all the winning scores W , the features x, and all other labels Li for i \u0338= j we have:\nP (Lj = 1, L\u0303j = 1) = P (Lj = 1) = 1/(d+ 1),\nP (Lj = \u22121, L\u0303j = 1) = P (Lj = \u22121) \u00b7 P (L\u0303j = 1|Lj = \u22121) = d/(d+ 1) \u00b7 (1\u2212 s), P (L\u0303j = 1) = P (Lj = 1, L\u0303j = 1) + P (Lj = \u22121, L\u0303j = 1)\n= 1/(d+ 1) + d/(d+ 1) \u00b7 (1\u2212 s) = [1 + d(1\u2212 s)]/(d+ 1),\nP (Lj = 1|L\u0303j = 1) = 1/(d+ 1)\n[1 + d(1\u2212 s)]/(d+ 1) =\n1\n1 + d(1\u2212 s) ,\nP (Lj = \u22121|L\u0303j = 1) = d(1\u2212 s)\n1 + d(1\u2212 s) .\n(1)\nMoreover, because the assignment of the pseudo-target/training decoy labels is determined for each hypothesis independently, and because we are only considering L\u0303j = 1 above, the above probabilities are also independent of all the training labels L\u0303.\nLet \u03a0 be any permutation, or reordering of them hypotheses that is chosen based on information gleaned from all the winning scores W , the features x, and all the training labels L\u0303. The above analysis shows that the last two lines of (1) hold under the new ordering, independently of all other labels Li for i \u0338= j.\nBecause RESET\u2019s reordering is based exactly on W , x, and the labels L\u0303, it follows that with c = 1/[1 + d(1 \u2212 s)] our labels satisfy the conditions of Theorem 3 above, and hence applying SeqStep+ to them controls the FDR.\nA.3 Algorithms\nNote that the detailed description of Percolator-RESET in Algorithm 3 in this section omits step 1 from its outline in Section 2.5.1. The reason is, that this initial PSM-and-peptide dynamiclevel competition is dependent on the size of the precursor tolerance (narrow or open search), whether variable modifications are considered, and whether one or two decoy-databases are used. Thus, it is conceptually easier to detail this step externally to Percolator-RESET, which we do in Algorithms 4a-4c for the single-decoy version, and Algorithms 5a-5c for the two-decoy version.\nAlgorithm 1 SeqStep / SeqStep+ (adopted from Selective Sequential Step+ of [1])\nInput: \u2022 (Wi, Li)mi=1 the list of winning scores, and labels (Li = \u00b11 indicating a target/decoy win); \u2022 c \u2208 (0, 1) - the probability of a null target win; \u2022 \u03b1 \u2208 (0, 1) - the FDR threshold; Output: A discovery list R 1: sort the paired (Wi, Li) in decreasing order of Wi \u25b7 ties are randomly broken 2: Dk \u2190 #{i \u2264 k : Li = \u22121} \u25b7 number of decoy wins in top k scores 3: Tk \u2190 #{i \u2264 k : Li = 1} \u25b7 number of target wins in top k scores 4: if SeqStep then 5: k0 \u2190 max{k : DkTk\u22281 \u00b7 c 1\u2212c \u2264 \u03b1}\n6: else if SeqStep+ then 7: k0 \u2190 max{k : Dk+1Tk\u22281 \u00b7 c 1\u2212c \u2264 \u03b1} 8: end if 9: return R\u2190 {i : the pre-sorted Wi is in the top k0 ranks when sorted}\nAlgorithm 2 RESET in feature-augmented TDC\nInput: \u2022 {(STi , SDi ,xTi ,xDi ) : i = 1, . . . ,m} - the list of target scores, decoy scores, target features and decoy features; \u2022 \u03b1 - FDR threshold for the discovery list; \u2022 s - the probability of assigning a decoy to the training set (default: s = 1/2); \u2022 S : R2 \u2192 R - a symmetric score function such that S(x, y) = S(y, x); \u2022 f - a semi-supervised machine learning model; \u2022 RESET+ - a boolean indicating whether the two decoy version of RESET is used; Output: A discovery list R 1: for i = 1, . . . ,m do \u25b7 if STi = S D i , randomly break ties\n2: if STi > S D i then 3: (Wi,xi, Li)\u2190 (S(W Ti ,WDi ),xTi , 1) \u25b7 e.g. S(x, y) := x \u2228 y or S(x, y) := |x\u2212 y| 4: else 5: (Wi,xi, Li)\u2190 (S(W Ti ,WDi ),xDi ,\u22121) 6: end if 7: end for 8: I \u2190 a subset of the decoy win indices, {i : Li = \u22121}, determined by randomly including each\none with probability s 9: L\u0303i \u2190 \u22121 for i \u2208 I \u25b7 training decoys 10: L\u0303i \u2190 1 for i \u2208 J := Ic \u25b7 pseudo-targets 11: (W\u0303i) m i=1 \u2190 f ( (Wi,xi, L\u0303i) m i=1 ) \u25b7 where f is any machine learning model 12: if RESET+ then \u25b7 use the estimating decoys to define the list of discoveries in the two-decoys version 13: R\u2190 SeqStep+((W\u0303i, Li)i\u2208J , c = 13\u22122s , \u03b1) 14: else \u25b7 same for single-decoy 15: R\u2190 SeqStep+((W\u0303i, Li)i\u2208J , c = 12\u2212s , \u03b1) 16: end if 17: return R\nAlgorithm 3 Percolator-RESET (one decoy)/Percolator-RESET+ (two decoys)\nInput: \u2022 {(Wi,xi, Li) : i = 1, . . . ,m} - the list of winning scores, features and labels obtained from one of the PSM-and-peptide variants for Percolator-RESET (Algorithm 4a-4c) or for Percolator-RESET+ (Algorithm 5a-5c); \u2022 \u03b1 - FDR threshold for the discovery list; \u2022 \u03b11 - FDR threshold for training (default: \u03b11 = 0.01); \u2022 total iter - the number of SVM iterations (default: total iter = 5); \u2022 s - the probability of assigning a decoy to the training set (default: s = 1/2); \u2022 RESET+ - a boolean indicating whether the two decoy version of RESET is used; Output: A discovery list R 1: I \u2190 a subset of the decoy win indices, {i : Li = \u22121}, determined by randomly including each\none with probability s 2: L\u0303i \u2190 \u22121 for i \u2208 I \u25b7 training decoys 3: L\u0303i \u2190 1 for i \u2208 J := Ic \u25b7 pseudo-targets 4: W\u0303i \u2190Wi \u25b7 the original winning scores are the initial SVM scores 5: if RESET+ then 6: ct \u2190 3\u22122s3 7: else 8: ct \u2190 2\u2212s2 9: end if 10: for iter \u2208 {1, . . . , total iter} do 11: T+ \u2190 SeqStep+((W\u0303i, L\u0303i)i\u2208J , ct, \u03b11) \u25b7 indices of positive training set determined by\nAlgorithm 1 12: if T+ = \u2205 then 13: T+ \u2190 SeqStep((W\u0303i, L\u0303i)i\u2208J , ct, \u03b11) \u25b7 try to drop the +1 penalty if set is empty 14: end if 15: \u03b7 \u2190 0.005 16: while T+ = \u2205 do 17: T+ \u2190 SeqStep((W\u0303i, L\u0303i)i\u2208J , ct, \u03b11 + \u03b7) 18: \u03b7 \u2190 \u03b7 + 0.005 \u25b7 keep relaxing the FDR threshold until you get a non-empty set 19: end while 20: T\u2212 \u2190 I \u25b7 indices of negative training set 21: (W\u0303i) m i=1 \u2190 applySVM((Wi,xi, L\u0303i)mi=1, T+, T\u2212) \u25b7 apply SVM to rescore all peptides 22: end for 23: if RESET+ then 24: ce \u2190 13\u22122s 25: else 26: ce \u2190 12\u2212s 27: end if 28: R\u2190 SeqStep+((W\u0303i, Li)i\u2208J , ce, \u03b1) \u25b7 use the estimating decoys to define the list of discoveries 29: return R\nAlgorithm 4a PSM-and-peptide adapted from [17] allowing it to pass through extra features (unused here) and omitting its final step of applying SeqStep+ (Algorithm 1).\nInput: \u2022 (T ,D) - the target and decoy databases; \u2022 {(\u03c3, \u03c0\u03c3T , \u03c0\u03c3D, S\u03c3T , S\u03c3D,x\u03c3T ,x\u03c3D) : \u03c3 \u2208 \u03a3} - the set of experimental spectra, their target and decoy matches, their associated scores, and their vector of features; \u2022 TD pairs - a mapping that pairs every target peptide to its associated decoy; Output: WP = {(\u03c0i,Wi,xi, Li) : i = 1, . . . ,m} - a list of winning peptides, scores, features and labels (Li = \u00b11 indicating a target/decoy win)\n1: for \u03c3 \u2208 \u03a3 do \u25b7 PSM-level competition 2: if S\u03c3T > S \u03c3 D then \u25b7 if S \u03c3 T = S \u03c3 D, randomly break ties 3: (\u03c0\u03c3, S\u03c3,x\u03c3)\u2190 (\u03c0\u03c3T , S\u03c3T ,x\u03c3T ) 4: else 5: (\u03c0\u03c3, S\u03c3,x\u03c3)\u2190 (\u03c0\u03c3D, S\u03c3D,x\u03c3D) 6: end if 7: end for 8: for \u03c0 \u2208 T \u222a D do \u25b7 obtaining the best score associated for each peptide \u03c0 9: S\u03c0 \u2190 max{S\u03c3 | \u03c0\u03c3 = \u03c0} \u25b7 where max(\u2205) = \u2212\u221e 10: x\u03c0 \u2190 randomly select from {x\u03c3 | \u03c0\u03c3 = \u03c0, S\u03c3 = S\u03c0} \u25b7 NULL if S\u03c0 = \u2212\u221e 11: end for 12: WP \u2190 \u2205 \u25b7 a list characterizing the winning target/decoy peptides 13: for \u03c0 \u2208 T do \u25b7 peptide-level competition 14: if S\u03c0 > STD pairs(\u03c0) then \u25b7 if S\u03c0 = STD pairs(\u03c0), randomly break ties 15: WP \u2190WP \u222a (\u03c0, S\u03c0,x\u03c0, 1) \u25b7 add a target winning peptide 16: else 17: WP \u2190WP \u222a (TD pairs(\u03c0), STD pairs(\u03c0),xTD pairs(\u03c0),\u22121) \u25b7 add a decoy winning peptide 18: end if 19: end for 20: return WP\nAlgorithm 4b PSM-and-peptide adapted for variable modifications in narrow search\nInput: \u2022 (T s,Ds) - the target and decoy databases containing only the stem form peptides; \u2022 {(\u03c3, \u03c0\u03c3T , \u03c0\u03c3D, S\u03c3T , S\u03c3D,x\u03c3T ,x\u03c3D) : \u03c3 \u2208 \u03a3} - the set of experimental spectra, their target and decoy matches, their associated scores and their vector of features; \u2022 TD pairs - a mapping that pairs every target peptide to its associated decoy; \u2022 remove mods - a mapping that takes a peptide and returns its stem form; \u2022 sum mods - a mapping that takes a peptide and returns the total mass from each modification; Output: WS = {(\u03c0i,Wi,xi, Li) : i = 1, . . . ,m} - a list of winning (representative) peptides, scores, features and labels (Li = \u00b11 indicating a target/decoy win)\n1: for \u03c3 \u2208 \u03a3 do \u25b7 PSM-level competition 2: if S\u03c3T > S \u03c3 D then \u25b7 if S \u03c3 T = S \u03c3 D, randomly break ties 3: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3, \u03b4\u03c3,x\u03c3)\u2190 (\u03c0\u03c3T , S\u03c3T , remove mods(\u03c0\u03c3T ), sum mods(\u03c0\u03c3T ),x\u03c3T ) 4: else 5: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3, \u03b4\u03c3,x\u03c3)\u2190 (\u03c0\u03c3D, S\u03c3D, remove mods(\u03c0\u03c3D), sum mods(\u03c0\u03c3D),x\u03c3D) 6: end if 7: end for 8: for \u03c0\u030a \u2208 T s do 9: M\u03c0\u030a \u2190 {\u03b4\u03c3 | \u03c0\u030a\u03c3 = \u03c0\u030a or \u03c0\u030a\u03c3 = TD pairs(\u030a\u03c0)} 10: MTD pairs(\u030a\u03c0) \u2190M\u03c0\u030a 11: end for 12: for \u03c0\u030a \u2208 T s \u222a Ds do \u25b7 obtaining the best score with each stem peptide \u03c0\u030a and mass mod \u03b4 13: for \u03b4 \u2208M\u03c0\u030a do 14: S\u03c0\u030a,\u03b4 \u2190 max{S\u03c3 | \u03c0\u030a\u03c3 = \u03c0\u030a, \u03b4\u03c3 = \u03b4} \u25b7 where max(\u2205) = \u2212\u221e 15: \u03c0\u03c0\u030a,\u03b4 \u2190 randomly select from {\u03c0\u03c3 | S\u03c3 = S\u03c0\u030a,\u03b4, \u03c0\u030a\u03c3 = \u03c0\u030a, \u03b4\u03c3 = \u03b4} \u25b7 NULL if S\u03c0\u030a,\u03b4 = \u2212\u221e 16: x\u03c0\u030a,\u03b4 \u2190 randomly select from {x\u03c3 | S\u03c3 = S\u03c0\u030a,\u03b4, \u03c0\u030a\u03c3 = \u03c0\u03c0\u030a,\u03b4, \u03b4\u03c3 = \u03b4} \u25b7 NULL if S\u03c0\u030a,\u03b4 = \u2212\u221e 17: end for 18: end for 19: WS \u2190 \u2205 \u25b7 tuples characterizing the winning target/decoy stems 20: for \u03c0\u030a \u2208 T s do \u25b7 stem-level (w/modification mass) competition 21: for \u03b4 \u2208M\u03c0\u030a do 22: if S\u03c0\u030a,\u03b4 > STD pairs(\u030a\u03c0),\u03b4 then \u25b7 if S\u03c0\u030a,\u03b4 = STD pairs(\u030a\u03c0),\u03b4, randomly break ties 23: WS \u2190WS \u222a (\u03c0\u03c0\u030a,\u03b4, S\u03c0\u030a,\u03b4,x\u03c0\u030a,\u03b4, 1) \u25b7 add a target winning representative peptide 24: else 25: WS \u2190WS \u222a (\u03c0TD pairs(\u030a\u03c0),\u03b4, STD pairs(\u030a\u03c0),\u03b4,xTD pairs(\u030a\u03c0),\u03b4,\u22121) \u25b7 add a decoy winning\nrepresentative peptide 26: end if 27: end for 28: end for 29: return WS\nAlgorithm 4c PSM-and-peptide adapted for variable modifications in open search\nInput: \u2022 (T s,Ds) - the target and decoy databases containing only the stem form peptides; \u2022 {(\u03c3, \u03c0\u03c3T , \u03c0\u03c3D, S\u03c3T , S\u03c3D,x\u03c3T ,x\u03c3D) : \u03c3 \u2208 \u03a3} - the set of experimental spectra, their target and decoy matches, their associated scores and their vector of features; \u2022 TD pairs - a mapping that pairs every target peptide to its associated decoy; \u2022 remove mods - a mapping that takes a peptide and returns its stem form; Output: WS = {(\u03c0i,Wi,xi, Li) : i = 1, . . . ,m} - a list of winning (representative) peptides, scores, features and labels (Li = \u00b11 indicating a target/decoy win)\n1: for \u03c3 \u2208 \u03a3 do \u25b7 PSM-level competition 2: if S\u03c3T > S \u03c3 D then \u25b7 if S \u03c3 T = S \u03c3 D, randomly break ties 3: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3,x\u03c3)\u2190 (\u03c0\u03c3T , S\u03c3T , remove mods(\u03c0\u03c3T ),x\u03c3T ) 4: else 5: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3,x\u03c3)\u2190 (\u03c0\u03c3D, S\u03c3D, remove mods(\u03c0\u03c3D),x\u03c3D) 6: end if 7: end for 8: for \u03c0\u030a \u2208 T s \u222a Ds do \u25b7 obtaining the best score for each stem peptide \u03c0\u030a 9: S\u03c0\u030a \u2190 max{S\u03c3 | \u03c0\u030a\u03c3 = \u03c0\u030a} \u25b7 where max(\u2205) = \u2212\u221e 10: \u03c0\u03c0\u030a \u2190 randomly select from {\u03c0\u03c3 | S\u03c3 = S\u03c0\u030a, \u03c0\u030a\u03c3 = \u03c0\u030a} \u25b7 NULL if S\u03c0\u030a = \u2212\u221e 11: x\u03c0\u030a \u2190 randomly select from {x\u03c3 | S\u03c3 = S\u03c0\u030a, \u03c0\u030a\u03c3 = \u03c0\u03c0\u030a} \u25b7 NULL if S\u03c0\u030a = \u2212\u221e 12: end for 13: WS \u2190 \u2205 \u25b7 tuples characterizing the winning target/decoy stems 14: for \u03c0\u030a \u2208 T s do \u25b7 stem-level competition 15: if S\u03c0\u030a > STD pairs(\u030a\u03c0) then \u25b7 if S\u03c0\u030a = STD pairs(\u030a\u03c0), randomly break ties 16: WS \u2190WS \u222a (\u03c0\u03c0\u030a, S\u03c0\u030a,x\u03c0\u030a, 1) \u25b7 add a target winning representative peptide 17: else 18: WS \u2190WS \u222a (\u03c0TD pairs(\u030a\u03c0), STD pairs(\u030a\u03c0),xTD pairs(\u030a\u03c0),\u22121) \u25b7 add a decoy representative\npeptide 19: end if 20: end for 21: return WS\nAlgorithm 5a PSM-and-peptide+\nInput: \u2022 (T ,D1,D2) - the target and two decoy databases; \u2022 {(\u03c3, \u03c0\u03c3T , {\u03c0 \u03c3,i D }2i=1, {S \u03c3,i T }2i=1, {S \u03c3,i D }2i=1,x\u03c3T , {x \u03c3,i D }2i=1) : \u03c3 \u2208 \u03a3} - the set of ex-\nperimental spectra, their target and decoy matches, their associated (Tailor) scores and their vector of features; \u2022 {TD matchi}2i=1 - a set of mappings that takes every target peptide to its associated ith decoy;\nOutput: WP = {(\u03c0i,Wi,xi, Li) : i = 1, . . . ,m} - a list of winning peptides, scores, features and labels (Li = \u00b11 indicating a target/decoy win)\n1: for \u03c3 \u2208 \u03a3 do \u25b7 PSM-level competition 2: S\u03c3T \u2190 1 2 ( S\u03c3,1T + S \u03c3,2 T ) \u25b7 average the two (Tailor) target scores 3: if S\u03c3T > S \u03c3,i D for i = 1, 2 then \u25b7 if any S \u03c3 T = S \u03c3,i D , randomly break ties 4: (\u03c0\u03c3, S\u03c3,x\u03c3)\u2190 (\u03c0\u03c3T , S\u03c3T ,x\u03c3T ) 5: else 6: i0 \u2190 argmaxi S \u03c3,i D \u25b7 if S \u03c3,1 D = S \u03c3,2 D , randomly break ties 7: (\u03c0\u03c3, S\u03c3,x\u03c3)\u2190 (\u03c0\u03c3,i0D , S \u03c3,i0 D ,x \u03c3,i0 D ) 8: end if 9: end for 10: for \u03c0 \u2208 T \u222a D1 \u222a D2 do \u25b7 obtaining the best score associated for each peptide \u03c0 11: S\u03c0 \u2190 max{S\u03c3 | \u03c0\u03c3 = \u03c0} \u25b7 where max(\u2205) = \u2212\u221e 12: x\u03c0 \u2190 randomly select from {x\u03c3 | \u03c0\u03c3 = \u03c0, S\u03c3 = S\u03c0} \u25b7 NULL if S\u03c0 = \u2212\u221e 13: end for 14: WP \u2190 \u2205 \u25b7 tuples characterizing the winning target/decoy peptides 15: for \u03c0 \u2208 T do \u25b7 peptide-level competition 16: if S\u03c0 > STD matchi(\u03c0) for i = 1, 2 then \u25b7 if any S\u03c0 = STD matchi(\u03c0), randomly break ties 17: WP \u2190WP \u222a (\u03c0, S\u03c0,x\u03c0, 1) \u25b7 add a target winning peptide 18: else 19: i0 \u2190 argmaxi STD matchi(\u03c0) \u25b7 randomly break ties 20: WP \u2190WP \u222a (TD matchi0(\u03c0), STD matchi0 (\u03c0),xTD matchi0 (\u03c0),\u22121) \u25b7 add a decoy winning\npeptide 21: end if 22: end for 23: return WP\nAlgorithm 5b PSM-and-peptide+ for variable modifications in narrow search\nInput: \u2022 (T s,Ds1,Ds2) - the target and two decoy databases containing only the stem form peptides;\n\u2022 {(\u03c3, \u03c0\u03c3T , {\u03c0 \u03c3,i D }2i=1, {S \u03c3,i T }2i=1, {S \u03c3,i D }2i=1,x\u03c3T , {x \u03c3,i D }2i=1) : \u03c3 \u2208 \u03a3} - the set of experimental spectra, their target and decoy matches, their associated (Tailor) scores and their vector of features; \u2022 {TD matchi}2i=1 - a set of mappings that maps every target peptide to its associated ith decoy; \u2022 remove mods - a mapping that takes a peptide and returns its stem; \u2022 sum mods - a mapping that takes a peptide and returns the total mass from each modification;\nOutput: WS = {(\u03c0i,Wi,xi, Li) : i = 1, . . . ,m} - a list of winning (representative) peptides, scores, features and labels (Li = \u00b11 indicating a target/decoy win)\n1: for \u03c3 \u2208 \u03a3 do \u25b7 PSM-level competition 2: S\u03c3T \u2190 1 2 ( S\u03c3,1T + S \u03c3,2 T ) \u25b7 average the two (Tailor) target scores 3: if S\u03c3T > S \u03c3,i D for i = 1, 2 then \u25b7 if any S \u03c3 T = S \u03c3,i D , randomly break ties 4: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3, \u03b4\u03c3,x\u03c3)\u2190 (\u03c0\u03c3T , S\u03c3T , remove mods(\u03c0\u03c3T ), sum mods(\u03c0\u03c3T ),x\u03c3T ) 5: else 6: i0 \u2190 argmaxi S \u03c3,i D \u25b7 if S \u03c3,1 D = S \u03c3,2 D , randomly break ties 7: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3, \u03b4\u03c3,x\u03c3)\u2190 (\u03c0\u03c3,i0D , S \u03c3,i0 D , remove mods(\u03c0 \u03c3,i0 D ), sum mods(\u03c0 \u03c3,i0 D ),x \u03c3,i0 D ) 8: end if 9: end for 10: for \u03c0\u030a \u2208 T s do 11: M\u03c0\u030a \u2190 {\u03b4\u03c3 | \u03c0\u030a\u03c3 = \u03c0\u030a or \u03c0\u030a\u03c3 = TD pairs1(\u030a\u03c0) or \u03c0\u030a\u03c3 = TD pairs2(\u030a\u03c0)} 12: MTD pairsi (\u030a\u03c0) \u2190M\u03c0\u030a for i = 1, 2 13: end for 14: for \u03c0\u030a \u2208 T s \u222a Ds1 \u222a Ds2 do \u25b7 obtaining the best score for each stem \u03c0\u030a and mass mod \u03b4 15: for \u03b4 \u2208M\u03c0\u030a do 16: S\u03c0\u030a,\u00b5 \u2190 max{S\u03c3 | \u03c0\u030a\u03c3 = \u03c0\u030a, \u03b4\u03c3 = \u03b4} \u25b7 where max(\u2205) = \u2212\u221e 17: \u03c0\u03c0\u030a,\u03b4 \u2190 randomly select from {\u03c0\u03c3 | S\u03c3 = S\u03c0\u030a,\u03b4, \u03c0\u030a\u03c3 = \u03c0\u030a, \u03b4\u03c3 = \u03b4} \u25b7 NULL if S\u03c0\u030a,\u03b4 = \u2212\u221e 18: x\u03c0\u030a,\u03b4 \u2190 randomly select from {x\u03c3 | S\u03c3 = S\u03c0\u030a,\u03b4, \u03c0\u030a\u03c3 = \u03c0\u03c0\u030a,\u03b4, \u03b4\u03c3 = \u03b4} \u25b7 NULL if S\u03c0\u030a,\u03b4 = \u2212\u221e 19: end for 20: end for 21: WS \u2190 \u2205 \u25b7 tuples characterizing the winning target/decoy stems 22: for \u03c0\u030a \u2208 T s do \u25b7 stem-level (w/modification mass) competition 23: for \u03b4 \u2208M\u03c0\u030a do 24: if S\u03c0\u030a,\u03b4 > STD matchi (\u030a\u03c0),\u03b4 for i = 1, 2 then \u25b7 if any S\u03c0\u030a,\u03b4 = STD matchi (\u030a\u03c0),\u03b4, randomly break\nties 25: WS \u2190WS \u222a (\u03c0\u03c0\u030a,\u03b4, S\u03c0\u030a,\u03b4,x\u03c0\u030a,\u03b4, 1) \u25b7 add a target winning representative peptide 26: else 27: i0 \u2190 argmaxi STD matchi (\u030a\u03c0),\u03b4 \u25b7 randomly break ties 28: WS \u2190WS \u222a (\u03c0TD matchi0 (\u030a\u03c0),\u03b4, STD matchi0 (\u030a\u03c0),\u03b4,xTD matchi0 (\u030a\u03c0),\u03b4,\u22121) \u25b7 add a decoy\nwinning representative peptide 29: end if 30: end for 31: end for 32: return WS\nAlgorithm 5c PSM-and-peptide+ for variable modifications in open search\nInput: \u2022 (T s,Ds1,Ds2) - the target and two decoy databases containing only the stem form peptides;\n\u2022 {(\u03c3, \u03c0\u03c3T , {\u03c0 \u03c3,i D }2i=1, {S \u03c3,i T }2i=1, {S \u03c3,i D }2i=1,x\u03c3T , {x \u03c3,i D }2i=1) : \u03c3 \u2208 \u03a3} - the set of experimental spectra, their target and decoy matches, their associated (Tailor) scores and their vector of features; \u2022 {TD matchi}2i=1 - a set of mappings that takes every target peptide to its associated ith decoy; \u2022 remove mods - a mapping that takes a peptide and returns its stem;\nOutput: WS - a list of winning representative peptides, scores, features and labels 1: for \u03c3 \u2208 \u03a3 do \u25b7 PSM-level competition 2: S\u03c3T \u2190 1 2 ( S\u03c3,1T + S \u03c3,2 T ) \u25b7 average the two (Tailor) target scores\n3: if S\u03c3T > S \u03c3,i D for i = 1, 2 then \u25b7 if any S \u03c3 T = S \u03c3,i D , randomly break ties 4: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3,x\u03c3)\u2190 (\u03c0\u03c3T , S\u03c3T , remove mods(\u03c0\u03c3T ),x\u03c3T ) 5: else 6: i0 \u2190 argmaxi S \u03c3,i D \u25b7 if S \u03c3,1 D = S \u03c3,2 D , randomly break ties 7: (\u03c0\u03c3, S\u03c3, \u03c0\u030a\u03c3,x\u03c3)\u2190 (\u03c0\u03c3,i0D , S \u03c3,i0 D , remove mods(\u03c0 \u03c3,i0 D ),x \u03c3,i0 D ) 8: end if 9: end for 10: for \u03c0\u030a \u2208 T s \u222a Ds1 \u222a Ds2 do \u25b7 obtaining the best score for each stem peptide \u03c0\u030a 11: S\u03c0\u030a \u2190 max{S\u03c3 | \u03c0\u030a\u03c3 = \u03c0\u030a} \u25b7 where max(\u2205) = \u2212\u221e 12: \u03c0\u03c0\u030a \u2190 randomly select from {\u03c0\u03c3 | S\u03c3 = S\u03c0\u030a, \u03c0\u030a\u03c3 = \u03c0\u030a} \u25b7 NULL if S\u03c0\u030a = \u2212\u221e 13: x\u03c0\u030a \u2190 randomly select from {x\u03c3 | S\u03c3 = S\u03c0\u030a, \u03c0\u030a\u03c3 = \u03c0\u03c0\u030a} \u25b7 NULL if S\u03c0\u030a = \u2212\u221e 14: end for 15: WS \u2190 \u2205 \u25b7 tuples characterizing the winning target/decoy stems 16: for \u03c0\u030a \u2208 T s do \u25b7 stem-level competition 17: if S\u03c0\u030a > STD matchi (\u030a\u03c0) for i = 1, 2 then \u25b7 if any S\u03c0\u030a = STD matchi (\u030a\u03c0), randomly break ties 18: WS \u2190WS \u222a (\u03c0\u03c0\u030a, S\u03c0\u030a,x\u03c0\u030a, 1) \u25b7 add a target winning stem 19: else 20: i0 \u2190 argmaxi STD matchi (\u030a\u03c0) \u25b7 randomly break ties 21: WS \u2190WS \u222a (\u03c0TD matchi0 (\u030a\u03c0), STD matchi0 (\u030a\u03c0),xTD matchi0 (\u030a\u03c0),\u22121) \u25b7 add a decoy winning stem 22: end if 23: end for 24: return WS\nAlgorithm 6 Precursor mass clustering (precursor clustering)\nInput: \u2022 {(\u03c1i, ci) : i = 1, . . . , n} - a set of precursor masses and charge states associated with a given stem peptide; \u2022 (wl, wu) - the experimental isolation window given as a pair of values; Output: {Pj} - the clustered precursors; 1: Reorder the precursors so that \u03c11 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c1n 2: j \u2190 1 3: Pj \u2190 {1} 4: for i \u2208 {2, . . . , n} do 5: if \u03c1i \u2264 \u03c1i\u22121 +max{ci\u22121wu, ciwl} then 6: Pj \u2190 Pj \u222a {i} 7: else 8: j \u2190 j + 1 9: Pj \u2190 {i} 10: end if 11: end for 12: return {Pj}\nAlgorithm 7 Generating the auxiliary list of target PSMs with variants of the discovered peptides\nInput: \u2022 {\u03c0\u2217i : i = 1, . . . ,M} - the set of peptide discoveries; \u2022 {(\u03c3i, \u03c0i, \u03c1i, ci, Si, S\u0303i) : i = 1, . . . ,m} - a set of tuples characterizing the m target PSMs where \u03c3i =spectrum, \u03c0i = peptide, \u03c1i =precursor mass, ci =precursor charge, Si =PSM score, S\u0303i =SVM score; \u2022 (wl, wu) - the experimental isolation window given as a pair of values; \u2022 S - the score threshold used to define the FDR-controlled list of discovered peptides; Output: \u2022 {(\u03c3i, \u03c0i, \u03c1i, ci, Si, S\u0303i)}i\u2208J - the augmented list of PSMs to be reported; \u2022 {ai : i \u2208 J} - labels indicating if the corresponding PSM is greater than or equal to the SVM score threshold (ai = 1) or if it is below (ai = 0);\n1: ai \u2190 0 for all i = 1, . . . ,m 2: J \u2190 \u2205 \u25b7 J is the indices of the PSMs to be returned 3: for \u03c0\u2217 \u2208 {\u03c0\u22171, . . . , \u03c0\u2217M} do 4: I \u2190 {i | \u03c0\u2217 = \u03c0i} \u25b7 all PSMs matched with the discovered peptide \u03c0\u2217 5: Cs\u2190 precursor clustering({(\u03c1i, ci)}i\u2208I , (wl, wu)) \u25b7 applying Algorithm 6 6: for C \u2208 Cs do 7: Sx \u2190 max{Sj | j \u2208 C} \u25b7 maximal score for the considered cluster of precursor masses 8: if Sx > \u2212\u221e then \u25b7 max{\u2205} = \u2212\u221e 9: l\u2190 randomly select from {i | Si = Sx} 10: J \u2190 J \u222a {l} \u25b7 add a maximal scoring PSM of the considered cluster 11: if S\u0303l \u2265 S then 12: al \u2190 1 \u25b7 the SVM-score of this maximal PSM is above the discovery threshold 13: end if 14: end if 15: end for 16: end for 17: return {(\u03c3i, \u03c0i, \u03c1i, ci, Si, S\u0303i, ai)}i\u2208J"
        }
    ],
    "title": "How to train a post-processor for tandem mass spectrometry proteomics database search while maintaining control of the false discovery rate",
    "year": 2023
}