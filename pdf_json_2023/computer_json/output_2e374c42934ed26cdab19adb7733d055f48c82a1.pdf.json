{
    "abstractText": "We explore the efficacy of multimodal behavioral cues for explainable prediction of personality and interview-specific traits. We utilize elementary head-motion units named kinemes, atomic facial movements termed action units and speech features to estimate these human-centered traits. Empirical results confirm that kinemes and action units enable discovery of multiple trait-specific behaviors while also enabling explainability in support of the predictions. For fusing cues, we explore decision and feature-level fusion, and an additive attention-based fusion strategy which quantifies the relative importance of the three modalities for trait prediction. Examining various long-short term memory (LSTM) architectures for classification and regression on the MIT Interview and First Impressions Candidate Screening (FICS) datasets, we note that: (1) Multimodal approaches outperform unimodal counterparts; (2) Efficient trait predictions and plausible explanations are achieved with both unimodal and multimodal approaches, and (3) Following the thin-slice approach, effective trait prediction is achieved even from two-second behavioral snippets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Surbhi Madan"
        },
        {
            "affiliations": [],
            "name": "Monika Gahalawat"
        }
    ],
    "id": "SP:471ebfc9a07f20c676a7f8362d68c12fdda9089c",
    "references": [
        {
            "authors": [
                "J.C.S.J. Junior",
                "Y. G\u00fc\u00e7l\u00fct\u00fcrk",
                "M. P\u00e9rez",
                "U. G\u00fc\u00e7l\u00fc",
                "C. Andujar",
                "X. Bar\u00f3",
                "H.J. Escalante",
                "I. Guyon",
                "M.A. Van Gerven",
                "R. Van Lier"
            ],
            "title": "First impressions: A survey on vision-based apparent personality trait analysis",
            "venue": "IEEE Transactions on Affective Computing, 2019. 11",
            "year": 2019
        },
        {
            "authors": [
                "A. Vinciarelli",
                "G. Mohammadi"
            ],
            "title": "A survey of personality computing",
            "venue": "IEEE Transactions on Affective Computing, vol. 5, no. 3, pp. 273\u2013291, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "R.R. McCrae",
                "P.T. Costa"
            ],
            "title": "Validation of the five-factor model of personality across instruments and observers.",
            "venue": "Journal of personality and social psychology,",
            "year": 1987
        },
        {
            "authors": [
                "J.M. Digman"
            ],
            "title": "Higher-order factors of the big five.",
            "venue": "Journal of personality and social psychology,",
            "year": 1997
        },
        {
            "authors": [
                "H.E. Cattell",
                "A.D. Mead"
            ],
            "title": "The sixteen personality factor questionnaire (16pf).",
            "year": 2008
        },
        {
            "authors": [
                "S. Raza",
                "B. Carpenter"
            ],
            "title": "A model of hiring decisions in real employment interviews.",
            "venue": "Journal of Applied Psychology,",
            "year": 1987
        },
        {
            "authors": [
                "L. Batrinca",
                "N. Mana",
                "B. Lepri",
                "F. Pianesi",
                "N. Sebe"
            ],
            "title": "Please, tell me about yourself: Automatic personality assessment using short self-presentations",
            "venue": "11 2011, pp. 255\u2013262.",
            "year": 2011
        },
        {
            "authors": [
                "K. Van Dam"
            ],
            "title": "Trait perception in the employment interview: A five\u2013factor model perspective",
            "venue": "International Journal of Selection and Assessment, vol. 11, no. 1, pp. 43\u201355, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "T. DeGroot",
                "J. Gooty"
            ],
            "title": "Trait perception in the employment interview: A five\u2013factor model perspective",
            "venue": "Journal of Business and Psychology, vol. 24, no. 2, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "A. Samanta",
                "T. Guha"
            ],
            "title": "On the role of head motion in affective expression",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 2886\u20132890.",
            "year": 2017
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Emotion sensing from head motion capture",
            "venue": "IEEE Sensors Journal, vol. 21, no. 4, pp. 5035\u20135043, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Sidorov",
                "S. Ultes",
                "A. Schmitt"
            ],
            "title": "Automatic recognition of personality traits: A multimodal approach",
            "venue": "Proceedings of the 2014 Workshop on Mapping Personality Traits Challenge and Workshop, 2014, pp. 11\u201315.",
            "year": 2014
        },
        {
            "authors": [
                "O. Kampman",
                "E.J. Barezi",
                "D. Bertero",
                "P. Fung"
            ],
            "title": "Investigating audio, visual, and text fusion methods for end-to-end automatic personality prediction",
            "venue": "arXiv preprint arXiv:1805.00705, 2018.",
            "year": 1805
        },
        {
            "authors": [
                "H. Malik",
                "H. Dhillon",
                "R. Goecke",
                "R. Subramanian"
            ],
            "title": "I am empathetic and dutiful, and so will make a good salesman: Characterizing hirability via personality and behavior",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Eddine Bekhouche",
                "F. Dornaika",
                "A. Ouafi",
                "A. Taleb-Ahmed"
            ],
            "title": "Personality traits and job candidate screening via analyzing facial videos",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2017, pp. 10\u201313.",
            "year": 2017
        },
        {
            "authors": [
                "H.J. Escalante",
                "H. Kaya",
                "A.A. Salah",
                "S. Escalera",
                "Y. G\u00fc\u00e7",
                "U. G\u00fc\u00e7l\u00fc",
                "X. Bar\u00f3",
                "I. Guyon",
                "J.C. Jacques",
                "M. Madadi"
            ],
            "title": "Modeling, recognizing, and explaining apparent personality from videos",
            "venue": "IEEE Transactions on Affective Computing, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "I. Naim",
                "M.I. Tanveer",
                "D. Gildea",
                "M.E. Hoque"
            ],
            "title": "Automated analysis and prediction of job interview performance",
            "venue": "IEEE Transactions on Affective Computing, vol. 9, no. 2, pp. 191\u2013204, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Madan",
                "M. Gahalawat",
                "T. Guha",
                "R. Subramanian"
            ],
            "title": "Head matters: Explainable human-centered trait prediction from head motion dynamics",
            "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction, 2021, pp. 435\u2013443.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Gucluturk",
                "U. Guclu",
                "X. Baro",
                "H.J. Escalante",
                "I. Guyon",
                "S. Escalera",
                "M.A.J. van Gerven",
                "R. van Lier"
            ],
            "title": "Multimodal first impression analysis with deep residual networks",
            "venue": "IEEE Trans. Affect. Comput., vol. 9, no. 3, p. 316\u2013329, Jul. 2018. [Online]. Available: https://doi.org/10.1109/TAFFC.2017.2751469",
            "year": 2018
        },
        {
            "authors": [
                "S. Hoppe",
                "T. Loetscher",
                "S.A. Morey",
                "A. Bulling"
            ],
            "title": "Eye movements during everyday behavior predict personality traits",
            "venue": "Frontiers in human neuroscience, p. 105, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J.F. Rauthmann",
                "C.T. Seubert",
                "P. Sachse",
                "M.R. Furtner"
            ],
            "title": "Eyes as windows to the soul: Gazing behavior is related to personality",
            "venue": "Journal of Research in Personality, vol. 46, no. 2, pp. 147\u2013156, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "D.B. Jayagopi",
                "H. Hung",
                "C. Yeo",
                "D. Gatica-Perez"
            ],
            "title": "Modeling dominance in group conversations using nonverbal activity cues",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 3, pp. 501\u2013513, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "R. Subramanian",
                "Y. Yan",
                "J. Staiano",
                "O. Lanz",
                "N. Sebe"
            ],
            "title": "On the relationship between head pose, social attention and personality prediction for unstructured and dynamic group interactions",
            "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction, 2013, pp. 3\u201310.",
            "year": 2013
        },
        {
            "authors": [
                "Y. G\u00fc\u00e7l\u00fct\u00fcrk",
                "U. G\u00fc\u00e7l\u00fc",
                "X. Baro",
                "H.J. Escalante",
                "I. Guyon",
                "S. Escalera",
                "M.A. Van Gerven",
                "R. Van Lier"
            ],
            "title": "Multimodal first impression analysis with deep residual networks",
            "venue": "IEEE Transactions on Affective Computing, vol. 9, no. 3, pp. 316\u2013329, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.W. Moy",
                "K.F. Lam"
            ],
            "title": "Selection criteria and the impact of personality on getting hired",
            "venue": "Personnel Review, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "C. Tay",
                "S. Ang",
                "L. Van Dyne"
            ],
            "title": "Personality, biographical characteristics, and job interview success: a longitudinal study of the mediating effects of interviewing self-efficacy and the moderating effects of internal locus of causality.",
            "venue": "Journal of Applied Psychology,",
            "year": 2006
        },
        {
            "authors": [
                "M.R. Barrick",
                "M.K. Mount"
            ],
            "title": "The big five personality dimensions and job performance: a meta-analysis",
            "venue": "Personnel psychology, vol. 44, no. 1, pp. 1\u201326, 1991.",
            "year": 1991
        },
        {
            "authors": [
                "L. Witt",
                "L.A. Burke",
                "M.R. Barrick",
                "M.K. Mount"
            ],
            "title": "The interactive effects of conscientiousness and agreeableness on job performance.",
            "venue": "Journal of applied psychology,",
            "year": 2002
        },
        {
            "authors": [
                "M.K. Mount",
                "M.R. Barrick",
                "G.L. Stewart"
            ],
            "title": "Five-factor model of personality and performance in jobs involving interpersonal interactions",
            "venue": "Human performance, vol. 11, no. 2-3, pp. 145\u2013165, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "S. Rothmann",
                "E.P. Coetzer"
            ],
            "title": "The big five personality dimensions and job performance",
            "venue": "SA Journal of industrial psychology, vol. 29, no. 1, pp. 68\u201374, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "W. Samek",
                "K.-R. M\u00fcller"
            ],
            "title": "Towards explainable artificial intelligence",
            "venue": "Explainable AI: interpreting, explaining and visualizing deep learning. Springer, 2019, pp. 5\u201322.",
            "year": 2019
        },
        {
            "authors": [
                "A.S. Wicaksana",
                "C.C. Liem"
            ],
            "title": "Human-explainable features for job candidate screening prediction",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE, 2017, pp. 1664\u20131669.",
            "year": 2017
        },
        {
            "authors": [
                "X.-S. Wei",
                "C.-L. Zhang",
                "H. Zhang",
                "J. Wu"
            ],
            "title": "Deep bimodal regression of apparent personality traits from short video sequences",
            "venue": "IEEE Transactions on Affective Computing, vol. 9, no. 3, pp. 303\u2013315, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Ventura",
                "D. Masip",
                "A. Lapedriza"
            ],
            "title": "Interpreting cnn models for apparent personality trait regression",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2017, pp. 55\u201363.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Gucluturk",
                "U. Guclu",
                "M. Perez",
                "H. Jair Escalante",
                "X. Baro",
                "I. Guyon",
                "C. Andujar",
                "J. Jacques Junior",
                "M. Madadi",
                "S. Escalera"
            ],
            "title": "Visualizing apparent personality analysis with deep residual networks",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, 2017, pp. 3101\u20133109.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ding",
                "L. Shi",
                "Z. Deng"
            ],
            "title": "Low-level characterization of expressive head motion through frequency domain analysis",
            "venue": "IEEE Transactions on Affective Computing, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Gunes",
                "M. Pantic"
            ],
            "title": "Dimensional emotion prediction from spontaneous head gestures for interaction with sensitive artificial listeners",
            "venue": "International conference on intelligent virtual agents. Springer, 2010, pp. 371\u2013377.",
            "year": 2010
        },
        {
            "authors": [
                "Z. Yang",
                "S.S. Narayanan"
            ],
            "title": "Modeling dynamics of expressive body gestures in dyadic interactions",
            "venue": "IEEE Transactions on Affective Computing, vol. 8, no. 3, pp. 369\u2013381, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "G. An",
                "R. Levitan"
            ],
            "title": "Lexical and acoustic deep learning model for personality recognition.",
            "venue": "INTERSPEECH,",
            "year": 2018
        },
        {
            "authors": [
                "F. Valente",
                "S. Kim",
                "P. Motlicek"
            ],
            "title": "Annotation and recognition of personality traits in spoken conversations from the ami meetings corpus",
            "venue": "Thirteenth annual conference of the international speech communication association, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "K. Mangalam",
                "T. Guha"
            ],
            "title": "Learning spontaneity to improve emotion recognition in speech",
            "venue": "arXiv preprint arXiv:1712.04753, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Tawari",
                "M.M. Trivedi"
            ],
            "title": "Speech emotion analysis: Exploring the role of context",
            "venue": "IEEE Transactions on multimedia, vol. 12, no. 6, pp. 502\u2013509, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "L. Abdel-Hamid"
            ],
            "title": "Egyptian arabic speech emotion recognition using prosodic, spectral and wavelet features",
            "venue": "Speech Communication, vol. 122, pp. 19\u201330, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.I. Levitan",
                "Y. Levitan",
                "G. An",
                "M. Levine",
                "R. Levitan",
                "A. Rosenberg",
                "J. Hirschberg"
            ],
            "title": "Identifying individual differences in gender, ethnicity, and personality from dialogue for deception detection",
            "venue": "Proceedings of the second workshop on computational approaches to deception detection, 2016, pp. 40\u201344. 12",
            "year": 2016
        },
        {
            "authors": [
                "A. Dhall",
                "J. Hoey"
            ],
            "title": "First impressions-predicting user personality from twitter profile images",
            "venue": "International Workshop on Human Behavior Understanding. Springer, 2016, pp. 148\u2013158.",
            "year": 2016
        },
        {
            "authors": [
                "N. Al Moubayed",
                "Y. Vazquez-Alvarez",
                "A. McKay",
                "A. Vinciarelli"
            ],
            "title": "Face-based automatic personality perception",
            "venue": "Proceedings of the 22nd ACM international conference on Multimedia, 2014, pp. 1153\u20131156.",
            "year": 2014
        },
        {
            "authors": [
                "K.S. Meng",
                "L. Leung"
            ],
            "title": "Factors influencing tiktok engagement behaviors in china: An examination of gratifications sought, narcissism, and the big five personality traits",
            "venue": "Telecommunications Policy, vol. 45, no. 7, p. 102172, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Song",
                "S. Jaiswal",
                "E. Sanchez",
                "G. Tzimiropoulos",
                "L. Shen",
                "M. Valstar"
            ],
            "title": "Self-supervised learning of person-specific facial dynamics for automatic personality recognition",
            "venue": "IEEE Transactions on Affective Computing, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Sharma",
                "T. Guha",
                "G. Sharma"
            ],
            "title": "Multichannel attention network for analyzing visual behavior in public speaking",
            "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018, pp. 476\u2013484.",
            "year": 2018
        },
        {
            "authors": [
                "L. R"
            ],
            "title": "Birdwhistell, Kinesics and context: Essays on body motion communication",
            "venue": "University of Pennsylvania press,",
            "year": 2010
        },
        {
            "authors": [
                "T. Baltru\u0161aitis",
                "P. Robinson",
                "L.-P. Morency"
            ],
            "title": "Openface: an open source facial behavior analysis toolkit",
            "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2016, pp. 1\u201310.",
            "year": 2016
        },
        {
            "authors": [
                "B. Schuller",
                "S. Steidl",
                "A. Batliner"
            ],
            "title": "The interspeech 2009 emotion challenge",
            "venue": "2009.",
            "year": 2009
        },
        {
            "authors": [
                "S. Koelstra",
                "I. Patras"
            ],
            "title": "Fusion of facial expressions and eeg for implicit affective tagging",
            "venue": "Image and Vision Computing, vol. 31, no. 2, pp. 164\u2013174, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Koppensteiner"
            ],
            "title": "Motion cues that make an impression: Predicting perceived personality by minimal motion information",
            "venue": "Journal of Experimental Social Psychology, vol. 49, no. 6, pp. 1137\u2013 1143, 2013. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S0022103113001467",
            "year": 2013
        },
        {
            "authors": [
                "R. Ishii",
                "C. Ahuja",
                "Y.I. Nakano",
                "L.-P. Morency"
            ],
            "title": "Impact of personality on nonverbal behavior generation",
            "venue": "ACM International Conference on Intelligent Virtual Agents, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Osugi",
                "J.I. Kawahara"
            ],
            "title": "Effects of head nodding and shaking motions on perceptions of likeability and approachability",
            "venue": "Perception, vol. 47, no. 1, pp. 16\u201329, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Lepri",
                "R. Subramanian",
                "K. Kalimeri",
                "J. Staiano",
                "F. Pianesi",
                "N. Sebe"
            ],
            "title": "Connecting meeting behavior with extraversion\u2014a systematic study",
            "venue": "IEEE Transactions on Affective Computing, vol. 3, no. 4, pp. 443\u2013455, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "O. Celiktutan",
                "H. Gunes"
            ],
            "title": "Automatic prediction of impressions in time and across varying context: Personality, attractiveness and likeability",
            "venue": "IEEE transactions on affective computing, vol. 8, no. 1, pp. 29\u201342, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "E. Oberzaucher",
                "K. Grammer"
            ],
            "title": "Everything is movement: on the nature of embodied communication",
            "venue": "Embodied communication in humans and machines, pp. 151\u2013177, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "K. Ruhland",
                "K. Zibrek",
                "R. McDonnell"
            ],
            "title": "Perception of personality through eye gaze of realistic and cartoon models",
            "venue": "Proceedings of the ACM SIGGRAPH Symposium on Applied Perception, 2015, pp. 19\u201323.",
            "year": 2015
        },
        {
            "authors": [
                "S.M. Breil",
                "S. Osterholz",
                "S. Nestler",
                "M.D. Back"
            ],
            "title": "13 contributions of nonverbal cues to the accurate judgment of personality traits",
            "venue": "The Oxford handbook of accurate personality judgment, p. 195, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. DeGroot",
                "D. Kluemper"
            ],
            "title": "Evidence of predictive and incremental validity of personality factors, vocal attractiveness and the situational interview",
            "venue": "International Journal of Selection and Assessment, vol. 15, no. 1, pp. 30\u201339, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "S.P. Levine",
                "R.S. Feldman"
            ],
            "title": "Women and men\u2019s nonverbal behavior and self-monitoring in a job interview setting",
            "venue": "Applied HRM Research, vol. 7, no. 1, pp. 1\u201314, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "S. Walther",
                "F. Ramseyer",
                "H. Horn",
                "W. Strik",
                "W. Tschacher"
            ],
            "title": "Less structured movement patterns predict severity of positive syndrome, excitement, and disorganization",
            "venue": "Schizophrenia bulletin, vol. 40, no. 3, pp. 585\u2013591, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "F. Eyben",
                "F. Weninger",
                "L. Paletta",
                "B.W. Schuller"
            ],
            "title": "The acoustics of eye contact: detecting visual attention from conversational audio cues",
            "venue": "Proceedings of the 6th workshop on Eye gaze in intelligent human machine interaction: gaze in multimodal interaction, 2013, pp. 7\u201312.",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Kinemes, Head-motion Units, Action Units, Behavioral Analytics, Explainable Prediction, Personality and Interview Traits, Unimodal vs Multimodal\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Personality is a psychological construct that describes human behavior in terms of habitual and fairly stable patterns of emotions, thoughts, and attributes [1], [2]. Personality is typically characterized by the OCEAN traits typified by the big-five model [3]: Openness (creative vs conservative), Conscientiousness (diligent vs disorganized), Extraversion (social vs aloof), Agreeableness (empathetic vs distant) and Neuroticism (anxious vs emotionally stable). Other popular personality models include the big-two model which categorizes these five traits into the Plasticity and Stability dimensions [4], and the 16 personality factors model [5].\nPersonality plays a crucial role in shaping an individual\u2019s behavioral and communication traits, and how one conducts themselves in different social situations. To this end, multimodal non-verbal cues are critical in exhibiting an individual\u2019s inter-personal skills in the context of \u2018multimedia CVs\u2019 [6], [7]. Subjective impressions of interviewee\u2019s personality traits can influence hiring decisions [8], and even one behavioral modality can explain personality attributions [9]. E.g., Conscientiousness characterizing diligence and honesty is reflected in an upright posture and minimal head movements, while Neuroticism indicating anxiety and stress is revealed through fidgeting and camera aversion in self-presentation videos [7].\nThis paper builds on the above findings, and explores the efficacy of multimodal behavioral cues to explainably\n\u2022 Surbhi Madan is with Department of Computer Science, Indian Institute of Technology Ropar, India. \u2022 Monika Gahalawat, Roland Goecke and Ramanathan Subramanian are with the Human-Centred Technology Research Centre, Faculty of Science and Technology, University of Canberra, ACT, Australia. \u2022 Tanaya Guha is with the School of Computing Science, University of Glasgow, UK.\npredict personality and job interview traits. In particular, we examine (i) elementary head motions termed kinemes, (ii) atomic facial movements called action units (AUs), and (iii) prosodic and acoustic speech features for traits prediction (see Fig. 1 for an overview). We first evaluate the efficacy of unimodal temporal characteristics of individual behavioral channel in predicting these traits using long-short term memory (LSTM) architectures. Next, we explore different multimodal fusion strategies (feature fusion, decision fusion, and additive soft attention) to enhance each channel\u2019s predictive power and explainability. Recent studies have already shown the effectiveness of kineme patterns for emotional trait prediction [10], [11], while acoustic features\nar X\niv :2\n30 2.\n09 81\n7v 2\n[ cs\n.L G\n] 2\n3 Fe\nb 20\n23\n2 and facial expressions have been successfully employed for estimating personality attributes [1], [12], [13] and candidate hireability (i.e., suitability to hire/interview later) [14], [15].\nExamining various LSTM architectures for classification and regression on the diverse FICS [16] and MIT interview [17] datasets, we make the following observations: (i) Both kinemes and AUs achieve explanative trait prediction. (ii) Multimodal approaches leverage cue-complementarity to better predict interview and personality attributes than unimodal ones. (iii) Trimodal fusion-based attention scores enable behavioral explanations, and provide insights into the relative contribution of each modality over time. (iv) Adequate predictive power is achieved even with 2 secondslong behavioral episodes or slices. Overall, we make the following research contributions:\n\u2022 Building upon our initial results [18], we novelly employ kinemes, action units and speech features for the estimation of personality and interview traits. Given the strong correlations among personality and interview traits [16], [19], we show that the three behavioral modalities are both predictive and explanative of these traits. We explore distinct strategies for temporally fusing behavioral features. Fusion approaches outperform unimodal ones by a large margin owing to the complementary nature of the cues and modalities. \u2022 Our experiments reveal that speech features are highly predictive of interview traits on the MIT dataset [17], and achieve performance comparable to kinemes and AUs for OCEAN trait prediction on the FICS dataset. \u2022 Kineme and AU features enable behavioral explanations to support their predictions. We employ scores obtained from the additive attention fusion model to assess the relative importance of our three modalities per trait. \u2022 We perform ablative studies presenting unimodal and multimodal results over thin-slices of varying lengths. We show that satisfactory continuous and discrete trait prediction performance can be achieved even with 2s slices, with more accurate predictions possible over longer slices in line with expectation."
        },
        {
            "heading": "2 LITERATURE REVIEW",
            "text": "This section reviews research on (a) personality and interview trait prediction, and (b) multimodal behavior analytics to position our work with respect to the literature."
        },
        {
            "heading": "2.1 Trait Prediction",
            "text": "Human thoughts, emotions and behavioral patterns are influenced by their personality, typically characterized via the OCEAN model [3] characterizing human personality in terms of Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. Various non-verbal behavioral cues such as eye movements [20], [21], head motion [22], [23], and facial features [13], [24] have been employed for personality trait prediction.\nNumerous studies have examined the relationship between a candidate\u2019s personality traits and their jobinterview performance [14], [17]; For instance, Conscientiousness is positively correlated with job and organiza-\ntional performance [25], [26]. Conscientiousness and Extraversion impact interview success [27], [28] and job ratings [29]. While Mount et al. [30] observed that Emotional stability, Conscientiousness and Agreeableness are positively related to job performance, Rothmannet al. [31] associated Conscientiousness, Extraversion, Emotional stability and Openness with job performance and creativity. While these correlations among personality and interview traits have been discovered via statistical analyses, very few studies have explored the relationships between non-verbal behavioral cues and personality-cum-interview traits in a predictive (regression/classification) setting.\nExplainable trait prediction: Despite achieving excellent performance on multiple prediction problems, deep learning models fall short in terms of explainability and interpretability due to their \u2018black-box\u2019 nature [32]. Recent studies alleviate this issue by interpreting the results of deep learning models, e.g., Wicaksana and Liem [33] predict OCEAN personality traits explicitly focusing on humanexplainable features and a transparent decision-making process. Wei et al. [34] propose a deep bimodal regression framework, in which Convolutional Neural Networks (CNNs) are modified to aggregate descriptors for improving regression performance on apparent personality analysis. A CNN-based approach for interpretability is explored, where the authors observe a correlation between AUs and CNNlearned features [35]. Another work [36] trains a deep residual network with audiovisual descriptors for personality trait prediction, where predictions are elucidated via face image visualization and occlusion analysis."
        },
        {
            "heading": "2.2 Multimodal Behavior Analytics",
            "text": "Low-level behavioral features have been largely employed for human-centred trait prediction. E.g., head-motion has been modeled with descriptors such as amplitude of Fourier components [37], Euler rotation angles and velocity. Head motion is often restricted to nods and shakes [38]. Yang and Narayanan [39] extract arbitrary head motion patterns, which do not have a physical interpretation. Subramanian et al. [23] predict Extraversion and Neuroticism employing positional and head pose patterns.\nAudio-visual features are typically combined to achieve effective trait prediction. Low-level speech descriptors such as pitch, intensity, spectral, cepstral coefficients and pause duration are commonly used for personality [40], [41] and affect recognition [42], [43], [44]. Other works use acoustic, prosodic and linguistic features for personality prediction [13], [45].\nMany trait prediction studies focus solely on visual cues, with facial cues playing a crucial role. E.g., multivariate regression is employed to infer user personality impressions from Twitter profile images [46], while eigenfaces are combined with Support Vector Machines are used to predict if a depicted person scores above/below the median for each of the big-five traits [47]. Meng et al. [48] investigate the connection between gratification-sought (e.g., escape, fashion, entertainment) and personality traits, and find that extroverts are more active in contributing to, and participating in engaging behaviors. Short-term facial dynamics are learned from short videos via an emotion-guided, encoderbased approach for personality analysis in [49].\n3"
        },
        {
            "heading": "2.3 Summary",
            "text": "Our literature review reveals the following research gaps: (1) Personality and interview traits are known to be highly\ncorrelated based on statistical observations, but few works have explored learning of features that can effectively predict as well as explain these traits. (2) While personality and interview traits have been predicted via machine/deep learning approaches, the majority employs statistics of low-level audiovisual features (statistics relating to head motion, eye-gaze, facial expression, speech and prosodic), which limits explanations to support the predictions. While head motion patterns have been identified as critical nonverbal behavioral cues, they have not been employed for personality or interview trait prediction. We show how kineme and AU features can intuitively explain trait-specific behaviors. (3) Multimodal behavioral analytics have been largely restricted to feature and decision fusion, treating all behavioral channels equally. Differently, we utilize additive soft attention [50]-based fusion that learns relative contribution of each channel from data. This allows for quantifying and explaining the relative contribution of the different modalities towards the prediction result."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 Feature Extraction",
            "text": "We now present feature extraction for the three employed modalities: (i) 3D head motions denoted via a sequence of kinemes, (ii) facial action units describing muscle movements, and (iii) low-level descriptors for speech representation. As in [18], we encode these features into 2s temporal segments with a 50% overlap to obtain feature vectors.\nKineme Representation: A compact approach to modeling head motion is by representing it in terms of a small number of fundamental and interpretable units termed kinemes [10]; they are analogous to phonemes in human speech [51]. We extract the 3D Euler rotation angles pitch (\u03b8p), yaw (\u03b8y) and roll (\u03b8r) per frame to represent head pose using the Openface toolkit [52]. Head motion for a time period T can be represented as a time-series of 3D angles: \u03b8 = {\u03b81:Tp , \u03b81:Ty , \u03b81:Tr }. This multivariate time-series \u03b8 of length T is divided into l-overlapping segments, where the ith segment is denoted by a vector h(i) = [\u03b8i:i+`p , \u03b8 i:i+` y , \u03b8 i:i+` r ]\nT . These overlapping segments enable shift-invariance and generate better representations of the head motion [11].\nFurther, we define the characterization matrix as H\u03b8 = [h(1),h(2), \u00b7 \u00b7 \u00b7 ,h(s)] with s denoting the number of segments in the training sample. All N training samples are combined to form the head motion matrix H = [H\u03b81 |H\u03b82 | \u00b7 \u00b7 \u00b7 |H\u03b8N ], where each column in the matrix H represents a single head motion time-series segment. Nonnegative Matrix Factorization is performed on the matrix H to obtain basis and coefficient matrices B and C respectively. We then employ Gaussian Mixture modeling to cluster coefficient vectors in a low dimensional space to obtain a k column matrix C\u2217 (k << Ns). The matrix C\u2217 is transformed as H\u2217 = BC\u2217, to obtain kinemes in the original space. Columns of H\u2217 yield the k kinemes {Ki}Ki=1.\nOn learning the kineme representation, any head motion time-series is represented viaK by mapping each time series segment to an individual kineme. To obtain the corresponding kineme, we compute the characterization matrix h(i) for the ith segment. Lastly, we project h(i) onto the learned subspace spanned by B to get c(i):\nc\u0302 = arg min c(i)\u22650 \u2016h(i) \u2212Bc(i)\u20162F\nWe maximize the posterior probability P (Ki|c\u0302) to associate the ith segment to its corresponding kineme Ki. Thus, we can map a head motion time-series to a kineme sequence. Selected kinemes are extracted from the MIT and FICS datasets are visualized in Figs. 5 and 6.\nAction Unit Detection: We extract 17 facial action units (AUs) per video frame using Openface. These 17 AUs are described in terms of a value specifying the visibility of an AU, and an intensity score representing AU sharpness on a 5-point scale (minimal to maximal). We employ mean intensity as a threshold to identify the dominant AUs over all 2s frames with 1s overlap as above. We present common AUs from the two datasets in Fig. 7.\nSpeech Feature Extraction: We extracted low-level audio descriptors (LLDs) via the Librosa library [53] following the Interspeech2009 emotion challenge [54]: Fundamental frequency (F0), voice probability, zero-crossing rate (ZCR) and Mel-frequency cepstral coefficients (MFCCs). A local feature vector is created by extracting the LLDs over a sliding window of 93ms with an overlap of 23ms over the entire video duration. These local features are averaged and concatenated to obtain a 23-dimensional feature vector for each 2s segment. For each dataset, these features are normalized to have zero mean and unit variance."
        },
        {
            "heading": "3.2 Models",
            "text": "Long short-term memory (LSTM) models for regression and classification: We trained LSTMs with the kineme (LSTM Kin), AU (LSTM AU) and speech sequences (LSTM Aud). We also performed bimodal feature fusion (FF) and decision fusion (DF) with all combinations (LSTM Kin+AU, LSTM Kin+Aud and LSTM AU+Aud), and trimodal LSTM fusion (LSTM Kin+AU+Aud). The kineme sequences are one-hot encoded, where the kineme denoting a given timewindow is coded to 1 and the rest to 0. AU sequences are encoded by setting the dominant AUs to 1 and rest to 0 for the time-window, creating a binary 17-element AU vector. Speech sequences are created by z-normalizing LLDs averaged over the time-window. For a behavioral slice involving L time windows with N training samples, the kineme, AU and speech features form 3D matrices of size 16\u00d7N \u00d7 L, 17 \u00d7N \u00d7 L, and 23 \u00d7N \u00d7 L respectively. Unimodal and feature fusion (FF): A single hidden LSTM layer is employed for unimodal prediction followed by a dense layer involving one neuron with sigmoidal/linear activation for classification/regression. For bimodal and trimodal feature fusion, unimodal descriptors are fused by applying a single LSTM layer to each feature. The subsequent outputs are merged followed by a dense layer comprising a single neuron as above (see Fig. 3). The hyperparameters\n4\nFig. 2. (a) Additive attention fusion architecture overview, and (b) Attention score computation process (FC layer comprises twelve neurons). N denotes the number of neurons per layer. Linear/sigmoid activation is applied on the dense layer output for regression/classification.\nsuch as number of neurons, activation function and dropout rate are tuned via the validation set. An Adam optimizer is utilized for training with learning rate of 0.01. We employ binary cross entropy and mean absolute error as loss functions for classification and regression respectively. Attention fusion (LSTM AF): To achieve multimodal explanations, we employ attention-based trimodal fusion as in [50] to assign importance weights to the three modalities at each time window (Fig. 2). Dense layers are employed for each cue in [50], while we use one LSTM layer per modality to quantify an importance weight. Also, while we compute weights based on softmax scores generated per time step, [50] focuses only on the channel with maximum attention weight discarding others. As in Fig. 2(a), an LSTM layer is employed for each modality to learn temporal dynamics, resulting in a fixed-length feature vector per modality. Unimodal descriptors are concatenated and passed through a fully connected layer, and a softmax layer composed of three neurons (Fig. 2(b)). Attention scores generated via the softmax layer are deemed as the relative contribution of each modality per time window. Layer normalization is applied over each unimodal feature vector. To fuse normalized features, we employ an additive layer to sum the weighted unimodal features. This is followed by a dense layer comprising a single neuron with sigmoidal/linear activation for classification/regression. We aggregate weights to compute modality contributions over behavioral slices spanning multiple time windows. Decision fusion (DF): We adopt the fusion weight estima-\n5"
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "The FICS dataset [16] contains 10K self-presentation snippets derived from YouTube videos of people talking into the camera. Averaging 15s in length, these videos are split into a 3:1:1 proportion for train, validation and test. All videos are annotated with OCEAN trait scores with \u2018N\u2019\nscores denoting emotional stability instead of Neuroticism. This MIT dataset [17] comprises audio-visual recordings of 138 mock job interviews with 69 undergraduate students, with videos being 4.7 minutes long on average. All videos are annotated with 16 interviewee-specific traits. We focus on the following traits: recommended hiring score (RH) denoting the candidate\u2019s hireability, level of excitement (Ex), friendliness (Fr) and eye-contact (EC). We also examine the Overall (Ov) interview score in prediction experiments. Examples from the two datasets are presented in Figure 4."
        },
        {
            "heading": "4.2 Quantitative Experiments",
            "text": "Prediction Settings: We consider both continuous and discrete prediction of personality and interview traits. For (binary) classification, we dichotomize trait scores by thresholding them at their median value. Tables 1 and 2 present regression results, while Tables 3 and 4 present classification results. Our models are fine-tuned on the FICS dataset via the pre-defined validation set, while hyperparameter tuning is achieved via 10-fold cross-validation (cv) on the smaller MIT Interview dataset. Results reported on the MIT dataset are \u00b5 \u00b1 \u03c3 statistics noted over 50 runs (5 repeated runs of 10-fold cv). Early stopping with a patience value of 4 epochs is employed to prevent model degradation. Chunk vs video-level prediction: To examine trait prediction over tiny behavioral episodes (or slices), we segment the original videos into smaller chunks of 2-7s for FICS, and 2-60s for the MIT dataset. All video chunks are assigned the source video label. We then compute metrics over a) all chunks (chunk-level performance), and b) over all videos by assigning the majority label/mean value over all chunks (video-level performance) for classification/regression. A comparison of chunk vs video-level predictions for the three modalities is presented in Figs. 8- 10 Performance Metrics: Due to the imbalanced class distribution in classification, we use two metrics: Accuracy (Acc) and F1-Score. For regression, accuracy (Acc) defined as 1-MAE (Mean Absolute Error) [24] and PCC (Pearson Correlation Coefficient) are considered."
        },
        {
            "heading": "4.3 Results and Discussion",
            "text": "Based on Tables 1\u20134, we make the following observations:\n\u2022 For regression benchmarking (Tables 1, 2), PCC is a more stringent measure than Acc, as very low PCC values are observed with relatively high Acc values for the FICS dataset (Table 2). Tables 1 and 3 show that regression and classification results are comparable for the (smaller) MIT dataset. For FICS, the regression scores are considerably higher than the classification scores, which can be attributed to Gaussian-distributed FICS traits with means around 0.5 [16]. \u2022 Speech features achieve optimal interview trait prediction (Table 1), while Kineme and AU features perform comparably. Optimal personality trait regression is also achieved with audio features (Table 2), even as AUs significantly outperform kinemes on the FICS dataset. \u2022 Higher PCC scores are achieved with multimodal as compared to unimodal methods on both the MIT and FICS datasets. Bimodal and trimodal fusion perform very similarly for both interview and personality trait\n6\n7\na peak PCC of 0.566 achieved for the Extraversion personality trait on FICS obtained with trimodal fusion. \u2022 Focusing on multimodal methods, bimodal combinations involving audio outperform others for interview trait prediction. Also, feature fusion is more effective than decision fusion in this case. Slightly different trends are noted for the FICS dataset with decision fusion slightly outperforming feature fusion; optimal PCC values are noted for the AU+Aud combination with decision fusion, implying that speech features individually and in combination with others acquire high predictive power, mirroring findings in [17]. Bimodal predictions improving over unimodal ones conveys that kinemes and AUs provide complementary information concerning interview and personality traits. \u2022 Among trimodal fusion methods, decision fusion slightly outperforms attention and feature fusion on the MIT dataset, while decision, attention and feature fusion approaches perform first, second and third best on the FICS dataset. These results again reveal the complementary utility of the kineme, AU and speech features; optimal performance achieved with trimodal decision fusion conveys that the AU and kineme classifiers improve prediction performance in instances where speech descriptors are ineffective. \u2022 Focusing on classification (Tables 3 and 4), considering unimodal results, audio features achieve optimal F1scores on Interview traits (highest F1 of 0.95 for Recommended Hiring and Excited), while AUs achieve the best classification on personality traits (maximum F1 of 0.651 for Extraversion). AUs and kinemes perform similarly on the MIT dataset, while speech descriptors\nmethods in categorizing both interview and personality traits. With respect to bimodal methods, combinations involving speech tend to perform well for both interview and personality prediction. There is little to choose between feature and decision fusion for interview trait prediction, while decision fusion slightly outperforms feature fusion for predicting personality traits. \u2022 Trimodal fusion performs best producing peak F1 scores of 0.98 and 0.695 for the RH interview, and Extraversion personality traits. Decision fusion produces optimal trait classification on both datasets, with feature and attention fusion performing comparably. The above results represent trait prediction at the video level, on examining 15s FICS videos or upon collating classification/regression results over 5\u201360s chunks/segments on the MIT dataset (the best results obtained by averaging chunk-level values, or computing the majority label over all chunks are listed in Tables 1 and 3)."
        },
        {
            "heading": "4.3.1 Thin-slice predictions:",
            "text": "We explore trait prediction over short behavioral episodes known as thin slices and present the multimodal results for classification and regression using soft additive attentionfusion over 2s behavioral slice in Table 5. The results convey that reasonable prediction performance can be achieved even with 2s-long slices expressing the efficacy of these small behavioral slices for predicting different traits. Further, we visualize the comparison of chunk and videolevel prediction performance for varying time-lengths over all three modalities in Figures 8\u201310. It can be noted that better prediction performance has been achieved with videolevel as compared to chunk-level implying that while episodic behaviors may be inconsistent with one another,\n8\nFig. 9. Chunk vs video-level predictions with AUs for FICS (left) and MIT (right). dataset.\nFig. 10. Chunk vs video-level predictions with speech features for FICS (left) and MIT (right). dataset.\ntrait specific behaviors tend to be homogeneous over longer time-span. For the OCEAN traits, kineme-based chunk and video-level PCC values deteriorate over larger time-slices (Fig. 8 (left)) while remaining stable in the case of AU features. The speech features are mostly consistent with different time-slices in case of chunk-level while decreasing slightly for video-level prediction. Conversely, chunk and video-level PCC values increase for all three modalities with increasing time-slice length for the MIT dataset (Fig. 8\u201310 (right)). This trend highlights that AUs, describing facial behavior, encode more trait specific information, specifically for personality traits as compared to kinemes characterizing head movement and speech features. The better prediction with larger time-slices for all modalities over the MIT dataset suggests that interview behavior can be captured better over longer time-span."
        },
        {
            "heading": "5 EXPLAINABILITY & INTERPRETABILITY",
            "text": ""
        },
        {
            "heading": "5.1 Interpretation via kinemes and AUs",
            "text": "Along with their predictive power, kinemes and AUs also enable facile trait-specific behavioral explanations. To this end, we considered the top and bottom 10-percentile videos for each trait, and computed the most frequently occurring AUs and kinemes for the same. The most frequently occurring four kinemes, and five dominant AUs for these high (H) and low (L)-rated videos are presented in Table 6. Analysing the table, we make the following remarks: \u2022 The presence of kineme 16 (denoting head nodding and shaking) in all OCEAN traits conveys the significance of head motion for the characterization of personality traits. Combination of head nodding and shaking with other kineme representations highlights the sub-\n9\nTABLE 6 Explaining OCEAN and interview traits via kinemes and AUs. MIT kinemes in bold font are visualized in Figure 6.\nDataset Trait Dominant Kin Dominant AUs Inferences\nFICS\nO (H) 2, 8, 10, 16 7, 12, 14, 25, 26 Persistent head movements (as noted in [56]) with nodding and smiling. C (H) 1, 8, 10, 16 7, 12, 17, 25, 26 Upward head-tilt indicative of upright demeanor and head nodding. E (H) 2, 10, 14, 16 10, 12, 17, 25, 26 Head tilt-down with nodding, and facial gestures related to speaking. A (H) 3, 8, 10, 16 7, 12, 14, 25, 26 Frequent head nodding and smiling (associated with courteous behavior [57], [58]). N (H) 2, 8, 10, 16 7, 12, 17, 25, 26 Frequent head movements with nodding and smiling. O (L) 1, 6, 11, 16 4, 10, 14, 17, 26 Relatively fewer head movements and frowning. C (L) 2, 4, 8, 16 4, 7, 10, 14, 25 Head tilt-down avoiding eye-contact, head shaking and frowning. E (L) 1, 4, 10, 16 4, 7, 10, 14, 17 Tilt-up, head shaking and frowning. A (L) 1, 8, 9, 16 4, 14, 17, 25, 26 Frequent head movements and frowning. N (L) 1, 5, 12, 16 4, 7, 10, 14, 25 Few head movements, head shaking and frowning.\nMIT\nRH (H) 16, 14, 3, 4 5, 10, 12, 14, 25 Head nodding and smiling, and being expressive. Ex (H) 14, 3, 4, 9 5, 10, 12, 14, 25 Head nodding and exhibiting persistent head motion. Smiling and expressive. EC (H) 14, 12, 4, 5 6, 7, 10, 14, 25 Head up, nodding and showing limited facial emotions. Fr (H) 16, 3, 11, 14 5, 10, 12, 14, 25 Frequent head movements and smiling. RH (L) 11, 1, 2, 5 6, 7, 12, 14, 25 Head shaking and exhibiting minimal facial expressions. Ex (L) 11, 16, 2, 3 4, 6, 7, 14, 25 Head shaking and nodding. Frowning and showing minimal facial expressions. EC (L) 13, 7, 16, 11 6, 7, 10, 12, 25 Frequent nodding is perceived as avoiding eye-contact. Fr (L) 3, 11, 4, 9 1, 4, 6, 7, 25 Head shaking, frowning and otherwise being minimally expressive.\ntle difference between high and low-rated personality impressions. Also, note that AUs 25 and 26 signifying talking behavior are present in all videos. \u2022 Focusing on other kinemes, high Openness is characterized by kinemes 2 and 8, which signify persistent head movements. This finding is echoed in [56], where large motion variations are found to associate with high O impressions. Presence of AUs 12 and 14 indicates that a smiling demeanor characterizes high O. Conversely, kineme 6 denoting minimal head motion and AUs 4 and 17 typical of frowning and diffident behavior are commonly noted for low O videos. \u2022 Kineme 1 denoting an upward head tilt is associated with high C, while kinemes 2 and 4 depicting tiltdown and head-shaking are associated with low C. This indicates that attempting to maintain eye-contact conveys diligence and honesty, while avoiding eyecontact conveys insincerity. \u2022 Extraversion appears to be conveyed better by AUs than kinemes; Dominant AUs for high E include 10, 12 and 17 indicating a friendly and talkative nature, while dominant kinemes 2 and 14 convey significant head movements. Conversely, low E is associated with kineme 4 denoting head-shaking and AUs 4, 7 and 17 indicating frowning, overall conveying a socially distant nature. \u2022 High Agreeableness is characterized by kineme 3 (headnod), and AUs 12 and 14 which constitute a smile. Conversely, kinemes 1, 8 and 9 dominate low A, and they collectively convey persistent head motion. Also, AUs dominant for low A are 4, 14 and 17, cumulatively describing a frown; overall, nodding and smiling is viewed as courteous, while frequent head movements and frowning convey hostility. \u2022 Emotional stability (high N) is associated with kinemes 2 and 8, and AUs 7, 12 and 17, indicating persistent head motion and facial expressiveness. On the other hand, a neurotic trait is conveyed via limited head mo-\ntion and head-shaking (kinemes 1, 5, 12) and frowning (described by AUs 4, 7, 10). \u2022 While kinemes for the MIT videos are less discernible, due to smaller face size (Fig. 4) and the fact that they capture an interactional setting, some patterns are nevertheless evident as seen in Fig. 6; these kinemes are highlighted in Table 6. As with FICS, Kineme 14 denoting a head-nod is commonly observed for all high trait videos, while kineme 11 depicting a head-shake is common for all low-trait videos. \u2022 High RH scores are elicited with expressive facial behavior involving head-nodding and smiling. Conversely, low RH scores are associated with headshaking and exhibiting limited facial expressions. Highly excited behavior is associated with identical AUs as high RH, and persistent head motion. Inversely, low excitement scores are connected with head shaking, and limited facial emotions. \u2022 Identical AUs are observed for both high and low eye-contact, implying that head movements primarily impact eye-contact impressions. Head nodding (kineme 14) is associated with high EC, while kinemes 11 and 16 depicting head shaking and frequent head-nodding elicit low EC scores. Therefore interestingly, while head nodding is beneficial, frequent nodding is perceived as avoiding eye-contact. \u2022 High friendliness is characterized by kinemes 11, 14 and 16, signifying persistent head motion along with expressive and smiling facial movements (AUs 5, 12 and 14). Conversely, low friendliness is associated with head-shaking (kineme 11) and frowning (AUs 4, 6, 7)."
        },
        {
            "heading": "5.2 Attention Score-based Interpretations",
            "text": "While Table 6 presents unimodal behavioral explanations via kinemes and AUs, behaviors are expressed and best modeled multimodally as seen from our empirical results (Section 4.3). For multimodal explanations, we explore the\n10\nFig. 11. Mean modality-specific attention weights for personality traits (left) and interview traits (right). Error bars denote standard error.\nattention-fusion network (Fig. 2) to estimate the relative contribution of each modality towards trait regression. We visualize softmax scores learned by the attention-fusion network as follows. For the FICS dataset, we present mean attention scores obtained over 10 runs on 15s test videos (Fig. 11(left)), while we present softmax scores averaged over 15s chunks for MIT videos across 50 runs (Fig. 11(right)). Our remarks from the weight plots are as follows:\n\u2022 Cumulatively, Fig. 11 conveys that while the relative contribution of speech features towards weighted fusion is not high for personality trait prediction, they tend to play a significant role in predicting interview traits on the MIT dataset. These observations mirror prior findings; the criticality of visual features such as head movements and facial movements for personality trait recognition has been noted in [14], [59] while the impact of prosodic speech features on interview trait impressions is discussed in [17]. \u2022 Fig. 11(left) conveys that either kineme or AU features are most critical for personality trait prediction. Specifically, kinemes maximally contribute to the prediction of Openness and Extraversion, while AUs are most critical for predicting Agreeableness and Neuroticism. Both kinemes and AUs are found to be equally critical for estimating Conscientiousness, in line with the findings in [60]. Extraversion and Openness are conveyed by exaggerated physical and head movements [56], [61], with different head movement patterns representing high and low Extraversion [62]. While Agreeableness is also positively correlated with head movements [61], [62], empathetic behavior is accurately conveyed via facial expressions as denoted by the higher AU weights. Facial movements (e.g., unconcerned or anxious) better convey emotional stability [63]. \u2022 From Fig. 11(right), it can be seen that facial movements have relatively less impact on interview trait prediction with the exception of eye contact. This can partly be attributed to the smaller face size in MIT videos, limiting the efficacy of AU detection. Conversely, speech features significantly impact trait prediction with the exception of recommended hiring and eye contact traits. While prosodic speech behavior has been found to considerably influence interview trait impressions [17], [64], other forms of non-verbal behavior such as positive facial expressions and frequent postural changes are known to impact hierabilty [65]. \u2022 For the Excited trait, speech plays a prominent role with a high correlation to continuous or restricted\nhead movement [66]. On the surprising finding of AUs and speech features impacting eye-contact, prior studies [67] have revealed a low-yet-meaningful correlation between eye contact impressions and vocal acoustic features. Friendliness is best characterized by head movement and voice features, showing that the integration of visual and auditory modalities can be crucial in discerning interviewee friendliness [68]."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This work demonstrates the efficacy of multimodal (kineme, AU and speech) behavioral cues to achieve explainable prediction of OCEAN and interview traits. Our results confirm that efficient trait prediction can be achieved with both unimodal and multimodal approaches. Also, multimodal approaches outperform their unimodal counterparts owing to complementary information provided by trait-specific behavioral cues. In addition, frequently occurring kineme and AU patterns enable behavioral explanations associated with each trait.\nIn terms of limitations, this work extracts all behavioral features over a fixed time window (same time-scale); however, behaviors associated with human personality may manifest over different time scales; e.g., facial expression or head motion patterns could be affected by speaking behavior (talkative: drastic variation in speaking behavior over video frames, or reserved: lingering silence over most video frames). Investigating the effect of temporal scales will be a future research direction. Trait-specific behavioral patterns can also be utilized to create virtual agents to train users in interviewing or public speaking settings. The authors do not advise using the proposed methodologies for complex processes like job recruitment per se; however, explanatory technologies can be utilized as a complementary tool in decision-making processes."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank A. Samanta, IIT Kanpur for sharing the kineme implementation."
        }
    ],
    "title": "Explainable Human-centered Traits from Head Motion and Facial Expression Dynamics",
    "year": 2023
}