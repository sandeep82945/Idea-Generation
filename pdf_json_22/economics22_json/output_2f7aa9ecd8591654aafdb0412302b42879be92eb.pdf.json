{
    "abstractText": "This paper considers a linear regression model with stochastic restrictions,we propose a new mixed Kibria\u2013Lukman estimator by combining the mixed estimator and the Kibria\u2013Lukman estimator.This new estimator is a general estimation, including OLS estimator, mixed estimator and Kibria\u2013Lukman estimator as special cases. In addition, we discuss the advantages of the new estimator based on MSEM criterion, and illustrate the theoretical results through examples and simulation analysis.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hongmei Chen"
        },
        {
            "affiliations": [],
            "name": "Jibo Wu"
        }
    ],
    "id": "SP:9b70ae52a30c550267de84854ac831606e625e70",
    "references": [
        {
            "authors": [
                "W.F. Massy"
            ],
            "title": "Principal components regression in exploratory statistical research",
            "venue": "J. Am. Stat. Assoc",
            "year": 1965
        },
        {
            "authors": [
                "A.E. Hoerl",
                "Kennard",
                "R.W. Ridge regression"
            ],
            "title": "Biased estimation for nonorthogonal problems",
            "venue": "Technometrics 12, 55\u201367",
            "year": 1970
        },
        {
            "authors": [
                "B.F. Swindel"
            ],
            "title": "Good estimators based on prior information",
            "venue": "Commun. Stat. Theroy Methods",
            "year": 1976
        },
        {
            "authors": [
                "A.F. Lukman",
                "K. Ayinde",
                "S. Binuomote",
                "A.C. Onate"
            ],
            "title": "Modified ridge-type estimator to cambat multicollinearity",
            "venue": "J. Chemom",
            "year": 2019
        },
        {
            "authors": [
                "K.J. Liu"
            ],
            "title": "A new class of biased estimate in linear regression",
            "venue": "Commun. Stat. Theroy Methods 22,",
            "year": 1993
        },
        {
            "authors": [
                "F. Akdeniz",
                "S. Kaciranlar"
            ],
            "title": "On the almost unbiased generalized Liu estimator and unbiased estimation of the bias and MSE",
            "venue": "Commun. Stat. Theroy Methods",
            "year": 1995
        },
        {
            "authors": [
                "K.J. Liu"
            ],
            "title": "Using Liu-type estimator to combat collinearity",
            "venue": "Commun. Stat. Theroy Methods",
            "year": 2003
        },
        {
            "authors": [
                "M.R. Baye",
                "Parker",
                "D.F. Combining ridge",
                "principal component regression"
            ],
            "title": "A money demand illustration",
            "venue": "Commun. Stat. Theroy Methods 13, 197\u2013225",
            "year": 1984
        },
        {
            "authors": [
                "S. Kaciranlar",
                "S. Sakallioglu"
            ],
            "title": "Combining the Liu estimator and the principal component regression estimator",
            "venue": "Commun. Stat. Theroy Methods",
            "year": 2001
        },
        {
            "authors": [
                "M.R. Ozkale",
                "S. Kaciranlar"
            ],
            "title": "The restricted and unrestricted two-parameter estimators",
            "venue": "Commun. Stat. Theroy Methods",
            "year": 2007
        },
        {
            "authors": [
                "F.M. Batah",
                "M.R. Ozkale",
                "S.D. Gore"
            ],
            "title": "Combining unbiased ridge and principal component regressions estimators",
            "venue": "Commun. Stat. Theroy Methods",
            "year": 2009
        },
        {
            "authors": [
                "H. Yang",
                "X.F. Chang"
            ],
            "title": "A new two-parameter estimator in linear regression",
            "venue": "Commun. Stat. Theroy Methods 39(6),",
            "year": 2010
        },
        {
            "authors": [
                "A.F. Lukman",
                "K. Ayinde",
                "O. Oludoun",
                "C.A. Onate"
            ],
            "title": "Combining modified ridge-type and principal component regression estimators",
            "venue": "Sci. Afr",
            "year": 2020
        },
        {
            "authors": [
                "B.M.G. Kibria",
                "Lukman",
                "A.F. A new ridge-type estimator for the linear regression model"
            ],
            "title": "Simulations and applications",
            "venue": "Scientificahttps:// doi. org/ 10. 1155/ 2020/ 97583 78",
            "year": 2020
        },
        {
            "authors": [
                "H. Theil",
                "A.S. Goldberger"
            ],
            "title": "On pure and mixed estimation in econometrics",
            "venue": "Int. Econ. Rev",
            "year": 1961
        },
        {
            "authors": [
                "H. Theil"
            ],
            "title": "On the use of incomplete prior information in regression analysis",
            "venue": "J. Am. Stat. Assoc",
            "year": 1963
        },
        {
            "authors": [
                "B. Schiffrin",
                "H. Toutenburg"
            ],
            "title": "Weighted mixed regression",
            "venue": "Z. Angew. Math. Mech. 70,",
            "year": 1990
        },
        {
            "authors": [
                "M.H. Hubert",
                "P. Wijekoon"
            ],
            "title": "Improvement of the Liu estimator in linear regression coefficient",
            "venue": "Stat. Pap. 47,",
            "year": 2006
        },
        {
            "authors": [
                "H. Yang",
                "J.W. Xu"
            ],
            "title": "An alternative stochastic restricted Liu estimator in linear regression model",
            "venue": "Stat. Pap. 50,",
            "year": 2009
        },
        {
            "authors": [
                "N. Ozbay",
                "Kaciranlar",
                "K.S. Estimation in a linear regression model with stochastic linear restrictions"
            ],
            "title": "A new two-parameterweighted mixed estimator",
            "venue": "J. Stat. Comput. Simul. 88, 1669\u20131683",
            "year": 2018
        },
        {
            "authors": [
                "M.H.J. Gruber"
            ],
            "title": "Improving Efficiency by Shrinkage: The James\u2013Stein and Ridge Regression estimators (Marcel",
            "venue": "Dekker Inc,",
            "year": 1998
        },
        {
            "authors": [
                "F. Akdeniz",
                "H. Erol"
            ],
            "title": "Mean Squared error matrix comparisons of some biased estimator in linear regression",
            "venue": "Commun. Stat. Theroy Methods 32(12),",
            "year": 2003
        },
        {
            "authors": [
                "M Arashi"
            ],
            "title": "Ridge regression and its applications in genetic studies",
            "venue": "PLoS One 16(4),",
            "year": 2021
        },
        {
            "authors": [
                "M. Roozbeh",
                "S.P. Azen"
            ],
            "title": "Optimal QR-based estimation in partially linear regression models with correlated errors using GCV criterion",
            "venue": "Comput. Stat. Data Anal. 117,",
            "year": 2018
        },
        {
            "authors": [
                "M. Roozbeh",
                "M. Arahi",
                "N.A. Hamzah"
            ],
            "title": "Generalized cross-validation for simultaneous optimization of tuning parameters in ridge regression. Iran",
            "venue": "J. Sci. Technol. Trans. A,",
            "year": 2020
        },
        {
            "authors": [
                "M. Roozbeh",
                "G. Hesamianb",
                "M.G. Akbaric"
            ],
            "title": "Ridge estimation in semi-parametric regression models under the stochastic restriction and correlated elliptically contoured errors",
            "venue": "J. Comput. Appl. Math. 378,",
            "year": 2020
        },
        {
            "authors": [
                "M. Roozbeh",
                "N.A. Hamzah"
            ],
            "title": "Uncertain stochastic ridge estimation in partially linear regression models with elliptically distributed errors",
            "venue": "Statistics 3,",
            "year": 2022
        },
        {
            "authors": [
                "M.C. McDonald",
                "D.I. Galarneau"
            ],
            "title": "A Monte Carlo evaluation of ridge-type estimators",
            "venue": "J. Am. Stat. Assoc",
            "year": 1975
        },
        {
            "authors": [
                "D.G. Gibbons"
            ],
            "title": "A simulation study of some ridge estimators",
            "venue": "J. Am. Stat. Assoc",
            "year": 1981
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nwww.nature.com/scientificreports\nOn the mixed Kibria\u2013Lukman estimator for the linear regression model Hongmei Chen1 & Jibo Wu2*\nThis paper considers a linear regression model with stochastic restrictions,we propose a new mixed Kibria\u2013Lukman estimator by combining the mixed estimator and the Kibria\u2013Lukman estimator.This new estimator is a general estimation, including OLS estimator, mixed estimator and Kibria\u2013Lukman estimator as special cases. In addition, we discuss the advantages of the new estimator based on MSEM criterion, and illustrate the theoretical results through examples and simulation analysis.\nConsider the following linear regression model:\nwhere y is the response variable vector of n\u00d7 1,X is the column full rank independent variables matrix of n\u00d7 (p+ 1),\u03b2 is the unknown coefficient vector of p\u00d7 1, \u03b5 is the random error vector of n dimension such that E(\u03b5) = 0 and Cov(\u03b5) = \u03c3 2I , where \u03c3 2 > 0 is mean squared error.\nIn the estimation of unknown coefficient vector \u03b2 , the OLS estimator is the most commonly used:\nIt is easy to know from formula (2), E\u03b2\u0302 = \u03b2 , and the OLS estimator has been widely used because of its unbiased nature and concise form. However, the ill condition of the design matrix X caused by the increasing number of dependent predictors often makes the OLS estimates unstable.\nMassy1 proposed principal component estimator. Hoerl and Kennard2 obtained the ridge estimation by introducing a ridge parameter k into the design X \u2032X matrix calculation. Swindel3 proposed a modified ridge estimator with prior information while Lukman et\u00a0al.4 proposed the two-parameter form of the ridge estimator called the modified ridge estimator (MRT). Liu5 obtained a linearized form of the ridge estimator called the Liu estimator. Akdeniz and Kaciranlara6 proposed the generalized Liu estimator. Liu7 obtained a two-parameter form of the Liu estimator.\nMany scholars have found that a new estimator can be obtained by combining the two estimators, which generally have good statistical properties. Baye and Parker8 proposed r\u2013k estimator by combining ridge estimator and principal component estimator. Kaciranlar and Sakallioglu9 proposed r\u2013d estimator by combining Liu estimator and principal component estimator. Ozkale and Kaciranlar10 proposed two parameter estimator by combining the James\u2013Stein Shrinkage estimator and the modified ridge estimator proposed by Swindel. Batah et\u00a0al.11 proposed a modified r-k estimator combining unbiased ridge estimator and principal component estimator. Yang and Chang12 proposed another two parameter estimator based on ridge estimator and Liu estimator. Lukman et\u00a0al.13 proposed a new estimator by combining modified ridge estimator (MRT) and principal component estimator. Kibria and Lukman14 proposed Kibria\u2013Lukman estimator by combining ridge estimator and Liu estimator.\nIn practice, in addition to the sample information given by model (1), additional information about parameters in the sample information, such as certain deterministic or stochastic restrictions on unknown parameters, can also be considered. This method can also overcome the complex collinearity problem. Theil and Goldberger15 and Theil16 proposed mixed estimator by comprehensively considering sample information and constraints. Schiffrin and Toutenburg17 proposed weighted mixed estimator for the different importance of sample information and prior information.\nIn recent years, biased estimation and estimation methods with prior information are often combined to form a broader biased estimation. Hubert and Wijekoon18 proposed a stochastic restricted Liu estimator by combining Liu estimator and mixed estimator. Yang and Xu19 obtained another stochastic mixed Liu estimator. In the same year, Yang and Chang further studied the stochastic mixed Liu estimator and obtained the weighted mixed Liu\n(1)y = X\u03b2 + \u03b5,\n(2)\u03b2\u0302OLS = ( X \u2032X )\u22121 X \u2032 y\nOPEN\n1College of Mathematics and Statistics, Chongqing Jiaotong University, Chongqing, China. 2School of Mathematics and Big Data, Chongqing University of Arts and Sciences, Chongqing, China. *email: linfen52@126.com\n2 Vol:.(1234567890) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nestimator. Yang and Li12 proposed another stochastic mixed ridge estimator. Ozbay and Kaciranlar20 integrated two parameter estimator and mixed estimator and proposed a two parameter mixed estimator.\nIn this paper, a new mixed KL estimator under stochastic restrictions is proposed, and its excellent properties under certain conditions are proved theoretically. The above theoretical results are verified and analyzed by examples and data simulation.\nThe proposed estimator Hoerl and Kennard2 proposed the ridge estimator (RE):\nwhere k > 0 is the parameter. In fact, ridge estimator is obtained by solving the following extreme value problem:\nwhere c is constant, k is the Lagrange constant. Kibria and Lukman14 proposed the Kibria Lukman (KL) estimator:\nwhere k > 0 is the parameter.KL estimator is obtained by solving the following extreme value problem:\nwhere c is constant, k is the Lagrange constant. Consider the following stochastic restrictions:\nwhere r is the known random vector of j \u00d7 1 , R is the row full rank sample data matrix of j \u00d7 p , let e be the j \u00d7 1 random error vector and independent of each other, and \u03c8 be the known positive definite matrix.\nTheil and Goldberger15 and Theil16 proposed the mixed estimator by integrating sample information and constraints. The derivation idea is to rewrite models (1) and (6) into a new linear model:\nIf y\u0303 = (\ny r\n)\n, X\u0303 =\n(\nX R\n)\n, \u03b5\u0303 =\n(\n\u03b5\ne\n)\n, above model is transformed into\nBy applying the least square estimator to the new linear model (7), the mixed estimator (ME)of parameter \u03b2 is obtained:\nCombined mixed estimator and ridge estimator and proposed stochastic mixed ridge estimation (RME):\nThe estimator proposed in this paper is obtained by solving the following extreme value problem:\nwhere c is constant, k is Lagrange constant. Regular equations can be obtained:\nfrom Eqs. (11) and (12), we can get the mixed KL estimator:\nIt can be seen from Eq. (13) that mixed estimator, KL estimator and OLS estimator can be regarded as special cases of mixed KL estimator.Namely\nWhen k = 0, \u03b2\u0302ME = \u03b2\u0302MKL = ( X \u2032X + R\u2032\u03c8\u22121R )\u22121( X \u2032y + R\u2032\u03c8\u22121r )\nis mixed estimator; When R = 0, \u03b2\u0302KL = \u03b2\u0302MKL = (X \u2032 X + kI)\u22121(X \u2032y \u2212 k\u03b2\u0302) is KL estimator;\n(3)\u03b2\u0302RE = ( X \u2032X + kI )\u22121 X \u2032y\n(y \u2212 X\u03b2)\u2032(y \u2212 X\u03b2)+ k ( \u03b2 \u2032\u03b2 \u2212 c )\n(4)\u03b2\u0302KL = ( X \u2032X + kI )\u22121\n( X \u2032y \u2212 k\u03b2\u0302 )\n(5)(y \u2212 X\u03b2)\u2032(y \u2212 X\u03b2)+ k [ (\u03b2 + \u03b2\u0302)\u2032(\u03b2 + \u03b2\u0302)\u2212 c ]\n(6)r = R\u03b2 + e, e \u223c ( 0, \u03c3 2\u03c8 ) ,\n(\ny r\n)\n=\n(\nX R\n)\n\u03b2 +\n(\n\u03b5\ne\n)\n(7)y\u0303 = X\u0303\u03b2 + \u03b5\u0303\n(8)\u03b2\u0302ME = ( X \u2032X + R\u2032\u03c8\u22121R )\u22121( X \u2032y + R\u2032\u03c8\u22121r )\n(9)\u03b2\u0302MRE = ( X \u2032X + kI + R\u2032\u03c8\u22121R )\u22121( X \u2032y + R\u2032\u03c8\u22121r )\n(10)\ufffd\u2217 = (y \u2212 X\u03b2)\u2032(y \u2212 X\u03b2)+ k [ (\u03b2 \u2212 d\u03b2\u0302)\u2032(\u03b2 \u2212 d\u03b2\u0302)\u2212 c ] + (r \u2212 R\u03b2)\u2032\u03c8\u22121(r \u2212 R\u03b2)\n(11)X \u2032 X\u03b2 \u2212 X \u2032 y + k(\u03b2 \u2212 d\u03b2\u0302)+ R\u2032\u03c8\u22121R \u2212 R\u2032\u03c8\u22121r = 0\n(12)(\u03b2 \u2212 d\u03b2\u0302)\u2032(\u03b2 \u2212 d\u03b2\u0302) = c\n(13)\u03b2\u0302MKL = ( X \u2032 X + kI + R\u2032\u03c8\u22121R )\u22121( X \u2032 y \u2212 k\u03b2\u0302 + R\u2032\u03c8\u22121r ) , k > 0\n3 Vol.:(0123456789) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nWhen k = 0,R = 0, \u03b2\u0302OLS = \u03b2\u0302MKL = (X \u2032 X)\u22121X \u2032 y is OLS estimator.\nThe performance of the new estimator If \u03b2\u0302 is the estimation of \u03b2 , then the mean square error matrix (MSEM) of \u03b2\u0302 is given as:\nwhere Cov(\u03b2\u0302) is the covariance matrix of \u03b2\u0302 , and Bias(\u03b2\u0302) = E(\u03b2\u0302)\u2212 \u03b2 is the deviation vector. Two estimates \u03b2\u03021 and \u03b2\u03022 , \u03b2\u03022 are better than \u03b2\u03021 under MSEM criterion if and only if:\nLemma 3.1 Suppose two n\u00d7 n matrix M > 0,N \u2265 0 , then M > N \u21d4 1 ( NM\u22121 ) < 1 , where 1 ( NM\u22121 )\nis the maximum eigenvalue of matrix NM\u22121.\nThe mean square error matrix of mixed KL estimator \u03b2\u0302MKL is calculated as follows:\nwhere Ak = ( X \u2032 X + kI + R\u2032\u03c8\u22121R )\u22121 .\nDeviation vector: Bias (\n\u03b2\u0302MKL\n) = E (\n\u03b2\u0302MKL\n)\n= \u22122kAk\u03b2.\nTherefore,\nwhere b1 = \u22122kAk\u03b2 .\nBy substituting k = 0 into Eq. (16), the mean square error matrix of the mixed estimator can be obtained:\nwhere M = X \u2032X + R\u2032\u03c8\u22121R.\nBy substituting R = 0 into Eq. (16), the mean square error matrix of the KL estimator can be obtained:\nMSEM(\u03b2\u0302) = E(\u03b2\u0302 \u2212 \u03b2)(\u03b2\u0302 \u2212 \u03b2)\u2032 = Cov(\u03b2\u0302)+ Bias(\u03b2\u0302)Bias(\u03b2\u0302) \u2032\n\ufffd\n(\n\u03b2\u03021, \u03b2\u03022\n) = MSEM (\n\u03b2\u03021\n) \u2212MSEM (\n\u03b2\u03022\n)\n\u2265 0\n(14)\nE (\n\u03b2\u0302MKL\n) =E [ ( X \u2032X + kI + R\u2032\u03c8\u22121R )\u22121 ( X \u2032 y \u2212 k\u03b2\u0302 + R\u2032\u03c8\u22121r )]\n=AkE ( X \u2032 y \u2212 k\u03b2\u0302 + R\u2032\u03c8\u22121r )\n=AkE ( X \u2032 y + k\u03b2\u0302 \u2212 2k\u03b2\u0302 + R\u2032\u03c8\u22121r )\n=Ak\n( A\u22121k \u2212 2k ) \u03b2\n=\u03b2 \u2212 2kAk\u03b2\n(15)\nCov (\n\u03b2\u0302MKL\n) =Cov [ ( X \u2032X + kI + R\u2032\u03c8\u22121R )\u22121 ( X \u2032 y \u2212 k\u03b2\u0302 + R\u2032\u03c8\u22121r )]\n=Cov [\nAk\n(\nX \u2032 y \u2212 k\u03b2\u0302 + R\u2032\u03c8\u22121r )]\n=AkCov ( X \u2032 y \u2212 k\u03b2\u0302 + R\u2032\u03c8\u22121r ) Ak\n=Ak ( \u03c3 2X \u2032X \u2212 k\u03c3 2S\u22121 + \u03c3 2R\u2032\u03c8\u22121R ) Ak\n= \u03c3 2Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak\n(16)\nMSEM(\u03b2\u0302MKL) =Cov(\u03b2\u0302MKL)+ Bias(\u03b2\u0302MKL)Bias(\u03b2\u0302MKL) \u2032\n= \u03c3 2Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak + 4k 2Ak\u03b2\u03b2 \u2032Ak\n= \u03c3 2Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak + b1b \u2032 1\n(17) MSEM(\u03b2\u0302ME) = \u03c3\n2 ( X \u2032X + R\u2032\u03c8\u22121R )\u22121( X \u2032X + R\u2032\u03c8\u22121R )( X \u2032X + R\u2032\u03c8\u22121R )\u22121\n= \u03c3 2 ( X \u2032X + R\u2032\u03c8\u22121R )\u22121\n= \u03c3 2M\u22121\n4 Vol:.(1234567890) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nwhere Sk = X \u2032X + kI , b2 = \u22122kS\u22121k \u03b2.\nBy substituting k = 0,R = 0 into Eq. (16), the mean square error matrix of the OLS estimator can be obtained:\nMean square error matrix of mixed ridge estimator:\nDeviation vector: Bias (\n\u03b2\u0302MRE\n) = E (\n\u03b2\u0302MRE\n)\n\u2212 \u03b2 = \u2212kAk\u03b2 .\nTherefore,\nComparison between mixed KL estimator and mixed estimator. From Eqs. (16) and (17), we make\nBecause\nfrom k > 0,so M\u22121 \u2212 Ak ( M \u2212 kS\u22121 ) Ak > 0 , Theorem\u00a03.2 is obtained.\nTheorem\u00a03.2 The necessary and sufficient conditions for mixed KL estimator \u03b2\u0302MKL to be superior to mixed estimator \u03b2\u0302ME under MSEM criterion are as follows:\n(18)\nMSEM (\n\u03b2\u0302KL\n)\n= \u03c3 2 ( X \u2032X + kI )\u22121( X \u2032X \u2212 kS\u22121 )( X \u2032X + kI )\u22121\n+ 4k2 ( X \u2032X + kI )\u22121 \u03b2\u03b2 \u2032 ( X \u2032X + kI )\u22121\n= \u03c3 2S\u22121k ( X \u2032X \u2212 kS\u22121 ) S\u22121k + 4k 2S\u22121k \u03b2\u03b2 \u2032S\u22121k\n= \u03c3 2S\u22121k ( X \u2032X \u2212 kS\u22121 ) S\u22121k + b2b \u2032 2\n(19)MSEM ( \u03b2\u0302 OLS ) = \u03c3 2S\u22121\n(20)\nE (\n\u03b2\u0302MRE\n) =E [ ( X \u2032X + kI + R\u2032\u03c8\u22121R )\u22121 ( X \u2032 y + R\u2032\u03c8\u22121r )]\n=AkE ( X \u2032y + R\u2032\u03c8\u22121r )\n=AkE ( X \u2032 y + k\u03b2\u0302 \u2212 k\u03b2\u0302 + R\u2032\u03c8\u22121r )\n=Ak\n( A\u22121k \u2212 kI ) \u03b2\n=\u03b2 \u2212 kAk\u03b2\nCov (\n\u03b2\u0302MRE\n) = Cov [ ( X \u2032X + kI + R\u2032\u03c8\u22121R )\u22121( X \u2032y + R\u2032\u03c8\u22121r ) ]\n= Cov [\nAk\n(\nX \u2032 y + R\u2032\u03c8\u22121r )]\n= AkCov ( X \u2032 y + R\u2032\u03c8\u22121r ) Ak\n= Ak ( \u03c3 2X \u2032X + \u03c3 2R\u2032\u03c8\u22121R ) Ak\n= \u03c3 2Ak ( X \u2032X + R\u2032\u03c8\u22121R ) Ak\n(21)MSEM ( \u03b2\u0302MRE ) = \u03c3 2Ak ( X \u2032X + R\u2032\u03c8\u22121R ) Ak + k 2Ak\u03b2\u03b2 \u2032Ak .\n(22)\n\ufffd1 =MSEM ( \u03b2\u0302ME ) \u2212MSEM ( \u03b2\u0302MKL )\n= \u03c3 2M\u22121 \u2212 \u03c3 2Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak \u2212 b1b \u2032 1\n= \u03c3 2M\u22121 \u2212 \u03c3 2Ak ( M \u2212 kS\u22121 ) Ak \u2212 b1b \u2032 1\n= \u03c3 2 [ M\u22121 \u2212 Ak ( M \u2212 kS\u22121 ) Ak ] \u2212 b1b \u2032 1\nM\u22121 \u2212 Ak ( M \u2212 kS\u22121 ) Ak\n= AkA \u22121 k M \u22121A\u22121k Ak \u2212 Ak ( M \u2212 kS\u22121 ) Ak\n= Ak\n[\nA\u22121k M \u22121A\u22121k \u2212\n( M \u2212 kS\u22121 )\n]\nAk\n= Ak [ (M + kI)M\u22121(M + kI)\u2212 ( M \u2212 kS\u22121 )] Ak\n= Ak ( M + 2kI + k2M\u22121 \u2212M + kS\u22121 ) Ak\n= Ak ( 2kI + k2M\u22121 + kS\u22121 ) Ak ,\n5 Vol.:(0123456789) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nComparison between mixed KL estimator and KL estimator. From Eqs. (16) and (18), we make\nBecause\nwhereN = S \u2212 kS\u22121,Q = R\u2032\u03c8\u22121R,B = NS\u22121k Q + QS \u22121 k N + QS \u22121 k NS \u22121 k Q \u2212 Q\nAccording to the Lemma 3.1, it can be obtained that if k < minpi=1 2 i , then N > 0 . So B > 0 if and only if\nk < min p i=1 2 i , 1Q\n(\nNS\u22121k Q + QS \u22121 k N + QS \u22121 k NS \u22121 k Q\n)\u22121 < 1.\nAs long as k < minpi=1 2 i , 1Q\n(\nNS\u22121k Q + QS \u22121 k N + QS \u22121 k NS \u22121 k Q\n)\u22121 < 1 , following conclusions can be\nobtained:\nTheorem\u00a03.3 When k < p\nmin i=1\n2 i , 1Q\n(\nNS\u22121k Q + QS \u22121 k N + QS \u22121 k NS \u22121 k Q\n)\u22121 < 1 , the necessary and sufficient\nconditions for mixed KL estimator \u03b2\u0302MKL to be superior to KL estimator \u03b2\u0302KL under MSEM criterion are as follows:\nComparison between mixed KL estimator and OLS estimator. From Eqs. (16) and (19), we make\nBecause\n(23)\u03c3\u22122b\u20321 [ M\u22121 \u2212 Ak ( M \u2212 kS\u22121 ) Ak ]\u22121 b1 \u2264 1\n(24)\n\ufffd2 = MSEM ( \u03b2\u0302KL ) \u2212MSEM ( \u03b2\u0302MKL )\n= \u03c3 2S\u22121k ( S \u2212 kS\u22121 ) S\u22121k + b2b \u2032 2 \u2212 \u03c3 2Ak ( S \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak \u2212 b1b \u2032 1 = \u03c3 2 [\nS\u22121k ( S \u2212 kS\u22121 ) S\u22121k \u2212 Ak ( S \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak\n]\n+ b2b \u2032 2 \u2212 b1b \u2032 1\nS\u22121k ( S \u2212 kS\u22121 ) S\u22121k \u2212 Ak ( S \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak\n= Ak\n[\nA\u22121k S \u22121 k\n( S \u2212 kS\u22121 ) S\u22121k A \u22121 k \u2212 ( S \u2212 kS\u22121 + R\u2032\u03c8\u22121R )\n]\nAk\n= Ak\n[\n(\nSk + R \u2032\u03c8\u22121R\n)\nS\u22121k NS \u22121 k\n(\nSk + R \u2032\u03c8\u22121R\n) \u2212 ( N + R\u2032\u03c8\u22121R )\n]\nAk\n= Ak\n[\n(Sk + Q)S \u22121 k NS \u22121 k (Sk + Q)\u2212 (N + Q)\n]\nAk\n= Ak\n[(\nI + QS\u22121k\n) N ( I + S\u22121k Q ) \u2212 (N + Q) ] Ak\n= Ak\n[\nN + NS\u22121k Q + QS \u22121 k N + QS \u22121 k NS \u22121 k Q \u2212 (N + Q)\n]\nAk\n= Ak\n(\nNS\u22121k Q + QS \u22121 k N + QS \u22121 k NS \u22121 k Q \u2212 Q\n)\nAk\n= AkBAk ,\n\ufffd2 \u2265 0 if and only if b \u2032 1\n(\n\u03c3 2AkBAk + b2b \u2032 2 )\u22121 b1 \u2264 1. Therefore, there is Theorem 3.2.\n(25)b\u20321 ( \u03c3 2AkBAk + b2b \u2032 2 )\u22121 b1 \u2264 1\n(26)\n\ufffd3 =MSEM ( \u03b2\u0302OLS ) \u2212MSEM ( \u03b2\u0302MKL )\n=\u03c3 2S\u22121 \u2212 \u03c3 2Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak \u2212 b1b \u2032 1\n=\u03c3 2 [ S\u22121 \u2212 Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak ] \u2212 b1b \u2032 1\n6 Vol:.(1234567890) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nwhere C = S\u22121Q + QS\u22121. B e c a u s e C = C\u2032 , a n d i ( S\u22121Q ) = i ( S\u2212 1 2QS\u2212 1 2 )\n> 0 , w e c a n g e t C > 0 , s o 2kI + kS\u22121 + k2S\u22121 + Q + kC + QS\u22121Q > 0 , that is S\u22121 \u2212 Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R )\nAk > 0 , Theorem\u00a03.4 is obtained.\nTheorem\u00a03.4 The necessary and sufficient conditions for mixed KL estimator \u03b2\u0302MKL to be superior to \u03b2\u0302OLS under MSEM criterion are as follows:\nComparison between mixed KL estimator and mixed ridge estimator. From Eqs. (16) and (22), we make\nTheorem\u00a03.5 The necessary and sufficient conditions for mixed KL estimator \u03b2\u0302MKL to be superior to the mixed ridge estimator \u03b2\u0302MRE under MSEM criterion are as follows:\n.\nNumerical example and simulation study In order to further explain the theoretical results, this section will verify and analyze the above theoretical results through examples.\nThe example analysis data adopts the percentage data of research and development expenses in GNP of several countries from 1972 to 1986 used by Gruber21, Akdeniz and Erol22, in which x1 represents France, x2 represents West Germany, x3 represents Japan, x4 represents the former Soviet Union and y represents the United States. See Table\u00a01 for specific data.\nThe data in Table\u00a01 are processed as follows\nS\u22121 \u2212 Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak\n= AkA \u22121 k S \u22121A\u22121k Ak \u2212 Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak\n= Ak\n[\nA\u22121k S \u22121A\u22121k \u2212\n( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R )\n]\nAk\n= Ak [ (S + kI + Q)S\u22121(S + kI + Q)\u2212 ( S \u2212 kS\u22121 + Q )] Ak\n= Ak [( I + kS\u22121 + QS\u22121 ) (S + kI + Q)\u2212 ( S \u2212 kS\u22121 + Q )] Ak\n= Ak [ S + kI + Q + ( I + kS\u22121 + QS\u22121 ) (kI + Q)\u2212 ( S \u2212 kS\u22121 + Q )] Ak\n= Ak [ kI + kS\u22121 + ( I + kS\u22121 + QS\u22121 ) (kI + Q) ] Ak\n= Ak ( 2kI + kS\u22121 + k2S\u22121 + Q + kS\u22121Q + kQS\u22121 + QS\u22121Q ) Ak\n= Ak [ 2kI + kS\u22121 + k2S\u22121 + Q + k ( S\u22121Q + QS\u22121 ) + QS\u22121Q ] Ak\n= Ak [ 2kI + kS\u22121 + k2S\u22121 + Q + kC + QS\u22121Q ] Ak\n(27)\u03c3\u22122b\u20321 [ S\u22121 \u2212 Ak ( X \u2032X \u2212 kS\u22121 + R\u2032\u03c8\u22121R ) Ak ] b1 \u2264 1\n(28)\n\ufffd4 =MSEM(\u03b2MRE)\u2212MSEM(\u03b2MKL)\n=\u03c3 2Ak(S + Q)Ak + k 2Ak\u03b2\u03b2 \u2032Ak \u2212 \u03c3 2Ak\n( S \u2212 kS\u22121 + Q ) Ak \u2212 4k 2Ak\u03b2\u03b2 \u2032Ak\n=\u03c3 2AkMAk \u2212 \u03c3 2Ak\n( M \u2212 kS\u22121 ) Ak \u2212 3k 2Ak\u03b2\u03b2 \u2032Ak\n=\u03c3 2Ak [ M \u2212 ( M \u2212 kS\u22121 )] Ak \u2212 3k 2Ak\u03b2\u03b2 \u2032Ak\n=k\u03c3 2AkS \u22121Ak \u2212 3k 2Ak\u03b2\u03b2 \u2032Ak\n=kAk ( \u03c3 2S\u22121 \u2212 3k\u03b2\u03b2 \u2032 ) Ak\n(29)3k\u03c3\u22122\u03b2 \u2032S\u03b2 \u2264 1\n7 Vol.:(0123456789) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nFirstly, it is easy to calculate that the eigenvalue of X \u2032X is 1 = 302.9626 , 2 = 0.7283 , 3 = 0.0446, 4 = 0.0345 ,the OLS estimator of \u03c3 2 is \u03c3\u0302 2 = 0.0015 , and OLS estimator of \u03b2 is \u03b2\u0302OLS = (0.6455, 0.0896, 0.1436, 0.1526)\u2032.\nWe can use the method proposey by Kibria and Lukman14 to choose the biasing parameter k, and we can also use the generalized cross validation (GCV) criterion and the cross validation (CV) to choose the biasing parameter, the reference can refer to Arashi et\u00a0al.23, Roozbeh24, and Roozbeh et\u00a0al.25. In this paper we use the method propose by Kibria and Lukman14 to choose the biasing parameter k, which is given as follows:\nwe take k = k\u0302min. Consider the following stochastic restrictions, this can refer to Roozbeh et\u00a0al.26 and Roozbeh and Hamzah27:\nFor the mixed estimator, KL estimator, OLS estimator, mixed ridge estimator and mixed KL estimator proposed in this paper. The MSE is presented in Table\u00a02.\nAs can be seen from Table\u00a02: When k takes k\u0302min = 0.018 , the MSE value of mixed KL estimator \u03b2\u0302MKL is better than that of mixed estimator, KL estimator,OLS estimator and mixed ridge estimator. Consistent with the theoretical results of this paper, it can be concluded that adding stochastic restrictions may have better estimation effect under certain conditions. So in practice we can use the stochastic restrictions to address the multicollinearity.\nNext, we consider Monte Carlo simulation analysis. Firstly, the generation of relevant parameters and data in the process of simulation analysis is briefly described.\n(30)k\u0302i = \u03c3\u0302 2\n2\u03b1\u0302i 2 +\n( \u03c3\u0302 2/ i )\nr = R\u03b2 + e,R = ( 1 \u22122 \u22122 \u22122 ) , r = 1, e \u223c ( 0, \u03c3\u0302 2 )\nYear x1 x2 x3 x4 y\n1972 1.9 2.2 1.9 3.7 2.3\n1975 1.8 2.2 2 3.8 2.2\n1979 1.8 2.4 2.1 3.6 2.2\n1980 1.8 2.4 2.2 3.8 2.3\n1981 2 2.5 2.3 3.8 2.4\n1982 2.1 2.6 2.4 3.7 2.5\n1983 2.1 2.6 2.6 3.8 2.6\n1984 2.2 2.6 2.6 4 2.6\n1985 2.3 2.8 2.8 3.7 2.7\n1986 2.3 2.7 2.8 3.8 2.7\n8 Vol:.(1234567890) Scientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\nwhere zij is the random number generated by the standard normal random variable, \u03c1 is the given constant, and \u03c12 theoretically represents the correlation between two different variables, so \u03c12 reflects the degree of complex collinearity of the model to some extent. In this simulation analysis, we consider three cases \u03c1 = 0.85, 0.9, 0.99 , set p = 3, r = 1,R = ( 1 \u22122 \u22122 )\n, e \u223c (0, \u03c3 2), n = 30, 50, 70, 100. For a given design matrix X, we take the orthogonalized eigenvector corresponding to the maximum eigen-\nvalue of X \u2032X as the real value of parameter vector \u03b2. The data corresponding to the response variable is generated by the following equation:\nwhere \u03b5i is the mean of zero, and random vector with variance of \u03c3 2 = 0.1, 1, 5, 10. See Tables\u00a03, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17 and 18 for simulation analysis and calculation results. Based on Tables\u00a03, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17 and 18, the following conclusions are drawn:\nyi = \u03b21xi1 + \u03b22xi2 + . . .+ \u03b2pxip + \u03b5i , i = 1, 2, . . . , n\n\u03c1 \u03b2\u0302ME \u03b2\u0302KL \u03b2\u0302OLS \u03b2\u0302MRE \u03b2\u0302MKL\n0.85 0.0024 0.0028 0.0028 0.0023 0.0023\n0.9 0.0032 0.0042 0.0043 0.0032 0.0032\n0.99 0.0197 0.0214 0.0282 0.0185 0.0160\n\u03c1 \u03b2\u0302ME \u03b2\u0302KL \u03b2\u0302OLS \u03b2\u0302MRE \u03b2\u0302MKL\n0.85 0.1908 0.1775 0.2012 0.1673 0.1675\n0.9 0.1932 0.1968 0.2340 0.1662 0.1660\n0.99 1.9234 1.3309 2.9377 1.3809 0.7301\n10\nVol:.(1234567890)\nScientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\n(1) The mean square error of all estimates increases with the increase of \u03c1 and decreases with the increase of n;\u00a0 (2) The new estimator mixed KL estimator always has the minimum MSE when all given n and \u03c3 2 ,and k takes k\u0302min . Consistent with the results of Theorems 3.2\u20133.5 in this paper, under certain conditions, mixed KL estimator \u03b2\u0302MKL is better than mixed estimator \u03b2\u0302ME , KL estimator \u03b2\u0302KL , least square estimator \u03b2\u0302OLS and mixed ridge estimator \u03b2\u0302MRE under MSE criterion; (3) Under the same conditions, mixed estimator \u03b2\u0302ME,mixed ridge estimator \u03b2\u0302MRE and mixed KL estimator \u03b2\u0302MKL are better than unconstrained least squares estimator \u03b2\u0302OLS under MSE criterion, mixed KL estimator \u03b2\u0302MKL is better than unconstrained KL estimator \u03b2\u0302KL under MSE criterion.\nConclusions In this paper, a new mixed KL estimator considering the prior information about parameters in sample information in linear model is proposed, and the properties of the new estimator are discussed. The necessary and sufficient conditions for KL estimator to be better than mixed estimator, KL estimator, OLS estimator and mixed ridge estimator under the criterion of mean square error matrix are given, and the proofs are given respectively. Then the theoretical results are verified by examples and simulation analysis.\nReceived: 3 April 2022; Accepted: 13 July 2022\nReferences 1. Massy, W. F. Principal components regression in exploratory statistical research. J. Am. Stat. Assoc. 60, 234\u2013256 (1965). 2. Hoerl, A. E. & Kennard, R. W. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics 12, 55\u201367 (1970). 3. Swindel, B. F. Good estimators based on prior information. Commun. Stat. Theroy Methods 5, 1065\u20131075 (1976). 4. Lukman, A. F., Ayinde, K., Binuomote, S. & Onate, A. C. Modified ridge-type estimator to cambat multicollinearity. J. Chemom.\ne3125, 1\u201312 (2019). 5. Liu, K. J. A new class of biased estimate in linear regression. Commun. Stat. Theroy Methods 22, 393\u2013402 (1993). 6. Akdeniz, F. & Kaciranlar, S. On the almost unbiased generalized Liu estimator and unbiased estimation of the bias and MSE.\nCommun. Stat. Theroy Methods 24, 1789\u20131797 (1995).\n\u03c1 \u03b2\u0302ME \u03b2\u0302KL \u03b2\u0302OLS \u03b2\u0302MRE \u03b2\u0302MKL\n0.85 23.4609 8.7136 27.9864 8.4935 8.0303\n0.9 28.7442 13.2491 31.8223 12.4392 11.8325\n0.99 343.6973 450.0098 539.6959 250.2456 106.987\nTable 17. Estimated MSE when \u03c3 2 = 10, n = 70.\nTable 18. Estimated MSE when \u03c3 2 = 10, n = 100.\n\u03c1 \u03b2\u0302ME \u03b2\u0302KL \u03b2\u0302OLS \u03b2\u0302MRE \u03b2\u0302MKL\n0.85 8.6620 3.0434 8.9405 3.1080 2.9602\n0.9 16.3215 5.6973 17.6756 5.6112 5.4424\n0.99 120.1565 67.5786 168.6498 62.5914 47.1027\n11\nVol.:(0123456789)\nScientific Reports | (2022) 12:12430 | https://doi.org/10.1038/s41598-022-16689-z\n7. Liu, K. J. Using Liu-type estimator to combat collinearity. Commun. Stat. Theroy Methods 32, 1009\u20131020 (2003). 8. Baye, M. R. & Parker, D. F. Combining ridge and principal component regression: A money demand illustration. Commun. Stat.\nTheroy Methods 13, 197\u2013225 (1984). 9. Kaciranlar, S. & Sakallioglu, S. Combining the Liu estimator and the principal component regression estimator. Commun. Stat.\nTheroy Methods 30, 2699\u20132705 (2001). 10. Ozkale, M. R. & Kaciranlar, S. The restricted and unrestricted two-parameter estimators. Commun. Stat. Theroy Methods 36,\n2707\u20132725 (2007). 11. Batah, F. M., Ozkale, M. R. & Gore, S. D. Combining unbiased ridge and principal component regressions estimators. Commun.\nStat. Theroy Methods 38, 2201\u20132209 (2009). 12. Yang, H. & Chang, X. F. A new two-parameter estimator in linear regression. Commun. Stat. Theroy Methods 39(6), 923\u2013934 (2010). 13. Lukman, A. F., Ayinde, K., Oludoun, O. & Onate, C. A. Combining modified ridge-type and principal component regression\nestimators. Sci. Afr. e536, 1\u20138 (2020). 14. Kibria, B. M. G. & Lukman, A. F. A new ridge-type estimator for the linear regression model: Simulations and applications. Sci-\nentificahttps:// doi. org/ 10. 1155/ 2020/ 97583 78 (2020). 15. Theil, H. & Goldberger, A. S. On pure and mixed estimation in econometrics. Int. Econ. Rev. 2, 65\u201378 (1961). 16. Theil, H. On the use of incomplete prior information in regression analysis. J. Am. Stat. Assoc. 58, 401\u2013414 (1963). 17. Schiffrin, B. & Toutenburg, H. Weighted mixed regression. Z. Angew. Math. Mech. 70, 735\u2013738 (1990). 18. Hubert, M. H. & Wijekoon, P. Improvement of the Liu estimator in linear regression coefficient. Stat. Pap. 47, 471\u2013479 (2006). 19. Yang, H. & Xu, J. W. An alternative stochastic restricted Liu estimator in linear regression model. Stat. Pap. 50, 369\u2013647 (2009). 20. Ozbay, N. & Kaciranlar, K. S. Estimation in a linear regression model with stochastic linear restrictions: A new two-parameter-\nweighted mixed estimator. J. Stat. Comput. Simul. 88, 1669\u20131683 (2018). 21. Gruber, M. H. J. Improving Efficiency by Shrinkage: The James\u2013Stein and Ridge Regression estimators (Marcel Dekker Inc, 1998). 22. Akdeniz, F. & Erol, H. Mean Squared error matrix comparisons of some biased estimator in linear regression. Commun. Stat.\nTheroy Methods 32(12), 2389\u20132413 (2003). 23. Arashi, M. et al. Ridge regression and its applications in genetic studies. PLoS One 16(4), e0245376 (2021). 24. Roozbeh, M. & Azen, S. P. Optimal QR-based estimation in partially linear regression models with correlated errors using GCV\ncriterion. Comput. Stat. Data Anal. 117, 45\u201361 (2018). 25. Roozbeh, M., Arahi, M. & Hamzah, N. A. Generalized cross-validation for simultaneous optimization of tuning parameters in\nridge regression. Iran. J. Sci. Technol. Trans. A, Sci. 44(2), 473\u2013485 (2020). 26. Roozbeh, M., Hesamianb, G. & Akbaric, M. G. Ridge estimation in semi-parametric regression models under the stochastic\nrestriction and correlated elliptically contoured errors. J. Comput. Appl. Math. 378, 112940 (2020). 27. Roozbeh, M. & Hamzah, N. A. Uncertain stochastic ridge estimation in partially linear regression models with elliptically distrib-\nuted errors. Statistics 3, 494\u2013523 (2022). 28. McDonald, M. C. & Galarneau, D. I. A Monte Carlo evaluation of ridge-type estimators. J. Am. Stat. Assoc. 70, 407\u2013416 (1975). 29. Gibbons, D. G. A simulation study of some ridge estimators. J. Am. Stat. Assoc. 76, 131\u2013139 (1981).\nAuthor contributions H.C. and J.W. wrote the main manuscript text. All authors reviewed the manuscript.\nFunding The authors are highly obliged to the editor and the reviewers for the comments and suggestions which improved the paper in its present form.This work was sponsored by the Natural Science Foundation of Chongqing [grant number cstc2020jcyj-msxmX0028] and the Scientific Technological Research Program of Chongqing Municipal Education Commission [grant number KJQN202001321].\nCompeting interests The authors declare no competing interests.\nAdditional information Correspondence and requests for materials should be addressed to J.W.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2022"
        }
    ],
    "title": "On the mixed Kibria\u2013Lukman estimator for the linear regression model",
    "year": 2022
}