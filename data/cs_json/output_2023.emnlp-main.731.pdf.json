{
    "abstractText": "Textbooks are one of the main mediums for delivering high-quality education to students. In particular, explanatory and illustrative visuals play a key role in retention, comprehension and general transfer of knowledge. However, many textbooks lack these interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. We collect a dataset of e-textbooks in the math, science, social science and business domains. We then set up a text-image matching task that involves retrieving and appropriately assigning web images to textbooks, which we frame as a matching optimization problem. Through a crowd-sourced evaluation, we verify that (1) while the original textbook images are rated higher, automatically assigned ones are not far behind, and (2) the precise formulation of the optimization problem matters. We release the dataset of textbooks with an associated image bank to inspire further research in this intersectional area of computer vision and NLP for education.",
    "authors": [
        {
            "affiliations": [],
            "name": "Janvijay Singh\u2217G"
        },
        {
            "affiliations": [],
            "name": "Vil\u00e9m Zouhar"
        },
        {
            "affiliations": [],
            "name": "Mrinmaya Sachan"
        }
    ],
    "id": "SP:86001f7b94f7591129b8482ee3f56b7d8648383a",
    "references": [
        {
            "authors": [
                "Rakesh Agrawal",
                "Sreenivas Gollapudi",
                "Anitha Kannan",
                "Krishnaram Kenthapadi."
            ],
            "title": "Enriching textbooks with images",
            "venue": "Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM \u201911, page 1847\u20131856, New",
            "year": 2011
        },
        {
            "authors": [
                "Rakesh Agrawal",
                "Sreenivas Gollapudi",
                "Krishnaram Kenthapadi",
                "Nitish Srivastava",
                "Raja Velu."
            ],
            "title": "Enriching textbooks through data mining",
            "venue": "Proceedings of the First ACM Symposium on Computing for Development, ACM DEV \u201910, New York, NY, USA.",
            "year": 2010
        },
        {
            "authors": [
                "Russell N. Carney",
                "Joel R. Levin."
            ],
            "title": "Pictorial illustrations still improve students\u2019 learning from text",
            "venue": "Educational Psychology Review, 14:5\u201326.",
            "year": 2002
        },
        {
            "authors": [
                "Kostas Dimopoulos",
                "Vasilis Koulaidis",
                "S. Sklaveniti"
            ],
            "title": "Towards an analysis of visual images in school",
            "year": 2003
        },
        {
            "authors": [
                "Anne Nielsen Hibbing",
                "Joan L. Rankin-Erickson."
            ],
            "title": "A picture is worth a thousand words: Using visual images to improve comprehension for middle school struggling readers author(s)",
            "venue": "The Reading Teacher.",
            "year": 2008
        },
        {
            "authors": [
                "Petros J. Katsioloudis"
            ],
            "title": "Identification of quality indicators of visual-based learning material in technology education programs for grades 7-12",
            "year": 2007
        },
        {
            "authors": [
                "Aniruddha Kembhavi",
                "Minjoon Seo",
                "Dustin Schwenk",
                "Jonghyun Choi",
                "Ali Farhadi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
            "venue": "2017 IEEE Conference on Com-",
            "year": 2017
        },
        {
            "authors": [
                "Dong Won Lee",
                "Chaitanya Ahuja",
                "Paul Pu Liang",
                "Sanika Natu",
                "Louis-Philippe Morency."
            ],
            "title": "Multimodal lecture presentations dataset: Understanding multimodality in educational slides",
            "venue": "ArXiv, abs/2208.08080.",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge J. Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "European Conference on Computer Vision.",
            "year": 2014
        },
        {
            "authors": [
                "L\u00e1szl\u00f3 Lov\u00e1sz."
            ],
            "title": "Submodular functions and convexity",
            "venue": "Mathematical programming the state of the art, pages 235\u2013257. Springer.",
            "year": 1983
        },
        {
            "authors": [
                "Richard E. Mayer."
            ],
            "title": "Multimedia learning",
            "venue": "Visible Learning Guide to Student Achievement.",
            "year": 2019
        },
        {
            "authors": [
                "George L. Nemhauser",
                "Laurence A. Wolsey",
                "Marshall L. Fisher."
            ],
            "title": "An analysis of approximations for maximizing submodular set functions\u2014i",
            "venue": "Mathematical Programming, 14:265\u2013294.",
            "year": 1978
        },
        {
            "authors": [
                "Saurabh Panjwani",
                "Luana Micallef",
                "Karl Fenech",
                "Kentaro Toyama."
            ],
            "title": "Effects of integrating digital visual materials with textbook scans in the classroom",
            "venue": "International Journal of Education and Development using ICT, 5:55\u201371.",
            "year": 2009
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever."
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139 of",
            "year": 2021
        },
        {
            "authors": [
                "Mrinmaya Sachan",
                "Avinava Dubey",
                "Eduard H. Hovy",
                "Tom M. Mitchell",
                "Dan Roth",
                "Eric P. Xing."
            ],
            "title": "Discourse in Multimedia: A Case Study in Extracting Geometry Knowledge from Textbooks",
            "venue": "Computational Linguistics, 45(4):627\u2013665.",
            "year": 2020
        },
        {
            "authors": [
                "Mrinmaya Sachan",
                "Kumar Dubey",
                "Eric Xing."
            ],
            "title": "From textbooks to knowledge: A case study in harvesting axiomatic knowledge from textbooks to solve geometry problems",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
            "year": 2017
        },
        {
            "authors": [
                "Mrinmaya Sachan",
                "Eric Xing."
            ],
            "title": "Learning to solve geometry problems from natural language demonstrations in textbooks",
            "venue": "Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 251\u2013261, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Florian Schneider",
                "\u00d6zge Ala\u00e7am",
                "Xintong Wang",
                "Chris Biemann."
            ],
            "title": "Towards multi-modal textimage retrieval to improve human reading",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Minjoon Seo",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Oren Etzioni",
                "Clint Malcolm."
            ],
            "title": "Solving geometry problems: Combining text and diagram interpretation",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Krishna Srinivasan",
                "Karthik Raman",
                "Jiecao Chen",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "WIT: Wikipedia-based image text dataset for multimodal multilingual machine learning",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Re-",
            "year": 2021
        },
        {
            "authors": [
                "Junke Wang",
                "Dongdong Chen",
                "Zuxuan Wu",
                "Chong Luo",
                "Luowei Zhou",
                "Yucheng Zhao",
                "Yujia Xie",
                "Ce Liu",
                "YuGang Jiang",
                "Lu Yuan"
            ],
            "title": "2022a. OmniVL: One foundation model for image-language and videolanguage tasks",
            "year": 2022
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Li Dong",
                "Johan Bjorck",
                "Zhiliang Peng",
                "Qiang Liu",
                "Kriti Aggarwal",
                "Owais Khan Mohammed",
                "Saksham Singhal",
                "Subhojit Som",
                "Furu Wei"
            ],
            "title": "2022b. Image as a foreign language: BEiT pretraining for all vision and vision",
            "year": 2022
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier."
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.",
            "year": 2014
        },
        {
            "authors": [
                "Yan Zeng",
                "Xinsong Zhang",
                "Hang Li."
            ],
            "title": "Multigrained vision language pre-training: Aligning texts with visual concepts",
            "venue": "ArXiv, abs/2111.08276.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11931\u201311944 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Students use textbooks as one of the primary mediums of learning. It is thus imperative that textbooks are designed to provide a rich learning and engaging environment. Visuals enhance learning through a number of means, including the ability to retain information, as well as its ability to promote comprehension and knowledge transfer (Carney and Levin, 2002; Dimopoulos et al., 2003; Katsioloudis, 2007; Hibbing and Rankin-Erickson, 2008; Panjwani et al., 2009; Mayer, 2019). Automatic approaches for retrieving and assigning images from the web to textbook chapters can therefore assist textbook designers in the creation of better textbooks. However, this is a very challenging task\n\u2217 Work done during an internship at ETH Z\u00fcrich 0Code & data: github.com/eth-nlped/textbook-enrichment\nas an ideal illustrative visual should not only be related to the textbook material, but also have pedagogical value (see Figure 1).\nAgrawal et al. (2010, 2011) already enhance textbooks with images from the Internet. They use similarity scores of textual captions from images and the text present in the textbooks for image assignment. This approach requires the image captions to be available. Beyond the problem of availability, the captions are also highly context dependent which reduces their utility in our setting. In this work, we aim to reinvigorate this area, and present and analyse a real dataset of textbooks. We do so by setting up the problem of image assignment using textbook and Wikipedia images.\nIn contrast to previous work, we rely on the recent advances in vision-language models, such as CLIP (Radford et al., 2021) and DALL-E (Ramesh et al., 2021). We analyze our dataset to gain insights into the organization of concepts and illustrative images within the textbooks. This analysis inspires the formulation of a new optimization\n11931\nproblem focused on modeling of illustrations in textbooks. The solution to this problem maximizes the coverage of the illustrations while minimizing redundancy during image-to-paragraph assigments. Our approach uses CLIP to retrieve and appropriately assign images to long-form text, particularly a textbook section. Then, the overall assignment is obtained in following stages: 1) Given a piece of text, produce a set of concepts\nthat should be addressed by a visualization, 2) Given an image and a concept, determine their\nmutual relevance, and 3) Given a piece of text (e.g., a textbook section),\nproduce an adequate assignment of images.\nEach of the three sub-problems listed above can be solved with a variety of approaches \u2014 indeed we explore several variants, which we describe in Section 4. Because of the modularity of this approach, each of the sub-problems can be improved independently and adapted to the dataset in future work. Overall, we contribute the following: \u2022 A dataset that contains text and images drawn\nfrom 35 textbooks covering math, business, social sciences, and science in addition to a secondary image bank of \u223c312K images taken from Wikipedia (Section 3). \u2022 Formalization of multiple textbook enrichment optimization goals (Section 4). \u2022 Human evaluation and an in-depth examination of the possible failure modes and challenges of the proposed methods (Section 5)."
        },
        {
            "heading": "2 Related Works",
            "text": "Vision Language Models: Alignment between texts and images has seen rapid progress recently with models such as CLIP (Radford et al., 2021) and DALL-E (Ramesh et al., 2021). However, their usages are limited to a short and specific text prompt to which the performance is usually quite sensitive. We focus on the problem of retrieving images for very long textual inputs, specifically a textbook section, where it is unclear which part of the text specifically describes the relevant image.\nImage Text Matching for Long Texts: Recently, Wang et al. (2022a,b); Zeng et al. (2022) trained better language-vision representations with more nuanced associations, such as multiple vision tasks or finer image-text alignments. However, this progress is mainly confined to datasets, like MSCOCO (Lin et al., 2014) and Flickr-30K (Young\net al., 2014), that contain natural images and their captions, which are rather short. Additionally, Schneider et al. (2021) show that current multimodal models perform poorly at retrieving relevant images for longer and more complex textual inputs. The reason for this poor performance is the pre-training on shorter and very specific image captions. This is a strong requirement to our work, which is focused on even longer text inputs. We explore the problem of assigning images to lengthy text, which highlights issues such as ensuring comprehensive coverage of concepts and avoiding redundant image illustrations. To move even closer to a real-world setting, we perform this task with actual textbooks and a human study.\nEnriching Text with Images: The task of textbook enrichment was first explored by Agrawal et al. (2010, 2011), who assume that web images have associated relevant captions. We note that the caption of images are largely dependent on the context where the image was originally assigned.1 The language of the caption may not even match the textbook\u2019s language. To alleviate all this, we do not assume that the images have associated relevant captions. Seo et al. (2015); Kembhavi et al. (2017); Lee et al. (2022) also studied associating images with textual information. However, their primary goal has largely been to comprehend the image content, and thus differs from our objective. Finally, there has been more past work on NLP applied to textbooks (Sachan and Xing, 2017; Sachan et al., 2017, 2020). However, the goal of these works also differ significantly from ours."
        },
        {
            "heading": "3 Dataset",
            "text": "We now present the process of curation and structure of our dataset with an analysis."
        },
        {
            "heading": "3.1 Data Collection",
            "text": "OpenStax Books. We source both the text and assigned images from 35 textbooks from an online textbook publisher openstax.org, covering four subjects: business, social sciences, sciences, and maths. Each textbook is organized into chapters, sections, subsections, and paragraphs. See their distribution in our dataset in Table 1. For each sub-\n1For example, the same image of a poster from Wikipedia has different captions on different pages: \u201cThe munitions industry heavily recruited women workers, as represented by the U.S. government\u2019s Rosie the Riveter propaganda campaign\u201d and \u201cJ. Howard Miller\u2019s We Can Do It! poster from 1943s\u201d.\nsection of the textbook,2 we identify the following key elements:\n\u2022 Text: Raw text from the subsection. \u2022 Phrases: Raw text from the subsection decom-\nposed with overlapping sliding windows.3\n\u2022 Concepts: Key concepts taught in the subsection are the bolded words or phrases, headings, index terms, and key terms (both marked explicitly in the book as such). \u2022 Images: Image(s) assigned within subsection.\nWikipedia images. To mimic the task of textbook enrichment, we use a dataset of images from Wikipedia that are relevant to the concepts in the OpenStax Books dataset. This dataset serves as a proxy for images from the web. We search for relevant Wikipedia articles for each concept, with a maximum of 20 articles retrieved per concept. From these articles, we extract images and their captions by searching the article in the WIT dataset (Srinivasan et al., 2021) or directly from the article. The final dataset has approximately \u223c312K unique images included in the relevant articles.\nImage Bank. We combine OpenStax Books images and the Wikipedia images to form the Image Bank. Our objective is to retrieve and assign relevant images from the Image Bank to each section that is present in the OpenStax Books dataset."
        },
        {
            "heading": "3.2 Dataset Analysis",
            "text": "We profile the dataset according to a series of questions, which will inform the problem formulation.\n2Example of a subsection: openstax.org/books/conceptsbiology/pages/5-1-overview-of-photosynthesis.\n3A window-size of 75 tokens and an overlap-ratio of 1/3.\nQ1. How are concepts distributed? The patterns in concept mentions are similar across subjects (Figure 2b). The distribution of concepts within subsections (Figure 2a) reveals an average of 5.6 concepts per subsection. An average concept is mentioned 2.7 times within a subsection (Figure 2b) \u2014 that is, concepts are infrequently mentioned in the subsection. Notably, each section concept is mentioned in only 1.7 subsections on average (Figure 2c), emphasizing the high localization of concepts to specific subsections.\nQ2. What influences the number of assigned images in a subsection? On average each section consists of 5.5 subsections and 3.3 assigned images (Table 1). To answer the question at hand, we conduct a regression analysis with the number of images in a subsection as the predicted variable, and the following as features:\n\u2022 concepts/words/paragraphs: their total # in the subsection. \u2022 concepts_uniq: # of unique concepts mentioned in the subsection. \u2022 %sec_concepts: % of unique concepts from the section in the subsection. \u2022 %sec_concept: % of total concept mentions from the section in the subsection. \u2022 %sec_words: % of total words in the section which are in the subsection. \u2022 %sec_paragraphs: % of paragraphs in the section in the subsection. \u2022 position of the subsection in the section from 0 (beginning) to 1 (end). \u2022 subject of the book.\nBased on the results in Table 3, the number of assigned images to a subsection can be best predicted from total number of concepts, words, and paragraphs of the subsection. Unexpectedly, this is not true for the number of unique concepts. Furthermore, the position of the subsection within the section is negatively correlated to the image count \u2014 that is, the subsections located later in the section have fewer images. The subject of the book also impacts the subsection\u2019s image count, with differing coefficients for each subject. Overall, the regression model yields Pearson correlation of 0.59 with p<10\u22124 \u2014 a high degree of predictability.\nQ3. Are images exclusive to the assigned subsection? We use CLIP\u2019s similarity scores for imagephrase relevance (detailed in Section 4). For each image in the textbook we distinguish between the\npresent subsection and the one before and after. Then, we assign images to the subsection with the highest-matching phrase. The percentages of mostsimilar images in the before and after subsections is nearly equal (Figure 3), which is contrary to the intuition that the subsections after the current one would refer to the concepts in the image. The difference between the before/after and present subsections is the greatest in the business category, indicating uniqueness of images assigned to a particular subsection compared. Such uniqueness, as determined by the CLIP scores, is most absent in mathematics books. Overall, the images do not exclusively best-match the phrases from the goldassigned subsection.\nQ4. Are concept mentions associated with assigned images? We rank subsection phrases using CLIP similarity scores with subsection images. We use this ranking to calculate the percentage of concepts that were mentioned in the top-similar phrases associated with the gold images in the subsection. This way, we evaluate whether textphrases with higher association to the gold-image also had more concept mentions. Indeed, there is a correspondence between the gold images and\nphrase with concept mentions (Figure 4). This warrants further usage of CLIP scores as a measure for matching concepts to images."
        },
        {
            "heading": "4 Image Retrieval and Assignment",
            "text": "We first describe an image retrieval model and then formalize our task and the optimization approach. CLIP (Radford et al., 2021) is a stateof-the-art vision-language model trained on many image-caption pairs from the web by maximizing the dot-product similarity the image and caption encodings. We further fine-tune CLIP on image-text pairs from the OpenStax Books dataset."
        },
        {
            "heading": "4.1 Image Assignment Formulation",
            "text": "Our formulation focuses on the assignment of images to subsections.4 We begin with notation:\nsubsections u in a section s = \u27e8u1, . . . , u|s|\u27e9 (1) concepts c in a subsection u = {c1, . . . , cK} (2)\n4We can assign images to the entire book by concatenating all chapters into one long \u201csection\u201d. One can also assign images to paragraphs by treating them as a subsection each with a single paragraph.\nWe decompose the text of a subsection into phrases using a sliding window approach. These phrases may mention a particular concept:\nphrases t in a subsection u = \u27e8t1, . . . , tL\u27e9 (3) Let m(tl, ck) denote mention of concept c in t\nm(tl, ck) = { 1 if ck mentioned in tl 0 otherwise (4)\nFor a fair comparison, we assign the same number of images to each subsection as in the gold assignment. This can also be automated with the image count prediction (Section 3.2/Q2)."
        },
        {
            "heading": "4.2 Local Assignment",
            "text": "The most straightforward solution is to select an image for each subsection independently by maximizing the subsection text-image similarity. Specifically, we assign each subsection u, with an image i \u2208 I, which maximises the following function:\nS({i}, u) = \u2211\nt\u2208u sim(i, t) (5)\nHere, I denotes set of all the images in the image bank. Moreover, sim(i, t) denotes probability (normalised dot-product similarity across images) of any image i and certain phrase t as given by the fine-tuned CLIP model.\nWhile local assignment is fast and simple, our qualitative analysis reveals that it lacks global coherence and may assign images depicting overlapping concepts to the same section. For example, if every subsection mentions the concept \u201cmolecule\u201d, then all subsections can be assigned the same image of a molecule. This finding aligns with our previous results (Section 3.2/Q3) and is supported by the redundancy metrics in Section 5."
        },
        {
            "heading": "4.3 Global Assignment",
            "text": "The analysis in Section 3.2 revealed that the mostrelevant phrases for gold images are not restricted to the assigned subsection and that the concepts are localized within their respective sections. Therefore, for better global coherence in our assignments, we assign images based on concepts rather than phrases. Specifically, we select a subset of images that covers most of the concepts (coverage) while avoiding overlaps (redundancy). To define coverage and redundancy functions for concepts in a section, we first define a boolean function for image, i, covering a concept, c, as follows:\ncov(c, i) = Isim(i,t)\u2265\u03c4 \u00b7m(t, c). Informally, cov(c, i) is 1 iff concept c is covered by image i, otherwise 0. Next, we formalize the coverage and redundancy.\nCoverage. Coverage for a section s and a subset of images I \u2032 is the number of unique concepts in section s which are covered by images in I \u2032:\nC(I \u2032, s)= \u2223\u2223{c \u2208 s | \u2203i \u2208 I \u2032 : cov(c, i) = 1} \u2223\u2223 (6)\nRedundancy. Redundancy of a section s and a a set of images I \u2032 is the total number of times concepts in s are multiply covered:\nR(I \u2032, s) = \u2211\nc\u2208s\n\u2211 i\u2208I\u2032 cov(c, i)\u2212 C(I \u2032, s) (7)\nWe now introduce the concept of set submodularity which will be necessary for proving approximation bounds of the optimization.\nDefinition 4.1. A function f is said to be set submodular if and only if \u2200A \u2286 B \u2200x : f(A\u222a{x})\u2212 f(A) \u2265 f(B \u222a {x})\u2212 f(B). Informally, the function yields diminishing returns for item x.\nTheorem 4.2. The coverage function C is set submodular. That is for I \u2032\u2032 such that I \u2032\u2032 \u2286 I \u2032 \u2286 I and i \u2208 I it holds that C(I \u2032\u2032 \u222a {i}) \u2212 C(I \u2032\u2032) \u2265 C(I \u2032 \u222a {i})\u2212 C(I \u2032). Proof. Let C\u2032 and C\u2032\u2032 be sets of concepts from a subsection covered by images in I \u2032 and I \u2032\u2032 respectively. As per the definition of C, we have C\u2032\u2032 \u2286 C\u2032. Let Ci be the concepts covered by image i.\nC(I \u2032 \u222a {i}, s)\u2212 C(I \u2032, s) = (8) = \u2223\u2223C\u2032 \u222a Cq \u2223\u2223\u2212 \u2223\u2223C\u2032 \u2223\u2223 (from def.) (9)\n\u2264 \u2223\u2223C\u2032\u2032 \u222a Cq \u2223\u2223\u2212 \u2223\u2223C\u2032\u2032 \u2223\u2223 (from C\u2032\u2032 \u2286 C\u2032) (10) = C(I \u2032\u2032 \u222a {i}, s)\u2212 C(I \u2032\u2032, s) (from def.) (11)\nTheorem 4.3. The negative redundancy function \u2212R is set submodular. That is for I \u2032\u2032 such that I \u2032\u2032 \u2286 I \u2032 \u2286 I and i \u2208 I it holds that \u2212R(I \u2032\u2032 \u222a {i}) +R(I \u2032\u2032) \u2265 \u2212R(I \u2032 \u222a {i}) +R(I \u2032). Proof. Similarly to Theorem 4.2, for redundancy function R we observe that:\nR(I \u2032 \u222a {iq}, s)\u2212R(I \u2032, s) = (12) = \u2223\u2223C\u2032 \u2229 Cq \u2223\u2223 (from def.) (13)\n\u2265 \u2223\u2223C\u2032\u2032 \u2229 Cq \u2223\u2223 (from C\u2032\u2032 \u2286 C\u2032) (14) = R(I \u2032\u2032 \u222a {iq}, s)\u2212R(I \u2032\u2032, s) (from def.) (15)\nObservation 4.4. Both C and R are monotone. For the global assignment, we choose images I \u2032 \u2286 I such that the following is maximised:\nG(I \u2032, s) = C(I \u2032, s)\u2212R(I \u2032, s) (16)\nG (Equation 13) is a submodular function because it is a sum of two submodular functions. Finding the optimal solution to G is NP-hard (Lov\u00e1sz, 1983). However, since G is a submodular function, greedy algorithm can lead to fairly good 1 \u2212 1/e \u2248 63%-approximation of optimisation of G under cardinality constraints |I \u2032| \u2264 B, where B is the budget (Nemhauser et al., 1978). Once images I \u2032 are greedily computed for a section, we assign an image i \u2208 I \u2032 to the subsection, u, which maximises C({i}, u); to the subsection in which the image i covers the most concepts."
        },
        {
            "heading": "4.4 Joint Assignment",
            "text": "The local assignment captures relevance while the global one also captures redundancy. To optimize both of them, we formulate the following objective with a trade-off hyper-parameter \u03b2 \u2265 0:\nJ(I \u2032, s) = S(I \u2032, s) + \u03b2 \u00b7G(I \u2032, s) (17) = \u2211\ni\u2208I\u2032\n\u2211 t\u2208s sim(i, t) + \u03b2 \u00b7G(I \u2032, s)\nNote that J is a submodular function, since it is the sum of two other submodular functions: \u03b2 \u00b7G and S. The former is submodular due to the nonnegativity of \u03b2 and previously proven submodularity of G. The submodularity of S is proven below.\nTheorem 4.5. The local assignment function S is set submodular. That is, for a set of images I \u2032 and I \u2032\u2032 such that I \u2032\u2032 \u2286 I \u2032 \u2286 I and any image i \u2208 I \u2212 I \u2032 it holds that S(I \u2032\u2032 \u222a {i}) \u2212 S(I \u2032\u2032) \u2265 S(I \u2032 \u222a {i})\u2212 S(I \u2032). Proof.\nS(I \u2032 \u222a {i}, s)\u2212 S(I \u2032, s) = \u2211\nt\u2208s sim(i, t) (18)\n\u2264 S(I \u2032\u2032 \u222a {i}, s)\u2212 S(I \u2032\u2032, s) (19)\nConsidering the submodularity of J , we select images greedily, similarly to optimizing G. Once a set of images, I \u2032, is greedily computed for a section, we assign an image i \u2208 I \u2032 to the subsection, u, which maximises S(I \u2032, u) + \u03b2 \u00b7C(I \u2032, u); to the subsection in which the image i covers the most\nconcepts and has most similar text. Note that the local and global assignments are specific cases of this formulation. This formulation achieves our desideratum \u2014 images are assigned to specific subsections also with global context consideration."
        },
        {
            "heading": "5 Human Evaluation",
            "text": "The goal of improving textbooks is to help students learn. Testing a wide range of models directly by monitoring learning progress would require a very expensive long-term evaluation. Instead we turn to an intrinsic crowd-sourced evaluation where we ask teachers what they think about the qualities of the assignment."
        },
        {
            "heading": "5.1 Setup",
            "text": "We selected 32 crowd-workers from Prolific who are native English speakers and work in education. We compare 4 different assignments: the gold one by a human and three automatic ones (Section 4). Each participant is assigned a single section and evaluates all 4 systems on this section. This methodology may cause an unwanted priming effect, which we address in Section 5.2. We chose this setup deliberately because a lot of the annotation time is spent on reading the section text and we wanted to share this cost by annotating multiple assignments at once. Our evaluation consists of close-ended (limited number of answers) questions that pertain to both the local (subsection) and global (section) textual context (full annotation guidelines are in Appendix A):\nLocal evaluation: \u2022 Is this image relevant to educational concepts\ndescribed in this subsection text? \u2022 Is this image redundant compared to previous\nimages in this subsection? \u2022 What is the type of this image?\nGlobal evaluation: \u2022 Is this image relevant to educational concepts\ndescribed in this section? \u2022 Is this image redundant compared to previous\nimages in this section? \u2022 Is this image didactically useful for explaining\nthis section text?\nThe annotators first answer the local questions for all images and then the global questions. This way we made sure that they scanned the entire section and had a some overview of all images and how they relate to each other before answering the\nglobal questions. The annotation pipeline (for 4 assignments, local/global) is shown in Figure 6 and the user-interface in Figure 5."
        },
        {
            "heading": "5.2 Evaluation Results",
            "text": "We first verify the evaluation setup validity by checking whether the position in the evaluation queue has an effect on the evaluation scores. Recall that the annotators were shown the same textbook section with 4 alternate images assignments. While Figure 7 shows some variance along the independent variable of evaluation position, the differences are not significant, justifying our evaluation setup.\nWe then focus on two most important evaluation criteria: relevancy and redundancy. The results for those, shown in Figure 8, clearly show preference for the Gold image assignment, suggesting that automatic assignment is still inferior to humans. From the automatic methods, the Local performs the best relevancy-wise. However, it is outperformed by Joint with respect to redundancy. We also note that there is very little difference between Local and Global evaluation category. This may be caused by evaluation bias (i.e. annotators are likely to give similar score for both local and global questions).\nNext, we examine all the remaining evaluation categories in Table 2. While Gold is the best across\nall, the significance of the difference varies. The Joint assignment is never the worst, suggesting it to be a robust choice."
        },
        {
            "heading": "5.3 Qualitative Analysis",
            "text": "The Joint optimization method aims to reduce the reader\u2019s cognitive load compared to the Local method that aggregates scores from all text-phrases. The Local method results in repetitive covering of a single concept from the section with top-images. In contrast, Joint and Global assign images covering a wider range and variety of concepts in the section, enabling a greater level of text enrichment. Appendix B shows examples of assignments by these approaches. We remark that structured image types, such as graphs, multiple images, or those that are less identifiable, receive lower ratings systematically (Figure 9). We now elaborate on the two major limitations in our models.\nVaried domain of images. One limitation of our approach is demonstrated in Figure 10 where it struggles to model non-natural images such as diagrams, graphs, and plots. These images often rep-\nresent abstract concepts, relationships, and events which cannot be well modeled by models like CLIP that are majorly trained on natural images.\nLong textual description of concepts. Another source of error was that some concepts had long textual descriptions. For example, the description of Stokes\u2019 theorem spans multiple paragraphs. Learning to associate image with a part of text may lead to loose and spurious associations, resulting in poor downstream assignment performance. This highlights the need for vision-language representations that can effectively model long text descriptions and establish better image-text associations."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We presented a dataset and a new task of enriching textbooks with visuals from the web. We proposed several technical solutions for this problem using neural image retrievers combined with a new assignment optimization setup. Annotations by workers in the education industry verified that, even though the human assignment is still of the highest quality, the automatic assignments are not far behind. There are multiple venues for making further progress on this problem. First, individual concept importances and text-image relevance models could be improved and plugged into the existing algorithms. The varying domains of images and lengthy textual descriptions of concepts present challenges that could pave the way for exploring new approaches for learning image-text associations. Professional textbook designers can be included to further refine the assignment optimization objective and pose this as a human AI collaboration problem."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers for their feedback on our paper. MS acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung; and an ETH Grant (ETH-19 21-1) for this work.\nLimitations and Ethics Statement\nWhile automatically enhancing textbooks with images holds promise, we point out: \u2022 Image selection bias: Images from the web are\nat risk of being biased because they do not necessarily come from the same distribution as textbook graphics. However, images from Wikipedia are possibly more suitable for this purpose because they are of encyclopedic nature. \u2022 Intellectual property: Practitioners who use our automatic image assignment method for textbooks should take care to always follow the associated copyrights and attributions. \u2022 Pedagogical usefulness: While we employed workers to intrinsically judge the quality of the assignments, the results should be replicated with an extrinsic evaluation (beyond the scope of our study) which also considers the impact on student learning and information retention. \u2022 Quality control: The target audience of textbooks are students, who are a sensitive group. In the current formulation, the optimization will always produce some assignment but there is no mechanism for quality assurance. This could result in inappropriate images being assigned and expert human scrutiny should be employed."
        },
        {
            "heading": "A Annotation Guidelines",
            "text": "The goal of your task is to help us evaluate our research on Artificial Intelligence (AI) based methods for creating more engaging and effective textbooks for students. You will be evaluating the performance of various AI-based methods that use images obtained from the internet to assign images to sections of a children\u2019s textbook.\nIn the first stage of your evaluation, you will be provided with text from a section of the textbook that has been divided into multiple subsections. Each subsection will be enclosed within a bounding box and will include images at the end. Your task is to evaluate the relevance, usefulness, and type of each image with respect to the text and images only within the same subsection.\nAfter you finish answering questions in the first stage, new questions will appear with all the images (before and after shown in below and above image), these will be about second stage of evaluation. In the second stage of the evaluation, you will be reviewing the same subsections and images as before. Your task is to evaluate the relevance, redundancy and usefulness of each image in relation to the text and images within all the subsections in this section. Here, you will be asked to answer the new questions. Note: Please make sure to read the subsections that do not have any images during the first stage, as they will be useful for answering the questions in the second stage. Choices concerning relevancy questions: \u2022 relevant: some concept described in the subsection/section\ntext is picturised in the image; \u2022 somewhat-relevant: image is loosely on the same sub-\nject/topic domain as the subsection/section text, but is not exactly about the same concepts; \u2022 irrelevant: not at all related to the subsection/section text. \u2022 undecided: unable to make a decision, maybe because of\nmy low understanding of the text or image. Choices for redundancy questions: \u2022 yes: this image illustrates exactly the same concept as one\nof the previously shown images in the section/subsection; \u2022 no: this image illustrates atleast some unique concepts\nas compared to the previously shown images in the section/subsection; \u2022 undecided: unable to make a decision, maybe because I do not understand the concepts illustrated by this image. Choices for the didactic-usefulness questions: \u2022 yes: image will be appropriate for teaching the section to a\nstudent - I would like it here (with an appropriate caption) if I was learning from this textbook; \u2022 no: image is not appropriate for teaching the section text; \u2022 undecided: unable to make the decision, maybe because of\nmy less understanding of the text or image.\u201d\nB Images Assigned with Joint\n\u2022 Section \u201cEngagement in a Democracy\u201d from textbook \u201cAmerican Government 3e\u201d \u2013 Subsection \u201cWhy Get Involved?\u201d \u2013 Fig. 11 \u2013 Sub. \u201cPathways to Engagement\u201d \u2013 Fig. 13 \u2013 Subsection \u201cFactors of Engagement\u201d \u2013 Fig. 14 \u2022 Section \u201cSystems of Gas Exchange\u201d taken from textbook \u201cBiology 2e\u201d \u2013 Subsection \u201cLearning Objectives\u201d \u2013 Fig. 16. \u2013 Subsection \u201cDirect Diffusion\u201d \u2013 Fig. 19. \u2013 Subsection \u201cSkin and Gills\u201d \u2013 Fig. 20. \u2013 Subsection \u201cTracheal Systems\u201d \u2013 Fig. 23. \u2013 Subsection \u201cMammalian Systems\u201d \u2013 Fig. 17. \u2013 Sub. \u201cLungs: Bronchi and Alveoli\u201d \u2013 Fig. 24. \u2022 Section \u201cStokes\u2019 Theorem\u201d taken from textbook \u201cCalculus Volume 3\u201d \u2013 Subsection \u201cStokes\u2019 Theorem\u201d \u2013 Fig. 18. \u2013 Subsection \u201cStokes\u2019 Theorem Proof\u201d \u2013 Fig. 25. \u2013 Subsection \u201cInterpretation of Curl\u201d \u2013 Fig. 15. \u2022 Section \u201cCorporate Law and Corporate Responsibility\u201d taken from textbook \u201cBusiness Ethics\u201d \u2013 Subsection \u201cThe Advantages of Corporate Sta-\ntus\u201d \u2013 Fig. 12. \u2013 Subsection \u201cBalancing the Many Responsibili-\nties of a Corporation\u201d \u2013 Fig. 21. \u2013 Subsection \u201cThe Two Sides of the Corporate\nResponsibility Debate\u201d \u2013 Fig. 22."
        },
        {
            "heading": "C CLIP Fine-tuning",
            "text": "The CLIP model is composed of two parts: (1) the Image Encoder responsible for encoding the pth input image into a 512-dimensional vector Ip; and (2) the Text Encoder that encodes the qth input text into a 512-dimensional vector Tq. The model is trained with contrastive loss on a dataset of 400 million image-text pairs from the web. During the training, relevant image-caption pairs maximize Ip \u00b7Tq while Ip \u00b7Tq is minimize for unrelated Ip\u2032 and Tq\u2032 . During our fine-tuning, we create mini-batches of image-text pairs extracted from the subsections in OpenStax Books Dataset.\nDuring inference, we encode all images in the Image Bank as {I1, I2, . . . , IN} and all textqueries belonging to a particular subsection as {T1, T2, . . . , TM}. Next, we get similarity scores for all images in the Image Bank with respect to the kth text-query by calculating Sk = \u27e8I1 \u00b7Tk, I2 \u00b7 Tk, . . . , IN \u00b7 Tk\u27e9. We then normalize Sk to get P k = SOFTMAX(Sk). Finally, for each subsection, we compute the relevance scores of all images in the Image Bank by aggregating the P k values, resulting in P = AGG(P 1, P 2, . . . , PM ), where AGG is an aggregate function such as the mean of N -dimensional vectors.\nIn search for the best image retrieval model, we provide details for various methods of creating image-text pairs from a subsection.\nC.1 Evaluation Metrics\nWe now explain the techniques for the retrieval model evaluation. We consider the images initially\nassigned in the same subsection as gold-images for that particular subsection. Therefore, for each subsection, the Image Bank is categorized into goldimages and not gold-images. To gauge the retrieval quality of a subsection from a given retrieval approach, we use these metrics. Ultimately we use the average of all of them.\n\u2022 Recall@K: fraction of gold-images retrieved in top-K retrievals. \u2022 Recall@R: fraction of gold-images retrieved in top-R retrievals; where R is the number of goldimages in the subsection. \u2022 Precision@K: fraction of retrieved images (K in total) that are gold-images. \u2022 Precision@R: fraction of retrieved images (R in total) which are gold-images; where R is the number of gold-images in the subsection. \u2022 Mean Gold Rank: average rank of each gold image given relevancy sorting.\nC.2 Zero-Shot CLIP\nIn the experiments discussed of this section, we use a pre-trained CLIP (without fine-tuning) to fetch relevant images for each subsection. The experiment results are presented in Table 4. For inference, we adopt a similar approach to the one described earlier. Below, we elaborate on the main differences in experimental settings for these studies:\n1. Concepts: We use all the concepts from a subsection as its text-queries (each concept is a separate text-query) and use mean for AGG. 2. Clustered-Concepts: We use all the concepts from a subsection as its text-queries (each concept is a separate text-query). Next, we form clusters of text queries using k-means (k = 10) clustering on text encodings. For inference, we apply AGG on the relevance scores P to textqueries belonging to each cluster. Using the cluster\u2019s aggregated relevance score, we retrieve one image at a time from each cluster in a roundrobin fashion. This experiment tests if giving equal importance to various \u201cconcepts-clusters\u201d leads to increased variation in retrieved results and better performance. 3. Concatenated-Concepts: We concatenate different concepts from a subsection together and use these concatenated phrases as the textqueries for each section and use mean as the agg function. This experiment tests if giving more context (multiple terms provide better con-\ntext about the section content) with input textqueries improves the performance. 4. Phrases: We follow the same setting as Exp C.2.1, except with overlapping phrases (each phrase is a separate text-query) drawn from subsection\u2019s raw-text instead of the concepts.\nC.3 Fine-tuned CLIP\nWe now finetune CLIP with the experiment results are shown in Table 5.\n1. Concepts: Fine-tuning: Let {i1, i2, . . . , in} and {t1, t2, . . . , tm} respectively be the set of images and concepts included in a subsection from OpenStax Books Dataset. We fine-tune CLIP using the following such \u2018correct\u2019 image-text pairs from all the sections in the train split of OpenStax Books Dataset: {(i1, t1), (i1, t2), . . . , (i1, tm), (i2, t1), (i2, t2), . . . (i2, tm), . . . (in, t1), . . . , (in, tm)}. Inference: We follow the exact same setting as Exp C.2.1 for inference; except that we compute scores across different data splits. 2. Concatenated Concepts: We follow the same setting as Exp C.3.1 except that, during both fine-tuning and inference, we concatenate concepts together to form text-queries for encoding (like in Exp C.2.3). 3. Phrases: We follow the same setting as Exp C.3.1, except that, both during fine-tuning and inference we use overlapping phrases drawn from subsection\u2019s raw-text instead of the concepts. 4. Pre-fine-tune: We pre-fine-tune the CLIP model on image-caption pairs from the Wikipedia images dataset. Following this, we perform fine-tuning and inference the same as that described in Exp C.3.2. 5. Frozen Encoders: We freeze the parameters of both text and image encoders except the last linear layers. Next, we carry out fine-tuning and inference same as Exp C.3.2. 6. Frozen Encoders: We freeze the parameters of both text and image encoders except the last linear and self-attention layers. Next, we carry out fine-tuning and inference same as Exp C.3.2.\nBased on our experimentation, using results on the test split, we determine that the gold-mean rank can be improved through the following means: (a) fine-tuning CLIP, as it outperforms zero-shot; (b) concatenating concepts to provide more contextual\ninformation, rather than using each concept as a separate text-query; (c) pre-finetuning on imagecaption from the Wikipedia images; (d) fine-tuning all layers of CLIP, as opposed to only fine-tuning the last few layers. Ultimately, we select the model from Exp C.3.3 as our retrieval model, even though Exp C.3.5 achieved a slightly better gold-mean rank. This is because Exp C.3.5 relies on Wikipedia image caption pairs, which may not be readily available for all web images."
        }
    ],
    "title": "Enhancing Textbooks with Visuals from the Web for Improved Learning",
    "year": 2023
}