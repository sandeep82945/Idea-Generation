{
    "abstractText": "We are amidst an explosion of artificial intelligence research, particularly around large language models (LLMs). These models have a range of applications across domains like medicine, finance, commonsense knowledge graphs, and crowdsourcing. Investigation into LLMs as part of crowdsourcing workflows remains an under-explored space. The crowdsourcing research community has produced a body of work investigating workflows and methods for managing complex tasks using hybrid human-AI methods. Within crowdsourcing, the role of LLMs can be envisioned as akin to a cog in a larger wheel of workflows. From an empirical standpoint, little is currently understood about how LLMs can improve the effectiveness of crowdsourcing workflows and how such workflows can be evaluated. In this work, we present a vision for exploring this gap from the perspectives of various stakeholders involved in the crowdsourcing paradigm \u2014 the task requesters, crowd workers, platforms, and end-users. We identify junctures in typical crowdsourcing workflows at which the introduction of LLMs can play a beneficial role and propose means to augment existing design patterns for crowd work.",
    "authors": [],
    "id": "SP:e5f9bf05e65da70ad7d60d283fcb03f6a5bef955",
    "references": [
        {
            "authors": [
                "Abubakar Abid",
                "Maheen Farooqi",
                "James Zou"
            ],
            "title": "Persistent anti-muslim bias in large language models",
            "venue": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2021
        },
        {
            "authors": [
                "Ashutosh Adhikari",
                "Achyudh Ram",
                "Raphael Tang",
                "Jimmy Lin"
            ],
            "title": "Docbert: Bert for document classification",
            "venue": "arXiv preprint arXiv:1904.08398",
            "year": 2019
        },
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "year": 2022
        },
        {
            "authors": [
                "Max Bartolo",
                "Tristan Thrush",
                "Sebastian Riedel",
                "Pontus Stenetorp",
                "Robin Jia",
                "Douwe Kiela"
            ],
            "title": "Models in the loop: Aiding crowdworkers with generative annotation assistants",
            "year": 2021
        },
        {
            "authors": [
                "Michael S Bernstein",
                "Greg Little",
                "Robert C Miller",
                "Bj\u00f6rn Hartmann",
                "Mark S Ackerman",
                "David R Karger",
                "David Crowell",
                "Katrina Panovich"
            ],
            "title": "Soylent: a word processor with a crowd inside",
            "venue": "In Proceedings of the 23nd annual ACM symposium on User interface software and technology",
            "year": 2010
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "ChaitanyaMalaviya",
                "Asli Celikyilmaz",
                "Yejin Choi"
            ],
            "title": "COMET:Commonsense transformers for automatic knowledge graph construction",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems",
            "year": 2020
        },
        {
            "authors": [
                "Jun Chen",
                "Chaokun Wang",
                "Yiyuan Bai"
            ],
            "title": "CrowdMR: Integrating crowdsourcing with MapReduce for AI-hard problems",
            "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence",
            "year": 2015
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). 4 Power-up! What Can Generative Models Do for Human Computation Workflows? CHI\u201923",
            "year": 2018
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Damai Dai",
                "Yifan Song",
                "Jingjing Xu",
                "Zhifang Sui",
                "Lei Li"
            ],
            "title": "Calibrating Factual Knowledge in Pretrained Language Models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Enrique Estell\u00e9s-Arolas"
            ],
            "title": "Towards an integrated crowdsourcing definition",
            "venue": "Journal of Information science 38,",
            "year": 2012
        },
        {
            "authors": [
                "Nat Friedman"
            ],
            "title": "Copilot: Your AI pair programmer",
            "year": 2021
        },
        {
            "authors": [
                "Ujwal Gadiraju",
                "Ricardo Kawase",
                "Stefan Dietze"
            ],
            "title": "A taxonomy of microtasks on the web",
            "venue": "In Proceedings of the 25th ACM conference on Hypertext and social media",
            "year": 2014
        },
        {
            "authors": [
                "Ujwal Gadiraju",
                "Jie Yang"
            ],
            "title": "What can crowd computing do for the next generation of AI systems",
            "year": 2020
        },
        {
            "authors": [
                "Katy Ilonka Gero",
                "Vivian Liu",
                "Lydia Chilton"
            ],
            "title": "Sparks: Inspiration for science writing using language models",
            "venue": "In Designing Interactive Systems",
            "year": 2022
        },
        {
            "authors": [
                "Donghoon Ham",
                "Jeong-Gwan Lee",
                "Youngsoo Jang",
                "Kee-Eung Kim"
            ],
            "title": "End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2",
            "venue": "In Proceedings of the 58th annual meeting of the association for computational linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Shuguang Han",
                "Xuanhui Wang",
                "Mike Bendersky",
                "Marc Najork"
            ],
            "title": "Learning-to-Rank with BERT in TF-Ranking",
            "venue": "arXiv preprint arXiv:2004.08476",
            "year": 2020
        },
        {
            "authors": [
                "Arian Hosseini",
                "Ankit Vani",
                "Dzmitry Bahdanau",
                "Alessandro Sordoni",
                "Aaron Courville"
            ],
            "title": "On the Compositional Generalization Gap of In-Context Learning",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Aniket Kittur",
                "Susheel Khamkar",
                "Paul Andr\u00e9",
                "Robert Kraut"
            ],
            "title": "CrowdWeaver: visually managing complex crowd work",
            "venue": "In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work",
            "year": 2012
        },
        {
            "authors": [
                "Aniket Kittur",
                "Boris Smus",
                "Susheel Khamkar",
                "Robert E Kraut"
            ],
            "title": "Crowdforge: Crowdsourcing complex work",
            "venue": "In Proceedings of the 24th annual ACM symposium on User interface software and technology",
            "year": 2011
        },
        {
            "authors": [
                "JunKong",
                "JinWang",
                "Xuejie Zhang"
            ],
            "title": "Hierarchical BERTwith an adaptive fine-tuning strategy for document classification",
            "venue": "Knowledge-Based Systems",
            "year": 2022
        },
        {
            "authors": [
                "Jieh-Sheng Lee",
                "Jieh Hsiang"
            ],
            "title": "Patent claim generation by fine-tuning OpenAI GPT-2",
            "venue": "World Patent Information",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "NamanGoyal",
                "MarjanGhazvininejad",
                "AbdelrahmanMohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "LukeZettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Lorraine Li",
                "Adhiguna Kuncoro",
                "Cyprien de Masson d\u2019Autume",
                "Phil Blunsom",
                "Aida Nematzadeh"
            ],
            "title": "Do Language Models Learn Commonsense Knowledge",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Greg Little",
                "Lydia B Chilton",
                "Max Goldman",
                "Robert CMiller"
            ],
            "title": "Exploring iterative and parallel human computation processes",
            "venue": "In Proceedings of the ACM SIGKDD workshop on human computation",
            "year": 2010
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A Smith",
                "Yejin Choi"
            ],
            "title": "Wanli: Worker and ai collaboration for natural language inference dataset creation",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Zhichao Liu",
                "Ruth A Roberts",
                "Madhu Lal-Nag",
                "Xi Chen",
                "Ruili Huang",
                "Weida Tong"
            ],
            "title": "AI-based language models powering drug discovery and development",
            "venue": "Drug Discovery Today 26,",
            "year": 2021
        },
        {
            "authors": [
                "Li Lucy",
                "David Bamman"
            ],
            "title": "Gender and representation bias in GPT-3 generated stories",
            "venue": "In Proceedings of the Third Workshop on Narrative Understanding",
            "year": 2021
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy"
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "arXiv preprint arXiv:2004.09456",
            "year": 2020
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Kyunghyun Cho"
            ],
            "title": "Passage Re-ranking with BERT",
            "venue": "arXiv preprint arXiv:1901.04085",
            "year": 2019
        },
        {
            "authors": [
                "TB OpenAI"
            ],
            "title": "2022",
            "venue": "Chatgpt: Optimizing language models for dialogue. OpenAI ",
            "year": 2022
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "Daniela Retelny",
                "Michael S Bernstein",
                "Melissa A Valentine"
            ],
            "title": "No workflow can ever be enough: How crowdsourcing workflows constrain complex work",
            "venue": "Proceedings of the ACM on Human-Computer Interaction",
            "year": 2017
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": " Why should i trust you?\" Explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
            "year": 2016
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Anchors: High-precision model-agnostic explanations",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jennifer Wortman Vaughan"
            ],
            "title": "Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research",
            "venue": "J. Mach. Learn. Res. 18,",
            "year": 2017
        },
        {
            "authors": [
                "Jesse Vig",
                "Sebastian Gehrmann",
                "Yonatan Belinkov",
                "Sharon Qian",
                "Daniel Nevo",
                "Yaron Singer",
                "Stuart Shieber"
            ],
            "title": "Investigating gender bias in language models using causal mediation analysis",
            "venue": "Advances in neural information processing systems",
            "year": 2020
        },
        {
            "authors": [
                "Xu Wang",
                "Simin Fan",
                "Jessica Houghton",
                "Lu Wang"
            ],
            "title": "Towards Process-Oriented, Modular, and Versatile Question Generation that Meets Educational Needs",
            "venue": "arXiv preprint arXiv:2205.00355 (2022)",
            "year": 2022
        },
        {
            "authors": [
                "JasonWei",
                "XuezhiWang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Tongshuang Wu",
                "Marco Tulio Ribeiro",
                "Jeffrey Heer",
                "Daniel S Weld"
            ],
            "title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Roman V Yampolskiy"
            ],
            "title": "AI-complete, AI-hard, or AI-easy\u2013classification of problems in AI",
            "venue": "In The 23rd Midwest Artificial Intelligence and Cognitive Science Conference,",
            "year": 2012
        },
        {
            "authors": [
                "Yi Yang",
                "Mark Christopher Siy Uy",
                "Allen Huang"
            ],
            "title": "Finbert: A pretrained language model for financial communications",
            "year": 2020
        },
        {
            "authors": [
                "Ann Yuan",
                "Andy Coenen",
                "Emily Reif",
                "Daphne Ippolito"
            ],
            "title": "Wordcraft: story writing with large language models",
            "venue": "In 27th International Conference on Intelligent User Interfaces",
            "year": 2022
        },
        {
            "authors": [
                "Julia El Zini",
                "Mariette Awad"
            ],
            "title": "On the explainability of natural language processing deep models",
            "venue": "Comput. Surveys 55,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 7.\n02 24\n3v 1\n[ cs\n.H C\n] 5\nJ ul\n2 02\n3"
        },
        {
            "heading": "Power-up! What Can Generative Models Do for Human Computation",
            "text": ""
        },
        {
            "heading": "Workflows?",
            "text": "GARRETT ALLEN, GAOLE HE, and UJWAL GADIRAJU, Delft University of Technology, Netherlands\nWe are amidst an explosion of artificial intelligence research, particularly around large language models (LLMs). These models have a range of applications across domains like medicine, finance, commonsense knowledge graphs, and crowdsourcing. Investigation into LLMs as part of crowdsourcing workflows remains an under-explored space. The crowdsourcing research community has produced a body of work investigating workflows and methods for managing complex tasks using hybrid human-AI methods. Within crowdsourcing, the role of LLMs can be envisioned as akin to a cog in a larger wheel of workflows. From an empirical standpoint, little is currently understood about how LLMs can improve the effectiveness of crowdsourcing workflows and how such workflows can be evaluated. In this work, we present a vision for exploring this gap from the perspectives of various stakeholders involved in the crowdsourcing paradigm \u2014 the task requesters, crowd workers, platforms, and end-users. We identify junctures in typical crowdsourcing workflows at which the introduction of LLMs can play a beneficial role and propose means to augment existing design patterns for crowd work.\nAdditional Key Words and Phrases: crowdsourcing, generative AI, large language models, workflows, human computation"
        },
        {
            "heading": "ACM Reference Format:",
            "text": "Garrett Allen, Gaole He, and Ujwal Gadiraju. 2023. Power-up! What Can Generative Models Do for Human Computation Workflows?. In Proceedings of the Generative AI and HCI Workshop at the ACM Conference on Human Factors in Computing Systems, April 23\u201328, 2023, Hamburg, Germany. ACM, New York, NY, USA, 6 pages."
        },
        {
            "heading": "1 INTRODUCTION AND BACKGROUND",
            "text": "Artificial intelligence (AI) research is being reinvigorated with current advances in large language models (LLMs). Since their inception, LLMs have increased in size, effectiveness, and applications. For instance, BERT [10], initially trained for masked language prediction, has been applied to other domains such as neural ranking [18, 31] and document classification [2, 22]. OpenAI\u2019s1 GPT family of models have been used in language tasks including goal-oriented dialogue [17], patent claim generation [23], and story generation [29]. The most recent GPT variant, ChatGPT [32], has seen an explosive growth in popularity, indicating the potential for a promising future where LLMs are deployed as work assistants. Due to such powerful generative capability, more researchers have started exploring generative LLMs in work assistant roles. For example, powerful generative LLMs have shown human-comparable writing skills in story generation [44] and scientific writing [16]. LLMs have also exhibited promising assistive capability in complex tasks like coding [13], drug discovery [28], and question generation for education needs [39].\nThe common thread running through all variations in LLMs is the need of high quality data for training and evaluation. Crowdsourcing has been widely adopted in machine learning practice to obtain high-quality annotations by\n1https://www.openai.com/\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2023 Association for Computing Machinery. Manuscript submitted to ACM\nquest the participation of a distributed crowd of individuals, who can contribute with their knowledge, expertise, and experience [12]. Such individuals, called crowd workers, are asked to complete a variety of tasks in return for monetary or other forms of compensation. Tasks are often decomposed into smaller atomic units and can vary in their purpose, including labelling images, editing text, or finding information on specific topics [14]. Tasks can be standalone, or organized as a series of smaller sub-tasks, depending on their overall complexity and the design choices made by requesters. More complex problems, such as software engineering or system design problems, require task workflows.\nCrowdsourcing workflows are distinct patterns that manage how large-scale problems are decomposed into smaller tasks to be completed by workers. The crowd-powered word processor Soylent applies the Find-Fix-Verify workflow to produce high-quality text by separating tasks into generating and reviewing text [6]. The Iterate-and-Vote workflow has been deployed in creating image descriptions, where workers are asked to write descriptions of images to assist those who are blind [26]. Subsequent voting tasks are used to decide on the optimal description. Chen et al. [9] introduce CrowdMR, which combines the Map-Reduce workflow with crowdsourcing to facilitate the solving of problems that require both human and machine intelligence, i.e., \u201cAI-Hard\" problems [42]. With CrowdForge, Kittur et al. [21] provide a framework for crowdsourcing to support complex and interdependent tasks. The authors follow up with the tool CrowdWeaver [20] for managing complex workflows, supporting such needs as data sharing between tasks and providing monitoring tools and real-time task adjustment capability. Taking a more holistic look at workflows, Retelny et al. [34] investigate the relationship between the need for adaptation and complex workflows within crowdsourcing, finding that the current state of crowdsourcing processes are inadequate for providing the necessary adaptation that complex workflows require.\nWithin crowdsourcing, the role of LLMs can be envisioned as akin to a cog in a larger workflow. Typically, LLMs are used for supporting individual writing or classification tasks within a workflow, as previous examples expressed. Researchers are also exploring the application of LLMs in assisting crowdworkers. Liu et al. [27] combine the generative power of GPT-3 and the evaluative power of humans to create a new natural language inference dataset that produces more effectivemodelswhen used as a training set. In a similar vein, Bartolo et al. [5] introduce a \u201cGenerative Annotation Assistant\" to help in the production of dynamic adversarial data collection, significantly improving the rate of collection. These works measure the effectiveness of the models and the individual tasks, yet there remains an open gap regarding the understanding of how LLMs improve the effectiveness of crowdsourcing workflows and how such workflows can be evaluated.\nIn this work, we present a vision for exploring the gap from the stakeholders\u2019 perspectives, e.g., task requesters, crowd workers, and end-users. In so doing, we highlight the junctures of crowdsourcing workflows at which introducing LLMs can be beneficial. We also propose means to augment existing design patterns for crowd work."
        },
        {
            "heading": "2 INCORPORATING LARGE LANGUAGE MODELS IN CROWDSOURCINGWORKFLOWS",
            "text": "As LLMs are pre-trained on large text corpora, they show great capability in understanding context-specific semantics. When further fine-tuned for specific uses with additional, smaller datasets, highly effective and domain-targeted models can be produced. Additionally, some LLMs (e.g., BART [24], GPT-3 [8]) are also good at generating responses to input queries, which can be fluent, human-like, and even professional. As it stands, LLMs have been effectively deployed within multiple domains such as medicine [4], finance [43], and others requiring commonsense reasoning [7]. As such, LLMs are an opportune and potentially very useful tool to use within crowdsourcing where domain knowledge may not always be available.\nbox neural backbone, LLMs suffer from a lack of transparency, which leads to difficulty in explaining how they achieve the performances they do [45]. Such opacity also makes it difficult to track the factual error of LLMs, which inhibits the potential for improving the models [11]. Further, language models are known to capture several different forms of biases [1, 30, 38]. Most existing LLMs tend to perform poorly on tasks that require commonsense knowledge [33], which is a common practice for children. Last but not least, current language models achieve poor compositional generalization [19], which is required for solving complex tasks. Noticeably, LLMs fall short in aspects that humans are good at, e.g., commonsense reasoning [25] and complex task planning [3, 40]. Putting LLMs into practice requires either addressing or working within these limitations."
        },
        {
            "heading": "2.1 The Lens of Complex Crowdsourcing Workflows",
            "text": "LLMs can easily fit into existing crowdsourcing workflows. Take the Find-Fix-Verify workflow as an example. This workflow iswell-suited for writing tasks, whether it be editing, revisions, or new content. Each step is an opportunity to include LLMs for improvements in the process. Let us take the example of revising a news story. During the \u201cFind\" stage, a workers would be tasked with reading the story and finding any errors, e.g., grammar, spelling, or false statements. Once these errors are identified, a new crowd of workers is recruited for the next stage: to \u201cFix\" the errors. We are now left with an updated draft of the news story that has fewer errors than the initial draft.Which brings us to the final stage of the workflow, \u201cVerify\", where yet another group of workers validate the work of the prior groups. In this particular example, it is fairly clear where an LLM can be swapped for the workers at each stage. A retrieval or error classification LLM can be deployed for finding the errors, a generative LLM can be used to produce repaired text, and yet another classification LLM can finish it all off as the verifier. However, not all tasks take this form, or follow this particular workflow. Adapting other workflows, i.e., Iterate-and-Vote or MapReduce, can be done in a similar manner. Even so, adaptations such as these prompt the question: Once introduced, what are the effects of LLMs within crowdsourcing workflows for each stakeholder of the crowdsourcing process?\nOn the surface, this appears like straightforward question. Crowdsourcing has many different stakeholders involved: the requesters, theworkers, and the end-users. The impact of including an LLM into workflows has the potential to affect each stakeholder in different ways. From the perspective of the requester, the monetary cost of completing tasks will be reduced as potentially fewer workers will need to be recruited. The tasks may take less time to complete which will result in further monetary savings. A reduction in time to gather data, complete tasks, and/or a reduced need for workers may have a negative impact on the income flow for workers, however. With available tasks taking less time and there being fewer tasks, it creates the potential for crowd workers to earn less. This can be offset by adjusting incentive structures on platforms. On the other hand, the reduction in costs for requesters could lead to more tasks being posted, leading to more high-quality labels. In turn, LLMs benefit from the better labels and improve in performance as well, creating a positive cycle that benefits both crowd workers and requesters. Further work is required to gain a better understanding of the financial opportunities and risks surrounding LLMs as part of crowdsourcing workflows.\nOf course, there are trade-offs that come alongside any benefits. The trade-off for the requesters is a learning curve around the LLMs. Timewill need to be dedicated to strategize and familiarize with the integration of LLMs inworkflows. A trade-off that crowdsourcing platforms will share, accompanied by the additional cost of the development to add the LLMs to their products. An LLM must be trained before it can be appropriately used within a crowdsourcing workflow. This training, or fine-tuning, creates an overhead for either the crowdsourcing platform or the requester. While the overhead is initially a burden for most stakeholders, there will be an efficiency gain in the long term.\nFurther consideration is needed regarding the transparency of LLMs versus humans. When crowd workers complete tasks, such as annotation or other decision-oriented varieties, requesters have the capability of performing a follow-up with the workers to elicit reasoning for the outcomes provided. This is not a simple job for LLMs. While there exist methods for model explainability [35, 36, 41], none have demonstrated a level of effectiveness on par with what a requester would achieve with a human-human conversation. This same lack of transparency also has the potential of confounding workflows at the worker level. For example, take a scenario where an LLM is tasked with making a prediction, and a human worker to validate the prediction of the model, and the model provides a prediction that is not in line with what the worker expects to see. In such a scenario, the worker may want to interrogate the model to gain insight into why the prediction was made. However, there is currently no such clear way for the worker to request such an explanation from the LLM.\nAlso worth considering is the concept of accountability. Whenever a machine is introduced into a system, be it a factory, an airplane, or a crowdsourcing workflow, the question of accountability requires definition. Adding LLMs into crowdsourcing workflows raises the question of who orwhat is accountable if things do not go according to plan? Is the model, the requester, the platform, or the crowd workers to be held responsible for mishaps? There are many questions around the benefits, viability, risks, and harms involved with introducing LLMs into crowdsourcing workflows. These questions provide rich research opportunities for the generative AI and human computation research communities.\nThe realm of creative crowdsourcing tasks is another place of opportunity for LLMs. Generative models can help by providing suggestions or starting points to spark brainstorming or idea generation sessions. Alternatively, classification LLMs can be used to consolidate the ideas produced. For tasks that are more engineering or design focused, LLMs may be able to serve as \u201crubber duck\" sounding boards. LLMs may also provide performance boosts in areas such as content creation, music composition, or protein discovery. The possibilities of how LLMs can be included in crowdsourcing are vast, yet the viability of these use cases warrants further investigation."
        }
    ],
    "year": 2023
}