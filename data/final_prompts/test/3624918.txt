Imagine you are a research scientist, read the following paper and generate top 5 possible future research ideas after brainstorming:  
``` The peer-review process is currently under stress due to the increasingly large number of submissions to top-tier venues, especially in Artificial Intelligence (AI) and Machine Learning (ML). Consequently, the quality of peer reviews is under question, and dissatisfaction among authors is not uncommon but rather prominent. In this work, we propose "ReviVal" (expanded as "REVIew eVALuation"), a system to automatically grade a peer-review report for its informativeness. We define review informativeness in terms of its Exhaustiveness and Strength, where Exhaustiveness signifies how exhaustively the review covers the different sections and qualitative aspects1 of the paper and Strength signifies how sure the reviewer is of their evaluation. We train ReviVal, a multitask deep network for review informativeness prediction on the publicly available peer reviews, which we curate from the openreview2 platform. We annotate the review sentence(s) with labels for (a) which sections and (b) what quality aspects of the paper those refer. We automatically annotate our data with the reviewer’s sentiment intensity to capture the reviewer’s conviction. Our approach significantly outperforms several intuitive baselines for this novel task. To the best of our knowledge, our work is a first-of-its-kind to automatically estimate the informativeness of a peer review report. 1 introduction Peer review is the central system of scientific research validation. However, it is not undebated that the system is without flaws. Several studies highlight the problems like bias [38], inconsistencies [28, 37], arbitrariness [9] of the process, thus, degrading the integrity and trust on the central system of research validation. Hence, it is not uncommon that because of these issues, sometimes good research gets ignored, and sub-par papers find a place. One such unfortunate example is the retractions of referred COVID19 articles [30]. Area chairs of conferences or editors of journals are responsible for mitigating these issues by assigning papers to relevant, expert reviewers and then evaluating those reviewers’ comments to make informed decisions. However, the exponential rise in paper submissions has put the peer-review system under severe stress in recent years, leading to a dearth of experienced reviewers. Sometimes, the chairs or editors are left with no other option than to delegate this critical yet voluntary job to inexperienced reviewers to the extent of even to graduate students3. Many reviewers evaluate the submitted research based on some poor indicators, e.g., whether the work is SOTA or not [35]. Fortunately, the Natural Language Processing (NLP) community has proactively paid attention to these problems. They have been arguing for changes to the current reviewing practices (e.g., the introduction of ACL Rolling Review to enforce good reviewing practises 4, EMNLP 2020/2023 releasing strict rubrics for research evaluation [12]), organizing tutorials [11] for researchers to write high-quality reviews [33]. However, peer review suffers another major problem: reviewers’ tendency to invest less time in this critical voluntary job [17]. Sometimes, the reviewers themselves are unsure of their judgment on the merit of the work, which becomes evident from their poor reviews [8] [29]. Thus, these reform programs, which are primarily targeted
3https://www.the-scientist.com/critic-at-large/opinion--exorcising-ghostwritingfrom-peer-review-66940 4https://www.aclweb.org/adminwiki/index.php?title=ACL_Rolling_Review_ Proposal
towards improving the reviewing practices of human reviewers, might not be sufficient. An important direction towards re-establishing trust in the peerreview process should be an attempt to establish confidence in the peer-review reports. Therefore, while promoting good quality and constructive reviews, a means to detect and penalize inferior reviews should also exist. Given the ever-increasing submission load and the importance of this problem, an automated system to judge reviews for quality would largely help. In this work, we propose a mechanism to detect the surface-level indicators of good and bad reviews. Our approach takes a review and grades it for informativeness. Defining peer-review quality is not straightforward [36]; it will vary across domains. In this work, we assert that: a good review should comment on the critical sections (e.g., Problem Statement, Methodology, Experiments, Results/Analysis, etc.) and address the critical aspects of the paper (e.g., Novelty, Theoretical Soundness, etc.) while clearly bringing out the reviewer’s stand on the work. As we mostly fixate our attention towards estimating the inclusiveness of the review (in terms of section, aspect, sentiment), instead of quality, we instead call our investigation a means to determine the informativeness of the review. Here, our contributions are two-fold:
• We define informativeness via two components: Exhaustiveness and Strength. Our deep neural architecture takes as input the full-paper text and the full review text, and we train in a multi-task setting. We employ reviewer confidence score prediction and review recommendation score as scaffolds for Exhaustiveness and Strength, respectively. • We propose a scoring mechanism to manifest exhaustiveness of a review via its section and aspect coverage of the paper. 2 relatedwork Although we did not find any prior work addressing the exact task, we do find literature on AI-support for the peer review system [15, 19, 32, 34]. Additionally, [36] enlists some major points to address in a peer review. There have been numerous discussions on the implications of AI in the peer review pipeline. Authors [39] study the effect of revealing the identity of the reviewer in the peer review process and if it can enhance the quality of peer reviews whereas [18] study the effect of blind review model. However, all these are studies on different peer review models to improve review quality. In another relevant work [44], authors predicts the helpfulness of a peer-review using manual features. Their objective is somewhat similar to ours in term of Exhaustive component. Our work in this direction is in line with the recent efforts to incorporate Artificial Intelligence (AI) in the peer review pipeline [13, 15, 32, 34] till generation of decision-awaremeta-reviews [6, 22– 24, 27]. In our earlier work, we try to model the harshness [41], constructiveness [3], informativeness [2], uncertainty [14] of peer reviews and confidence of reviewers [5] from a computational perspective. We also investigated aspect extraction [26, 42] and decision/recommendation score prediction [4, 7, 25] from peer reviews, which served as subtasks in our current work. Publishers and allied stakeholders are already considering AIassistants in the peer-review cycle [16]. An AI that could grade the reviews based on some quality standards would help regulating
a thrust on poor reviews. It would also help human reviewers to take up their job seriously. Editors would be able to discard trivially written reviews and ask emergency reviewers to step in. It could also be a step towards building a reviewer profile data of their review quality, thereby increasing trust in reviewers. 3 peer review informativeness As we discussed earlier, quality of a review is a somewhat subjective term and may encompass many other aspects. However, our definition of informativeness builds upon the expectations from a good review in general AI/ML venues, which are probably similar across venues from other domains. As per the rubrics defined in [36], we expect a good review should demonstrate the reviewer’s understanding of the work with a concise summary, an evaluation of the writing quality (clarity), comments on the novelty of the proposal, and a fair evaluation of the experiments/results, and all the other major sections of the paper. To this end, we define review informativeness in terms of its Exhaustiveness and implicit Strength. We view Exhaustiveness as to how detailed the review is, and Strength as how opinionated it is in reflecting the reviewer’s perspective. Eventually, we want to distinguish reviews written by experienced reviewers vs. those written by novice/non-expert/slack reviewers. A good review should be detailed and also high on the opinion. It should be comprehensive enough to discuss the merit of the work based on aspects like Impact, Empirical Soundness, etc., and opinionated enough to bring out the reviewer’s stand on the work. However, the review’s exhaustiveness and opinionatedness can textually reflect in varied proportions across different reviews. For example, when the work is excellent, we do not expect to see many comments about the work in general, but the opinion will be strongly positive. Our objective is to detect non-informative reviews, which are unfortunately very common in academia. To this end, our definition of Exhaustiveness has two components. Review Exhaustiveness component serves as a simple heuristic for whether the reviewer has read/understood the paper. If the reviewer has understood the paper, we assert that the review would be more detailed with comments on several parts of the paper, be it the problem statement, the proposed methodology, or the proposed results etc. This hypothesis works against the kind of reviews illustrated in R1 below. R1: This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights. ICLR 2018, id=BJjquybCW
Another essential role of the review is to assess the impact of the work in a broader academic context. Reviewers assess the proposed work for its novelty, soundness of the research methodology, etc. Aspect Exhaustiveness score puts thrust on this critical component. A review should have information that not only discusses the research but also validates it. Hence, we split the Exhaustiveness of the review into two components: Section Exhaustiveness and Aspect Exhaustiveness. We define the third component, Review Strength, as a measure of the degree of review opinionatedness and hence
quantify the intensity of the review. With intensity, we do not emphasize the review’s positive or negative sentiment but how strong a particular sentiment is. 3.1 section exhaustiveness Wedefine set𝐸 = {Abstract(ABS), Introduction(INT), RelatedWorks(RWK), ProblemDefinition/Idea(PDI), Data/Datasets(DAT), Methodology(MET), Experiments(EXP), Results(RES), Tables and Figures(TNF), Analysis(ANA), Future Work(FWK), Overall(OAL), Bibliography(BIB), External(EXT)}. Here, we consider the general components in a typical AI/ML paper with obvious exceptions. Considering 𝐸, we score section exhaustiveness of review 𝑟 as:
𝑆𝑒 =
( ∑ |𝐸 | 𝑙𝑖 ∈𝐸 𝑤𝑙𝑖 × 𝑓𝑙𝑖 ) × 𝑐 × 𝑛𝑠∑ |𝐸 |
𝑙𝑖 ∈𝐸 𝑓𝑙𝑖 (1)
where,𝑤𝑙𝑖 and 𝑓𝑙𝑖 denote the relative importance and the frequency of the label 𝑙𝑖 ∈ 𝐸 in 𝑟 . 𝑛𝑠 equals total number of sentences in the review, and 𝑐 denotes the coverage of the review defined as
𝑐 = |{𝑙 ∈ 𝐸, 𝑓𝑙 ≠ 0}|
|𝐸 | (2)
We consider relative importance in our scoring function to give more importance to certain sections, like Methodology, Experiments, etc., discussed in almost all the reviews. This is intuitive as ICLR is an empirical venue; it is imperative for the reviewers to talk about methodology, experiments, related work for comparison, etc. Hence, we agree that the proposed scoring depends on the venue (whether theoretical or empirical). 3.2 aspect exhaustiveness For Aspect Exhaustiveness, we define 𝑆 = {Appropriateness (APR), Originality/Novelty (NOV), Significance/Impact (IMP), Meaningful Comparison (CMP), Presentation and Formatting (PNF), Recommendation (REC), Empirical/Theoretical Soundness (EMP), Substance (SUB), Clarity(CLA)} as the set containing this reviewer subjectivity aspects. We again score a review r for aspect exhaustiveness as:
𝑆𝑠 =
( ∑ |𝑆 | 𝑙𝑖 ∈𝑆 𝑤𝑙𝑖 × 𝑓𝑙𝑖 ) × 𝑐 × 𝑛𝑠∑ |𝑆 |
𝑙𝑖 ∈𝑆 𝑓𝑙𝑖 (3)
Symbols carry the same meaning as before. 3.3 review strength Since the sentiment intensity scores could vary arbitrarily across annotators, we do automatic annotation via pre-trained models. To get Review Strength score, we use the pre-trained RoBERTa [31] model, specifically RoBERTa-large. We get the sentiment intensity of each of the sentences in the review. Then, we average them to define the intensity of the review. The intensity values lie in the range [−1, 1]. 4 dataset description To proceed with our investigation, we require the papers and reviews with their confidence and recommendation scores. To do a fair evaluation, we also consider rejected paper reviews. However,
peer review data are sensitive and not very straightforward to obtain, especially that of rejected papers. We collect papers, reviews, recommendation scores, and confidence scores corresponding to the three editions of the International Conference on Learning Representations (ICLR) from the openreview platform.We take only the official ICLR-appointed reviewer comments for our experiments. We also study a subset of these reviews and score these reviews based on our scoring mechanism in Section 2. The dataset details are in Table 1. 4.1 annotation process We appointed three annotators for the taskwhowere duly paid. Two annotators hold a master’s degree in engineering and are currently Ph.D. students (NLP/ML). The third annotator is one of the coauthors. We intentionally chose our annotators from a technical background as they would understand the scholarly texts better. We resolved the confusion in team meetings with the primary investigators. We also conduct a one-month training exercise with the annotators on the peer review process, review aspects, general machine learning paper content, and the corresponding labels. The annotation period lasted for six months. We annotate 1002 reviews from ICLR 2018 against sections and aspects of the paper. The objective was to identify to which section and to which aspect of the paper the review text corresponds. • Review-Paper Section Correspondence Section 3.1 enlists the section labels common in AI/ML papers. The labels are simple and obvious if one is familiar with ML papers. Not all the labels are prominent in the review texts (e.g., ABS, INT, FWK), as the reviewers do not generally comment on certain sections unless there are some explicit issues to
highlight. We find that some labels like MET, EXP, RES, and ANA are very interlinked and sometimes can be hard to distinguish in the context of a review text. • Review-Paper Aspect CategoryWe follow the ACL 2016 reviewing guidelines for our aspect labels (Section 3.2) [20]. Even though ICLR reviewing criteria do not explicitly command these aspects, an ideal review is expected to more or less address those. Please note that it may happen that a review sentence may not conform to any of the aspects prescribed in the ACL 2016 guidelines. We leave out those instances. We adhere to certain additional guidelines:
(1) We perform multi-label sentence-wise annotation for each review document across sections and aspects. However, for certain exceptions (long, compound sentences addressing multiple aspects and sections), we select a text segment to label (but do sentence-level annotations in most cases). (2) We consider sentence context when a single review sentence does not make sense if considered in isolation. e.g., What was the need for this? This statement is unclear if we do not consider the preceding context. (3) We put the confusing instances in the CANNOT DECIDE category and leave out the ambiguous instances from the annotation process after discussion. Figure 3 shows annotation for a sample review. Figure 1 shows the distribution of the scores for these 1002 reviews. We also measure the inter-annotator agreement on a subset of data (100 full reviews for the two exhaustiveness annotations). Considering a multi-label scenario, Inter-Annotator Agreement Krippendorf’s Alpha for Section Exhaustiveness is 0.73 and for Aspect Exhaustiveness is 0.70. 5 key observations and motivations Our initial analysis shows an interplay among the recommendation score, confidence score and review sentiment. Some key observations are:
• When the recommendation score, confidence score and the review sentiment (positive) are high, the number of annotations ∑ |𝐸 | 𝑙𝑖
𝑓𝑙𝑖 from Eq. 1 is low. This seems intuitive, as there is not much to discuss when the paper is outstanding. (Eg. https://openreview.net/forum?id=BJJLHbb0-&noteId= S1f48huxz) • When the recommendation score is low, the confidence score is high, and the review sentiment (negative) is high, this also correlates with fewer annotations. When the reviewer feels the paper is inferior, they become less interested in highlighting all the deficits. • One interesting trend is when the recommendation score is high and the sentiment is positive, but the confidence score is relatively low (1-3). Even in such cases, the total number of annotations is less. One possible explanation could be non-expert reviewers writing the review without clearly understanding the work, hence less attention to detail. (E.g. https://openreview.net/forum?id=H1pri9vTZ&noteId= rkeYOm_lM)
For the prediction of the informativeness scores, we use the scores from the 1002 annotated reviews for supervision. However, the data is less. Given this, exploiting the relationship between the recommendation score, confidence score, sentiment, and the informativeness scores seems promising. Sadly, the recommendation and confidence scores have flaws and can hint at non-desirable relationships, as evident from the third observation above. We exploit the interplay between the recommendation score, confidence score and the informativeness scores in a multi-task framework. Such a multi-task framework can help reduce the homoscedastic aleatoric uncertainty associated with each task, thus nullifying the noisy or undesirable relationships. Our model takes a paper and a review as input, and we train it to predict the confidence score, recommendation score and the informativeness scores. In essence, our main task is to predict the review informativeness scores, and the recommendation and confidence score predictions are our scaffold tasks. 6 methodology  6.1 pre-processing We first convert the PDFs into .json using the ScienceParse5 library. We do not consider the figures tables in the processing as these entities are not correctly parsed. We consider only paper full-text sentences and strip off the headings, metadata, and other nonrelevant information. We show our model architecture in Figure 2. 6.2 encoder The inputs to our model are the full paper and the review text. Let us denote the paper 𝑃 = (𝑠𝑝1 , 𝑠 𝑝 2 , ..., 𝑠 𝑝 𝑛𝑝 ) and the review 𝑅 = (𝑠𝑟1, 𝑠 𝑟 2, ..., 𝑠 𝑟 𝑛𝑟 ) as a sequence of their respective sentences. For a sentence 𝑠𝑝 𝑖 we get a 𝑑 dimensional embedding vector s𝑝 𝑖 ∈ R𝑑 using SciBERT encoder [1]. We use the pretrained SciBERT model to have rich representations of scientific texts, as this language model is trained on a large collection of scientific papers. We get the paper representation P ∈ R𝑛𝑝×𝑑 by concatenating these vectors. P = s𝑝1 ⊕ s 𝑝 2 ⊕ ... ⊕ s 𝑝 𝑛𝑝 , P ∈ R 𝑛𝑝×𝑑 (4)
Similarly, we get the review representation R ∈ R𝑛𝑟 ×𝑑 as
R = s𝑟1 ⊕ s 𝑟 2 ⊕ ... ⊕ s 𝑟 𝑛𝑟 ,R ∈ R𝑛𝑟 ×𝑑 (5) 6.3 context modeling We employ the basic co-attention module [43] to extract the relative representation of the paper and review with respect to each other. We get the affinity matrix E ∈ R𝑛𝑟 ×𝑛𝑝 as follows:
E𝑖 𝑗 = 1 √ 𝑑 F {(s𝑟𝑖 ) 𝑇 }F {s𝑝 𝑗 }, E𝑖 𝑗 ∈ R (6)
Here F is a linear layer (w𝑇 x + b). Thus, E𝑖 𝑗 gives the measure of similarity between the sentence 𝑠𝑟
𝑖 and 𝑠𝑝 𝑗 , which we convert to
attention weights using the following normalizations:
𝑐𝑖 𝑗 = expE𝑖 𝑗∑𝑛𝑝 𝑘=1 expE𝑖𝑘 , 𝑐𝑖 𝑗 ∈ R (7)
5https://github.com/allenai/science-parse
We get the relative representation of review sentence s𝑟 𝑖 with
respect to paper 𝑃 by:
r𝑝 𝑖 = 𝑛𝑝∑︁ 𝑗=1 𝑐𝑖 𝑗 s𝑟𝑖 , r 𝑝 𝑖 ∈ R𝑑 (8)
Similarly, we get the paper representation given the review sentence s𝑟
𝑖 by:
p𝑟𝑖 = 𝑛𝑝∑︁ 𝑗=1 𝑐𝑖 𝑗 s 𝑝 𝑗 , p𝑖 ∈ R𝑑 (9)
We concatenate all the corresponding vectors in r𝑝 𝑖 , p𝑟 𝑖 to get
R𝑝 ∈ R𝑛𝑟 ×𝑑 , P𝑟 ∈ R𝑛𝑟 ×𝑑 . 6.4 sequential feature extractor Now we do the individual processing of the R𝑝 , R, and R𝑐 = R𝑝 ⊕ P𝑟 ⊕ R in our coder module. In simple terms, a coder module is a collection of 𝐾 attention modules. To encode extracted useful information from the review R, a coder module can be described in terms of individual sentences of the reviews s𝑟1, s 𝑟 2, ...s 𝑟 𝑛𝑟 . The 𝑘𝑡ℎ code extraction can be done as
h𝑟𝑖 = ReLU(w 𝑇 s𝑟𝑖 + 𝑏) (10)
𝑤𝑘𝑖 = exp(h𝑟 𝑖 .c𝑘 )∑𝑛𝑟
𝑗=1 exp(h 𝑟 𝑗 .c𝑘 )
(11)
𝑦𝑘 = 𝑛𝑟∑︁ 𝑗=1 𝑤𝑘𝑗 h 𝑟 𝑗 (12)
Thus, we get 𝑦1, 𝑦2, ..., 𝑦𝐾 features representations of a review 𝑅, each encoded using a special trainable coder c𝑘 which is randomly initialized at first. This module is a shallow approximation to the popular self-attention operation [40] in NLP. We perform this operation sequentially for𝑚 times. We get outputs c𝑝 , c𝑅 and cRc for each of R𝑝 ,R,Rc respectively. Note that we are working with full paper and review text; thus, a shallow attention mechanism is used. However, any feasible contextual feature representation method can be used here if computation and resources are not constraints. 6.5 feedforward prediction layers We concatenate the outputs from the𝑚𝑡ℎ coder layer i.e. c𝑝 , c𝑅 and cRc together in one flattened vector c. We pass c to the two coder modules specific to Recommendation Score Prediction and Confidence Score Prediction to get c𝑟𝑒𝑐. and c𝑐𝑜𝑛𝑓 . respectively. Each task has its own prediction layer, which is a multi-layered Perceptron (MLP) layer. For the scaffold tasks prediction, each of these c𝑠 are passed to their corresponding MLPs. For the main task prediction, we pass c ⊕ c𝑟𝑒𝑐. ⊕ c𝑐𝑜𝑛𝑓 . to another MLP for the main task. Thus, the last layers of the model have task-specific trainable parameters, while the parameters in the lower layers are shared. 6.6 training and experimental setup To account for the different numerical ranges of the loss values across multiple scaffold tasks, we combine recommendation task lossL𝑟𝑒𝑐. and confidence task lossL𝑐𝑜𝑛𝑓 . using a method proposed in [21] to get L𝑠𝑐𝑎𝑓 𝑓 𝑜𝑙𝑑 as
L𝑠𝑐𝑎𝑓 𝑓 𝑜𝑙𝑑 = 1
2𝜎2𝑟𝑒𝑐. L𝑟𝑒𝑐. + 1 2𝜎2 𝑐𝑜𝑛𝑓 . L𝑐𝑜𝑛𝑓 . + log𝜎𝑟𝑒𝑐.𝜎𝑐𝑜𝑛𝑓 . (13)
Similarly, for L𝑚𝑎𝑖𝑛 ,
L𝑚𝑎𝑖𝑛 = 1
2𝜎2 𝑒𝑥ℎ. L𝑒𝑥ℎ. + 1
2𝜎2 𝑠𝑢𝑏 𝑗 . L𝑠𝑢𝑏 𝑗 . + 1
2𝜎2 𝑖𝑛𝑡 . L𝑖𝑛𝑡 . + log𝜎𝑒𝑥ℎ.𝜎𝑠𝑢𝑏 𝑗 .𝜎𝑖𝑛𝑡 . (14) Here, 𝜎𝑠 are the trainable parameters to optimize during training. L𝑖 is the mean-squared error loss. We also normalize the Section Exhaustiveness, Aspect Exhaustiveness, and Review Strength scores to lie in the range [1,10]. The normalization is done using
𝑠𝑐𝑜𝑟𝑒 = (𝑚𝑎𝑥 − 𝑠𝑐𝑜𝑟𝑒 𝑚𝑎𝑥 −𝑚𝑖𝑛 ) ∗ 9 + 1 (15)
This normalization of the Review Strength scores means that a larger value indicates strong positive sentiment and a lesser value indicates strong negative sentiment. 6.7 hyperparameters details The dimensionality of the sentence embeddings is 768. We use a sequence of 3 coder layers in the Sequential coder having 64, 32, 8 codes (c𝑘 ). The linear layer in each coder layer transforms the input to a 256-dimensional space. The coder in the prediction layers has eight codes. We train using a batch size of 8, with a learning rate of 0.009, using Adam optimizer. We use l2-penalty of 0.009. These
hyperparameters were found using multiple runs with different combinations and picking the best validation set performance. The training of our main task and scaffold task are done concurrently, i.e., we have L = L𝑚𝑎𝑖𝑛 + L𝑠𝑐𝑎𝑓 𝑓 𝑜𝑙𝑑 (16) where L𝑚𝑎𝑖𝑛 and L𝑠𝑐𝑎𝑓 𝑓 𝑜𝑙𝑑 are defined as above. 7 evaluation As mentioned earlier, our main task is to predict the Informativeness scores for each review. This task is a regression problem, and so are our scaffolds. We evaluate the model on the held-out test set. We split the 1002 reviews into 800/100/102 train/validation/test sets. The training process is monitored using the validation set. Since there is no gold-standard data for evaluating the main task, we use simple baselines as comparing systems. Also, since we have much less data with gold annotated scores, we cannot compare with other sophisticated deep-learning techniques as they are dataintensive. Our primary contribution to the prediction task is using multi-task learning to facilitate the predictions even using less data. We employ the following simple baselines:
(1) Mean Baseline: As most deep neural network models for regression tasks are susceptible to predicting the mean of the prediction target, we use this baseline as a comparing system to see if our model is not suffering from the same. (2) Ridge Regression : We perform simple Ridge Regression with average sentence embeddings as the input to the model. The model for Ridge Regression [10] is a two-layer feedforward network with an L2 regularizer. We keep the model shallow and carefully tune the L2 regularizer weight to prevent overfitting (the chances of which are high due to less training data). (3) Ridge Regression with Score Fusion: In this baseline, we additionally give recommendation score and confidence score as the input to the model, along with the average sentence embeddings described in the previous baseline. (4) CNN as a Sequential Coder: We also experiment by replacing the sequential Feature Extractor (section 6.4) with a Convolutional Neural Network with max-pooling in the multi-task framework. (5) Length Baseline: As both 𝑆𝑒 and 𝑆𝑠 are linearly dependent on 𝑛𝑠 , it’s intuitive to think that 𝑛𝑠 itself is a good measure of the scores. Also, it is common in academia to regard longer reviews as informative and good reviews. We, therefore, use a simple ridge regression model to predict the scores given only the review length as input. 8 results Table 2 shows the results for all the systems. Given the long-tail distribution of the scores, it seems reasonable that the mean baseline performs better. Due to fewer reviews with informativeness scores, ridge regression performs approximately the same as the Mean Baseline. This also means that the Ridge Regression method is under-parameterized to the complexity of the task. Fusing the recommendation and confidence scores into the Ridge Regression leads to improvement, further hinting towards our motivation for multi-task learning. Following the intuition, the length baseline
gives competitive performance among all the comparing systems. We observe that length is somewhat an important metric for the informativeness of the review. However, we also opine that it is an uninformed metric. Each scaffold individually also leads to good improvements to the mean baseline. Combining both tasks in a single model substantially improves the other comparing systems. Interestingly, CNN, as a sequential coder, also performs better. Furthermore, we observe that the Recommendation task results in good improvements in Review Strength scores, whereas the Confidence task has improved performance for Section Exhaustiveness. We can see this as a piece of evidence that Recommendation scores are a proxy for Review Strength, while Confidence scores are the proxy for Review Exhaustiveness. This also seems intuitive (except for the reviewer’s inconsistency and implicit subjectivity in translating their views of the research work to numerical scores), as a high recommendation score means that the review text has a positive polarity (negative for otherwise). Similarly, the confidence score is expected to be high when the reviewer has thoroughly understood/read the paper and has written the review in detail. Thus, Recommendation Score and Confidence Score prediction tasks can also be seen as
scaffolds for Exhaustiveness and Strength of the review, respectively. We hypothesize that they can take care of the inconsistency in a symbiotic fashion by drawing information from the other task. This somewhat simulates the case when area chairs/editors pay lesser importance to the review, which received extreme recommendation scores (pos/neg) from the reviewers, but the reviewers themselves have low confidence. Figure 3 visualizes the attentions for review representation R in the first layer of the sequential coder. We observe that the code representation c𝑘 learns to encode useful information about the review, and the weighted sum then acts as a feature extractor. One can also observe that the sample review has a lesser Section Exhaustiveness and Aspect Exhaustiveness score as the review text is less detailed (with less coverage). It only comments very briefly on the Novelty, Meaningful Comparison, and Empirical/Theoretical soundness aspects. Most of the review text is about summarizing the research work with minimal reviewer scrutiny. 9 conclusions Automatically grading peer reviews for their quality is challenging. There is ongoing research on what defines a good review, and we agree that the definition would vastly vary across domains. The current work is to quantify peer-review informativeness standing on the shoulders of two questions: how exhaustive was the review? and how strong was the review?. We design a scoring mechanism to manifest exhaustiveness in terms of coverage of the review across paper sections and aspects. We then propose a multi-task deep network to predict the informativeness of the peer review with scaffolds for exhaustiveness and strength. We demonstrate that our scoring mechanism is intuitive, and the proposed approach yields encouraging performance.```
5 possible future research ideas from the paper are: 