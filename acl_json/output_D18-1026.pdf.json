{
    "abstractText": "Semantic specialization is a process of finetuning pre-trained distributional word vectors using external lexical knowledge (e.g., WordNet) to accentuate a particular semantic relation in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (i.e., seen words), leaving the vectors of all other words unchanged. We propose a novel approach to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with a adversarial loss: the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks: word similarity, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Edoardo M. Ponti"
        },
        {
            "affiliations": [],
            "name": "Ivan Vuli\u0107"
        },
        {
            "affiliations": [],
            "name": "Goran Glava\u0161"
        },
        {
            "affiliations": [],
            "name": "Nikola Mrk\u0161i\u0107"
        },
        {
            "affiliations": [],
            "name": "Anna Korhonen"
        }
    ],
    "id": "SP:d131c6372c0ca0bbb960e967bcd3cbdeea239d4a",
    "references": [
        {
            "authors": [
                "Rami Al-Rfou",
                "Bryan Perozzi",
                "Steven Skiena."
            ],
            "title": "Polyglot: Distributed word representations for multilingual NLP",
            "venue": "Proceedings of CoNLL, pages 183\u2013192.",
            "year": 2013
        },
        {
            "authors": [
                "Mohit Bansal",
                "Kevin Gimpel",
                "Karen Livescu."
            ],
            "title": "Tailoring continuous word representations for dependency parsing",
            "venue": "Proceedings of ACL, pages 809\u2013 815.",
            "year": 2014
        },
        {
            "authors": [
                "Jiang Bian",
                "Bin Gao",
                "Tie-Yan Liu."
            ],
            "title": "Knowledge-powered deep learning for word embedding",
            "venue": "Proceedings of ECML-PKDD, pages 132\u2013 148.",
            "year": 2014
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the ACL, 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "Elia Bruni",
                "Nam-Khanh Tran",
                "Marco Baroni."
            ],
            "title": "Multimodal distributional semantics",
            "venue": "Journal of Artificial Intelligence Research, 49:1\u201347.",
            "year": 2014
        },
        {
            "authors": [
                "Danqi Chen",
                "Christopher D. Manning."
            ],
            "title": "A fast and accurate dependency parser using neural networks",
            "venue": "Proceedings of EMNLP, pages 740\u2013750.",
            "year": 2014
        },
        {
            "authors": [
                "Xilun Chen",
                "Yu Sun",
                "Ben Athiwaratkun",
                "Claire Cardie",
                "Kilian Weinberger."
            ],
            "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
            "venue": "arXiv preprint arXiv:1606.01614.",
            "year": 2016
        },
        {
            "authors": [
                "Moustapha Cisse",
                "Piotr Bojanowski",
                "Edouard Grave",
                "Yann Dauphin",
                "Nicolas Usunier."
            ],
            "title": "Parseval networks: Improving robustness to adversarial examples",
            "venue": "Proceedings of ICML, pages 854\u2013863.",
            "year": 2017
        },
        {
            "authors": [
                "Ronan Collobert",
                "Jason Weston",
                "L\u00e9on Bottou",
                "Michael Karlen",
                "Koray Kavukcuoglu",
                "Pavel P. Kuksa."
            ],
            "title": "Natural language processing (almost) from scratch",
            "venue": "Journal of Machine Learning Research, 12:2493\u20132537.",
            "year": 2011
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample",
                "Marc\u2019Aurelio Ranzato",
                "Ludovic Denoyer",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Word translation without parallel data",
            "venue": "In Proceedings of ICLR (Conference Track)",
            "year": 2018
        },
        {
            "authors": [
                "Ryan Cotterell",
                "Hinrich Sch\u00fctze",
                "Jason Eisner."
            ],
            "title": "Morphological smoothing and extrapolation of word embeddings",
            "venue": "Proceedings of ACL, pages 1651\u20131660.",
            "year": 2016
        },
        {
            "authors": [
                "John C. Duchi",
                "Elad Hazan",
                "Yoram Singer."
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of Machine Learning Research, 12:2121\u20132159.",
            "year": 2011
        },
        {
            "authors": [
                "Manaal Faruqui."
            ],
            "title": "Diverse Context for Learning Word Representations",
            "venue": "Ph.D. thesis, Carnegie Mellon University.",
            "year": 2016
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Jesse Dodge",
                "Sujay Kumar Jauhar",
                "Chris Dyer",
                "Eduard Hovy",
                "Noah A. Smith."
            ],
            "title": "Retrofitting word vectors to semantic lexicons",
            "venue": "Proceedings of NAACL-HLT, pages 1606\u20131615.",
            "year": 2015
        },
        {
            "authors": [
                "Lev Finkelstein",
                "Evgeniy Gabrilovich",
                "Yossi Matias",
                "Ehud Rivlin",
                "Zach Solan",
                "Gadi Wolfman",
                "Eytan Ruppin."
            ],
            "title": "Placing search in context: The concept revisited",
            "venue": "ACM Transactions on Information Systems, 20(1):116\u2013131.",
            "year": 2002
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky."
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "Journal of Machine Learning Research, 17(1):59:1\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Juri Ganitkevitch",
                "Benjamin Van Durme",
                "Chris Callison-Burch."
            ],
            "title": "PPDB: The Paraphrase Database",
            "venue": "Proceedings of NAACL-HLT, pages 758\u2013764.",
            "year": 2013
        },
        {
            "authors": [
                "Daniela Gerz",
                "Ivan Vuli\u0107",
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "SimVerb-3500: A largescale evaluation set of verb similarity",
            "venue": "Proceedings of EMNLP, pages 2173\u20132182.",
            "year": 2016
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Sanja \u0160tajner"
            ],
            "title": "Simplifying lexical simplification: Do we need simplified corpora",
            "venue": "In Proceedings of ACL,",
            "year": 2015
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative adversarial nets",
            "venue": "Proceedings of NIPS, pages 2672\u2013 2680.",
            "year": 2014
        },
        {
            "authors": [
                "Zellig S. Harris."
            ],
            "title": "Distributional structure",
            "venue": "Word, 10(23):146\u2013162.",
            "year": 1954
        },
        {
            "authors": [
                "Matthew Henderson",
                "Blaise Thomson",
                "Jason D. Wiliams."
            ],
            "title": "The Second Dialog State Tracking Challenge",
            "venue": "Proceedings of SIGDIAL, pages 263\u2013 272.",
            "year": 2014
        },
        {
            "authors": [
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation",
            "venue": "Computational Linguistics, 41(4):665\u2013695.",
            "year": 2015
        },
        {
            "authors": [
                "Colby Horn",
                "Cathryn Manduca",
                "David Kauchak."
            ],
            "title": "Learning a lexical simplifier using Wikipedia",
            "venue": "Proceedings of ACL, pages 458\u2013463.",
            "year": 2014
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros."
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "Proceedings of CVPR, pages 1125\u20131134.",
            "year": 2017
        },
        {
            "authors": [
                "Sujay Kumar Jauhar",
                "Chris Dyer",
                "Eduard H. Hovy."
            ],
            "title": "Ontologically grounded multi-sense representation learning for semantic vector space models",
            "venue": "Proceedings of NAACL, pages 683\u2013693.",
            "year": 2015
        },
        {
            "authors": [
                "Douwe Kiela",
                "Felix Hill",
                "Stephen Clark."
            ],
            "title": "Specializing word embeddings for similarity or relatedness",
            "venue": "Proceedings of EMNLP, pages 2044\u2013 2048.",
            "year": 2015
        },
        {
            "authors": [
                "Joo-Kyung Kim",
                "Gokhan Tur",
                "Asli Celikyilmaz",
                "Bin Cao",
                "Ye-Yi Wang."
            ],
            "title": "Intent detection using semantically enriched word embeddings",
            "venue": "Proceedings of SLT.",
            "year": 2016
        },
        {
            "authors": [
                "Barbara Ann Kipfer."
            ],
            "title": "Roget\u2019s 21st Century Thesaurus (3rd Edition)",
            "venue": "Philip Lief Group.",
            "year": 2009
        },
        {
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang"
            ],
            "title": "Photo-realistic single image super-resolution using a generative adversarial",
            "year": 2017
        },
        {
            "authors": [
                "Ira Leviant",
                "Roi Reichart."
            ],
            "title": "Separated by an un-common language: Towards judgment language informed vector space modeling",
            "venue": "CoRR, abs/1508.00106.",
            "year": 2015
        },
        {
            "authors": [
                "Omer Levy",
                "Yoav Goldberg."
            ],
            "title": "Dependencybased word embeddings",
            "venue": "Proceedings of ACL, pages 302\u2013308.",
            "year": 2014
        },
        {
            "authors": [
                "Chuan Li",
                "Michael Wand."
            ],
            "title": "Precomputed realtime texture synthesis with markovian generative adversarial networks",
            "venue": "Proceedings of ECCV, pages 702\u2013716.",
            "year": 2016
        },
        {
            "authors": [
                "Quan Liu",
                "Hui Jiang",
                "Si Wei",
                "Zhen-Hua Ling",
                "Yu Hu."
            ],
            "title": "Learning semantic word embeddings based on ordinal knowledge constraints",
            "venue": "Proceedings of ACL, pages 1501\u20131511.",
            "year": 2015
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Awni Y. Hannun",
                "Andrew Y. Ng."
            ],
            "title": "Rectifier nonlinearities improve neural network acoustic models",
            "venue": "Proceedings of ICML.",
            "year": 2013
        },
        {
            "authors": [
                "Oren Melamud",
                "Jacob Goldberger",
                "Ido Dagan."
            ],
            "title": "Context2vec: Learning generic context embedding with bidirectional LSTM",
            "venue": "Proceedings of CoNLL, pages 51\u201361.",
            "year": 2016
        },
        {
            "authors": [
                "Oren Melamud",
                "David McClosky",
                "Siddharth Patwardhan",
                "Mohit Bansal."
            ],
            "title": "The role of context types and dimensionality in learning word embeddings",
            "venue": "Proceedings of NAACL-HLT, pages 1030\u2013 1040.",
            "year": 2016
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Gregory S. Corrado",
                "Jeffrey Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Proceedings of NIPS, pages 3111\u2013 3119.",
            "year": 2013
        },
        {
            "authors": [
                "Mehdi Mirza",
                "Simon Osindero."
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "arXiv preprint arXiv:1411.1784.",
            "year": 2014
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Tsung-Hsien Wen",
                "Blaise Thomson",
                "Steve Young."
            ],
            "title": "Neural belief tracker: Data-driven dialogue state tracking",
            "venue": "Proceedings of ACL, pages 1777\u20131788.",
            "year": 2017
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Blaise Thomson",
                "Milica Ga\u0161i\u0107",
                "Lina Maria Rojas-Barahona",
                "PeiHao Su",
                "David Vandyke",
                "Tsung-Hsien Wen",
                "Steve Young."
            ],
            "title": "Counter-fitting word vectors to linguistic constraints",
            "venue": "Proceedings of NAACL-",
            "year": 2016
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Ivan Vuli\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Ira Leviant",
                "Roi Reichart",
                "Milica Ga\u0161i\u0107",
                "Anna Korhonen",
                "Steve Young."
            ],
            "title": "Semantic specialisation of distributional word vector spaces using monolingual and cross-lingual constraints",
            "venue": "Transactions",
            "year": 2017
        },
        {
            "authors": [
                "Vinod Nair",
                "Geoffrey E. Hinton."
            ],
            "title": "Rectified linear units improve restricted Boltzmann machines",
            "venue": "Proceedings of ICML, pages 807\u2013814.",
            "year": 2010
        },
        {
            "authors": [
                "Kim Anh Nguyen",
                "Maximilian K\u00f6per",
                "Sabine Schulte im Walde",
                "Ngoc Thang Vu."
            ],
            "title": "Hierarchical embeddings for hypernymy detection and directionality",
            "venue": "Proceedings of EMNLP, pages 233\u2013243.",
            "year": 2017
        },
        {
            "authors": [
                "Kim Anh Nguyen",
                "Sabine Schulte im Walde",
                "Ngoc Thang Vu."
            ],
            "title": "Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction",
            "venue": "Proceedings of ACL, pages 454\u2013459.",
            "year": 2016
        },
        {
            "authors": [
                "Maximilian Nickel",
                "Douwe Kiela."
            ],
            "title": "Poincar\u00e9 embeddings for learning hierarchical representations",
            "venue": "Proceedings of NIPS, pages 6341\u20136350.",
            "year": 2017
        },
        {
            "authors": [
                "Augustus Odena",
                "Christopher Olah",
                "Jonathon Shlens."
            ],
            "title": "Conditional image synthesis with auxiliary classifier gans",
            "venue": "Proceedings of ICML, pages 2642\u20132651.",
            "year": 2017
        },
        {
            "authors": [
                "Masataka Ono",
                "Makoto Miwa",
                "Yutaka Sasaki."
            ],
            "title": "Word embedding-based antonym detection using thesauri and distributional information",
            "venue": "Proceedings of NAACL-HLT, pages 984\u2013989.",
            "year": 2015
        },
        {
            "authors": [
                "Dominique Osborne",
                "Shashi Narayan",
                "Shay Cohen."
            ],
            "title": "Encoding prior knowledge with eigenword embeddings",
            "venue": "Transactions of the ACL, 4:417\u2013430.",
            "year": 2016
        },
        {
            "authors": [
                "Deepak Pathak",
                "Philipp Kr\u00e4henb\u00fchl",
                "Jeff Donahue",
                "Trevor Darrell",
                "Alexei A. Efros."
            ],
            "title": "Context encoders: Feature learning by inpainting",
            "venue": "Proceedings of CVPR, pages 2536\u20132544.",
            "year": 2016
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of EMNLP, pages 1532\u2013 1543.",
            "year": 2014
        },
        {
            "authors": [
                "Shutova",
                "Anna Korhonen."
            ],
            "title": "Modeling language variation and universals: A survey on typological linguistics for natural language processing",
            "venue": "arXiv preprint arXiv:1807.00914.",
            "year": 2018
        },
        {
            "authors": [
                "Milos Radovanovi\u0107",
                "Alexandros Nanopoulos",
                "Mirjana Ivanovi\u0107."
            ],
            "title": "Hubs in space: Popular nearest neighbors in high-dimensional data",
            "venue": "Journal of Machine Learning Research, 11:2487\u20132531.",
            "year": 2010
        },
        {
            "authors": [
                "Scott Reed",
                "Zeynep Akata",
                "Xinchen Yan",
                "Lajanugen Logeswaran",
                "Bernt Schiele",
                "Honglak Lee."
            ],
            "title": "Generative adversarial text to image synthesis",
            "venue": "Proceedings of ICML, pages 1060\u20131069.",
            "year": 2016
        },
        {
            "authors": [
                "Sascha Rothe",
                "Hinrich Sch\u00fctze."
            ],
            "title": "AutoExtend: Extending word embeddings to embeddings for synsets and lexemes",
            "venue": "Proceedings of ACL, pages 1793\u20131803.",
            "year": 2015
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Anders S\u00f8gaard."
            ],
            "title": "A survey of cross-lingual embedding models",
            "venue": "CoRR, abs/1706.04902.",
            "year": 2017
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen."
            ],
            "title": "Improved techniques for training GANs",
            "venue": "Proceedings of NIPS, pages 2234\u20132242.",
            "year": 2016
        },
        {
            "authors": [
                "Peter H Sch\u00f6nemann."
            ],
            "title": "A generalized solution of the orthogonal Procrustes problem",
            "venue": "Psychometrika, 31(1):1\u201310.",
            "year": 1966
        },
        {
            "authors": [
                "Roy Schwartz",
                "Roi Reichart",
                "Ari Rappoport."
            ],
            "title": "Symmetric pattern based word embeddings for improved word similarity prediction",
            "venue": "Proceedings of CoNLL, pages 258\u2013267.",
            "year": 2015
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna."
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "Proceedings of CVPR, pages 2818\u20132826.",
            "year": 2016
        },
        {
            "authors": [
                "Sara Tonelli",
                "Alessio Palmero Aprosio",
                "Francesca Saltori."
            ],
            "title": "SIMPITIKI: A simplification corpus for Italian",
            "venue": "Proceedings of CLiC-it.",
            "year": 2016
        },
        {
            "authors": [
                "Shyam Upadhyay",
                "Manaal Faruqui",
                "Chris Dyer",
                "Dan Roth."
            ],
            "title": "Cross-lingual models of word embeddings: An empirical comparison",
            "venue": "Proceedings of ACL, pages 1661\u20131670.",
            "year": 2016
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Goran Glava\u0161",
                "Nikola Mrk\u0161i\u0107",
                "Anna Korhonen."
            ],
            "title": "Post-specialisation: Retrofitting vectors of words unseen in lexical resources",
            "venue": "Proceedings of NAACL-HLT, pages 516\u2013527.",
            "year": 2018
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "On the role of seed lexicons in learning bilingual word embeddings",
            "venue": "Proceedings of ACL, pages 247\u2013257.",
            "year": 2016
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Nikola Mrk\u0161i\u0107."
            ],
            "title": "Specialising word vectors for lexical entailment",
            "venue": "Proceedings of NAACL-HLT, pages 1134\u20131145.",
            "year": 2018
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Nikola Mrk\u0161i\u0107",
                "Roi Reichart",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Steve Young",
                "Anna Korhonen."
            ],
            "title": "Morph-fitting: Fine-tuning word vector spaces with simple language-specific rules",
            "venue": "Proceedings of ACL, pages 56\u201368.",
            "year": 2017
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Roy Schwartz",
                "Ari Rappoport",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Automatic selection of context configurations for improved class-specific word representations",
            "venue": "Proceedings of CoNLL, pages 112\u2013122.",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "David Vandyke",
                "Nikola Mrk\u0161i\u0107",
                "Milica Ga\u0161i\u0107",
                "Lina M. Rojas-Barahona",
                "Pei-Hao Su",
                "Stefan Ultes",
                "Steve Young."
            ],
            "title": "A networkbased end-to-end trainable task-oriented dialogue system",
            "venue": "Proceedings of EACL.",
            "year": 2017
        },
        {
            "authors": [
                "Jason Weston",
                "Samy Bengio",
                "Nicolas Usunier."
            ],
            "title": "WSABIE: Scaling up to large vocabulary image annotation",
            "venue": "Proceedings of IJCAI, pages 2764\u20132770.",
            "year": 2011
        },
        {
            "authors": [
                "John Wieting",
                "Mohit Bansal",
                "Kevin Gimpel",
                "Karen Livescu."
            ],
            "title": "From paraphrase database to compositional paraphrase model and back",
            "venue": "Transactions of the ACL, 3:345\u2013358.",
            "year": 2015
        },
        {
            "authors": [
                "Jason D. Williams",
                "Antoine Raux",
                "Matthew Henderson."
            ],
            "title": "The Dialog State Tracking Challenge series: A review",
            "venue": "Dialogue & Discourse, 7(3):4\u201333.",
            "year": 2016
        },
        {
            "authors": [
                "Chang Xu",
                "Yalong Bai",
                "Jiang Bian",
                "Bin Gao",
                "Gang Wang",
                "Xiaoguang Liu",
                "Tie-Yan Liu."
            ],
            "title": "RCNET: A general framework for incorporating knowledge into word representations",
            "venue": "Proceedings of CIKM, pages 1219\u20131228.",
            "year": 2014
        },
        {
            "authors": [
                "Steve Young."
            ],
            "title": "Cognitive User Interfaces",
            "venue": "IEEE Signal Processing Magazine.",
            "year": 2010
        },
        {
            "authors": [
                "Mo Yu",
                "Mark Dredze."
            ],
            "title": "Improving lexical embeddings with semantic knowledge",
            "venue": "Proceedings of ACL, pages 545\u2013550.",
            "year": 2014
        },
        {
            "authors": [
                "Jingwei Zhang",
                "Jeremy Salwen",
                "Michael Glass",
                "Alfio Gliozzo."
            ],
            "title": "Word semantic representations using bayesian probabilistic tensor factorization",
            "venue": "Proceedings of EMNLP, pages 1522\u20131531.",
            "year": 2014
        },
        {
            "authors": [
                "Meng Zhang",
                "Yang Liu",
                "Huanbo Luan",
                "Maosong Sun."
            ],
            "title": "Adversarial training for unsupervised bilingual lexicon induction",
            "venue": "Proceedings of ACL, volume 1, pages 1959\u20131970.",
            "year": 2017
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Philipp Kr\u00e4henb\u00fchl",
                "Eli Shechtman",
                "Alexei A. Efros."
            ],
            "title": "Generative visual manipulation on the natural image manifold",
            "venue": "Proceedings of ECCV, pages 597\u2013613.",
            "year": 2016
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros."
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "Proceedings of CVPR, pages 2223\u20132232.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 282\u2013293 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n282"
        },
        {
            "heading": "1 Introduction",
            "text": "Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models\n\u2217Both authors equally contributed to this work.\nare grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vulic\u0301 et al., 2017) in the induced word vector spaces.1\nA common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre-trained vectors and specialize the distributional spaces for a particular relation, e.g., synonymy (i.e., true similarity) (Faruqui et al., 2015; Mrk\u0161ic\u0301 et al., 2017) or hypernymy (Nickel and Kiela, 2017; Nguyen et al., 2017; Vulic\u0301 and Mrk\u0161ic\u0301, 2018).\nThe best-performing specialization models (cf. Mrk\u0161ic\u0301 et al. 2017) are deployed as post-processors of the vector space: distributional vectors are finetuned to satisfy linguistic constraints extracted from external resources to offer improved support to downstream NLP applications (Faruqui, 2016). Such models are versatile as they can be applied to arbitrary distributional spaces, but they have a major drawback: they locally update only vectors of words present in linguistic constraints (i.e., seen words), whereas vectors of all other (i.e., unseen) words remain intact (see Figure 1).\n1For instance, it is difficult to discern synonyms from antonyms in distributional vector spaces: this has a negative impact on language understanding tasks such as statistical dialog modeling or text simplification (Glava\u0161 and \u0160tajner, 2015; Faruqui et al., 2015; Mrk\u0161ic\u0301 et al., 2016; Kim et al., 2016)"
        },
        {
            "heading": "1. Initial specialization",
            "text": "Vulic\u0301 et al. (2018) have recently proposed a model which, based on the updates of vectors of seen words, learns a global specialization function that can be applied to the large subspace of unseen words. Their global method, termed postspecialization and implemented as a deep feedforward network, effectively specializes all distributional vectors.\nIn this paper, we propose a new approach to post-specialization which addresses the following two research questions: a) Is it possible to use a more sophisticated learning approach to yield more realistic specialized vectors for the full vocabulary? b) Given that specialization methods inherently require a large number of constraints, is it possible to specialize distributional word vectors where such resources are scarce or non-existent? Our novel model learns the global specialization function by casting the feed-forward specialization network as a generator component of an adversarial architecture, see Figure 2. The corresponding discriminator component learns to discern original specialized vectors (produced by any local specialization model) from vectors produced by transforming distributional vectors with the feed-forward post-specialization network (i.e., the generator).\nWe show that the proposed adversarial model yields state-of-the-art performance on standard word similarity benchmarks, outperforming the post-specialization model of Vulic\u0301 et al. (2018). We further demonstrate the effectiveness of the pro-\nposed model in two downstream tasks: lexical text simplification and dialog state tracking. Finally, we demonstrate that, by coupling our adversarial specialization model with any unsupervised model for inducing bilingual vector spaces, such as the algorithm proposed by Conneau et al. (2018), we can successfully perform zero-shot language transfer of the specialization, that is, we can specialize distributional spaces of languages without any linguistic constraints in those languages, and without any bilingual data."
        },
        {
            "heading": "2 Methodology",
            "text": "The post-specialization procedure (Vulic\u0301 et al., 2018) is a two-step process. First, a subspace of vectors for words observed in external resources is fine-tuned using any off-the-shelf specialization model, such as the original retrofitting model (Faruqui et al., 2015), counter-fitting (Mrk\u0161ic\u0301 et al., 2016), dLCE (Nguyen et al., 2016), or state-of-theart ATTRACT-REPEL (AR) specialization (Mrk\u0161ic\u0301 et al., 2017; Vulic\u0301 et al., 2017). We outline the initial specialization algorithms in \u00a72.1. In the second step, the initial specialization is propagated to the entire vocabulary, including words not observed in the resources, relying on an adversarial architecture augmented with a distance loss. This adversarial post-specialization model, compatible with any specialization model, is described in \u00a72.2.\nFinally, in \u00a72.3, we introduce a cross-lingual zero-shot specialization model which transfers the specialization to a target language without any lexical resources. An overview of the proposed methodology from this section is provided in Figure 1."
        },
        {
            "heading": "2.1 Initial Specialization",
            "text": "Linguistic Constraints. Adopting the nomenclature from Mrk\u0161ic\u0301 et al. (2017), post-processing models are generally guided by two broad sets of constraints: 1) ATTRACT constraints specify which words should be close to each other in the finetuned vector space (e.g. synonyms like graceful and amiable); 2) REPEL constraints describe which words should be pulled away from each other (e.g. antonyms like innocent and sinful). Earlier postprocessors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT constraints, and are thus not suited to model both aspects contributing to the specialization process.\nWe first outline the state-of-the-art ATTRACTREPEL specialization model (Mrk\u0161ic\u0301 et al., 2017)\nwhich leverages both sets of constraints. Here, we again stress two important aspects relevant to our post-specialization model: a) all initial specialization models fine-tune only representations for the subspace of words seen in the external constraints, while all other words remain unaffected by specialization; b) post-specialization is not tied to ATTRACT-REPEL in particular; it is applicable on top of any other post-processor.2\nSpecialization of Seen Words. The key idea is to inject the knowledge from linguistic constraints into pre-trained distributional word vectors. Given a set A of ATTRACT word pairs and a set R of REPEL word pairs, each word pair (vl, vr) from the vocabulary Vs of seen words present in these sets can be represented as a vector pair (xl, xr).\nThe optimization is driven by mini-batches of ATTRACT pairs BA (batch size kA), and of REPEL pairs BR (size kR). For both of these, two sets of negative example pairs of equal size are drawn from the 2(kA + kR) vectors occurring in BA and BR. This defines the minibatches TA(BA) = [(t1l , t 1 r) . . . (t kA l , t kA r )] and TR(BR) = [(t 1 l , t 1 r) . . . (t kR l , t kR r )]. Negative examples tl and tr for ATTRACT (or REPEL) pairs are the nearest (or farthest) neighbours by cosine similarity to xl and xr, respectively. They ensure that the paired vectors for words in the constraints are closer to each other (or more distant for antonyms) than to their respective negative examples.\nThe overall objective function consists of three terms. The first term pulls ATTRACT pairs together:\nAtt(BA, TA) = kA\u2211 i=1 [ \u03c4 ( \u03b4A + x i lt i l \u2212 xilxir ) +\n+ \u03c4 ( \u03b4A + x i rt i r \u2212 xilxir )] (1)\n\u03c4(z) = max(0, z) is the standard rectifier (Nair and Hinton, 2010). \u03b4A is the ATTRACT margin: it specifies the tolerance for the difference between the two distances (with the other pair member and with the negative example). The second term,Rep(BR, TR), is similar but now pushes REPEL pairs away from each other, relying on the REPEL margin \u03b4R:\n2We have empirically validated the robustness of the proposed adversarial post-specialization by applying it also on top of other post-processing methods: retrofitting (Faruqui et al., 2015) and counter-fitting (Mrk\u0161ic\u0301 et al., 2016). For brevity, we only report the (best) results with ATTRACT-REPEL, the best-performing initial/local specialization model.\nRep(BR, TR) = kR\u2211 i=1 [\u03c4 ( \u03b4R \u2212 xiltil + xilxir ) +\n+ \u03c4 ( \u03b4R \u2212 xirtir + xilxir ) ] (2)\nThe final term is tasked to preserve the quality of the original vectors through L2-regularization:\nPre(BA,BR) = \u2211\nxi\u2208BA\u222aBR\n\u03bbP ||yi \u2212 xi||2 (3)\nyi is the vector specialized from the original distributional vector xi, and \u03bbP is a regularization hyper-parameter. The optimizer finally minimizes the following objective: LAR = Att(BA, TA) + Rep(BR, TR) + Pre(BA,BR)."
        },
        {
            "heading": "2.2 Adversarial Post-Specialization",
            "text": "Motivation. The AR method affects only a subset of the full vocabulary V , and consequently only a (small) subspace of the original space X (see Figure 1). In particular, it specializes the embeddings Xs corresponding to Vs, the vocabulary of words observed in the constraints. It leaves the embeddings Xu corresponding to all other (unseen) words Vu identical.\nNevertheless, the perturbation underwent by the original observed embeddings can provide evidence about the general effects of specialization. In particular, it allows to learn a global mapping function f : X \u2208 Rd \u2192 Y \u2208 Rd for d-dimensional vectors. The parameters for this function can be trained in a supervised fashion from pairs of original and initially specialized word embeddings (x(s)i , y (s) i ) from Vs, as illustrated by Figure 2. Subsequently, the mapping can be applied to distributional word vectors xu from the vocabulary of unseen words Vu to predict y\u0302u, their specialized counterpart. This procedure, called post-specialization, effectively propagates the information stored in the external constraints to the entire word vector space.\nHowever, this mapping should not just model the inherent transformation, but also ensure that the resulting vector is \u2018natural\u2019. In particular, assuming that word representations lie on a manifold, the mapping should return one of its values. The intuition behind our formulation of the training objective is that: a) an L2-distance loss can retrieve a faithful mapping whereas b) an adversarial loss can prevent unrealistic outputs, as already proven in the the visual domain (Pathak et al., 2016; Ledig et al., 2017; Odena et al., 2017).\nGenerator Discriminator\ny : G (x)\nObjective Function. The pairs of original and specialized embeddings for seen words allow to train the global mapping function. In principle, this can be any differentiable parametrized function G(x; \u03b8G). Vulic\u0301 et al. (2018) showed that nonlinear functions ensure a better mapping than linear transformations which seem inadequate to mimic the complex perturbations of the specialization process, guided by possibly millions of pairwise constraints. Our preliminary experiments corroborate this intuition. Thus, in this work we also opt for implementing G(x; \u03b8G) as a deep neural network. Each of the l hidden layers of size h non-linearly transforms its input. The output layer is a linear transformation into the prediction y\u0302 \u2208 Rd.\nThe parameters \u03b8G are learned by minimizing theL2 distance between the training pairs. In particular, the loss is a contrastive margin-based ranking loss with negative sampling (MM) as proposed by Weston et al. (2011, inter alia). The gist of this loss is that the first component increases the cosine similarity cos of predicted and initially specialized vectors of the same word up to a margin \u03b4MM . On the other hand, the second component encourages the predicted vectors to distance themselves from k random confounders. These are negative examples sampled uniformly from the batch B excluding the current vector:\nLMM = ||Vs||\u2211 i=1 k\u2211 j=1|j 6=i \u03c4 [\u03b4MM\u2212cos(G(x(s)i ; \u03b8G),y (s) i )+\n+ cos(G(x (s) i ; \u03b8G),y (s) j )] (4)\nOne of the original contributions of this work is combining the L2 distance with an adversarial loss, resulting in an auxiliary-loss Generative Adversarial Network (AuxGAN) as shown in Figure 2. The role of the adversarial component, as mentioned above, is to \u2018soften\u2019 the mapping and guarantee realistic outputs from the target distribution.\nThe mapping can be considered a generator G(x|\u03b8G). On top of this, a discriminator D(x|\u03b8D), implemented also as a multi-layer neural net, tries to distinguish whether a vector is sampled from the predicted vectors or the AR-specialized vectors. Its output layer performs binary classification through softmax. The objective minimizes the loss LD:\nLD = \u2212 n\u2211\ni=1\nlogP (specialized = 0|G(xi; \u03b8G); \u03b8D)\u2212\n\u2212 m\u2211 i=1 logP (specialized = 1|yi; \u03b8D) (5)\nIn a two-player game (Goodfellow et al., 2014), the generator is trained to fool the discriminator by maximizing log(1 \u2212 P (0|G(xi; \u03b8G); \u03b8D)). However, to avoid vanishing gradients of G early on, the loss LG is reformulated by swapping the labels of Eq. (5) as follows:\nLG = \u2212 n\u2211\ni=1\nlogP (specialized = 1|G(xi; \u03b8G); \u03b8D)\u2212\n\u2212 m\u2211 i=1 logP (specialized = 0|yi; \u03b8D) (6)\nDuring the optimization procedure through stochastic gradient descent, we alternate among s steps for\nLD, one step for LG, and one step for LMM to avoid the overfitting of D. The reason why s \u2265 1 is that D can be kept close to a minimum of its loss function by updating G less frequently."
        },
        {
            "heading": "2.3 Zero-shot Transfer to Other Languages",
            "text": "Once the AuxGAN has learned a global mapping function G(x; \u03b8G) in a resource-rich language, it can be directly applied to unseen words. In this work, we propose a method to additionally postspecialize the whole vocabulary Vt of a resourcepoor target language. We assume a real-world scenario where no target language constraints are available to specialize it directly.\nWhat is more, we assume that no bilingual data or dictionaries are available either. Hence, we rely on unsupervised cross-lingual word embedding induction, and in particular on Conneau et al. (2018)\u2019s method. By virtue of these assumptions, there is no limitation to the range of potential target languages that can be specialized. Incidentally, please note that the proposed transfer method is equally applicable on top of other cross-lingual word embedding induction methods. These may require more bilingual supervision to learn the crosslingual vector space.3\nAfter learning the shared cross-lingual word embedding space in an unsupervised fashion (Conneau et al., 2018), the global post-specialization function learnt on the seen source language vectors is applied to the target language vectors, since they lie in the same shared space (see Figure 1 again). By virtue of the transfer, linguistic constraints in the source language can enhance the distributional vectors of target language vocabularies.\nConneau et al. (2018) learn a shared crosslingual vector space as follows. They first learn a coarse initial mapping between two monolingual embedding spaces in two different languages through a GAN where the generator is a linear transformation with an orthogonal matrix W\u0302. Its loss is identical to Eq. (5) and Eq. (6), but unlike our AuxGAN model it discriminates between embeddings drawn from the source language and the target language distributions. Using the shared space, they extract for each source vector the closest target vector according to a distance metric designed to mitigate the hubness problem (Radovanovic\u0301 et al.,\n3See the recent survey papers on cross-lingual word embeddings and their typology (Upadhyay et al., 2016; Vulic\u0301 and Korhonen, 2016; Ruder et al., 2017)\n2010), the Cross-Domain Similarity Local Scaling (CSLS).\nThis creates a bilingual synthetic dictionary that allows to further refine the coarse initial mapping. In particular, the optimal parameters for the linear mapping minimizing the L2-distance between source-target pairs are provided by the closed-form Procrustes solution (Sch\u00f6nemann, 1966) based on singular value decomposition (SVD):\nW\u0302 = arg minW ||W Xt \u2212Xs||F = UV >\nU\u03a3V> = SVD(XtX>s ) (7)\nwhere || \u00b7 ||F is the Frobenius norm. After mapping the original target embeddings into the shared space with this method, we post-specialize them with the function outlined in \u00a72.2, learnt on the source language. This yields the specialized target vectors Y\u0302t = G(W\u0302Xt; \u03b8G)."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "Distributional Vectors. We estimate the robustness of adversarial post-specialization by experimenting with three widely used collections of distributional English vectors. 1) SGNS-W2 vectors are trained on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE-CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4\nConstraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vulic\u0301 et al., 2018). These constraints, extracted from WordNet (Fellbaum, 1998) and Roget\u2019s Thesaurus (Kipfer, 2009), comprise a total of 1,023,082 synonymy/ATTRACT word pairs and 380,873 antonymy/REPEL pairs.\nNote that the sets of constraints cover only a fraction of the full distributional vocabulary, providing direct motivation for post-specialization methods\n4Experiments with other standard word vectors, such as CONTEXT2VEC (Melamud et al., 2016a) and dependencybased embeddings (Bansal et al., 2014) show similar trends and lead to same conclusions.\nwhich are able to specialize the full vocabulary. For instance, only 15.3% of the SGNS-W2 vocabulary words are seen words present in the constraints.5\nThe constraints are initially injected into the distributional vector space (see Figure 1 again) using ATTRACT-REPEL, a state-of-the-art specialization model, for which we adopt the original suggested model setup (Mrk\u0161ic\u0301 et al., 2017).6 Hyperparameter values are set to: \u03b4A = 0.6, \u03b4R = 0.0, \u03bbP = 10\n\u22129. The models are trained for 5 epochs with Adagrad (Duchi et al., 2011), with batch sizes set to kA = kR = 50, again as in the original work.\nAuxGAN Setup and Hyper-Parameters. Both the generator and the discriminator are feedforward nets with l = 2 hidden layers, each of size h = 2048, and LeakyReLU as non-linear activation (Maas et al., 2013). The dropout for the input and hidden layers of the generator is 0.2 and for the input layer of the discriminator 0.1. In evaluation, the noise is blanketed out in order to ensure a deterministic mapping (Isola et al., 2017). Moreover, we smooth the golden labels for prediction by a factor of 0.1 to make the model less vulnerable to adversarial examples (Szegedy et al., 2016).\nWe train our model with SGD for 10 epochs of 1 million iterations each, feeding mini-batches of size 32. For each pair in a batch we generate 25 negative examples; s = 5 (see \u00a72.2). As a way to normalize the mini-batches (Salimans et al., 2016), these are constructed to contain exclusively either original or specialized vectors. At each epoch, the initial learning rate of 0.1 is decayed by a factor of 0.98, or 0.5 if the score on the validation set (computed as the average cosine similarity between the predicted and AR-specialized embeddings)7 has not increased. The hyper-parameters k and \u03b4MM are tuned via grid search on the validation set.\nZero-Shot Specialization Setup. The GAN discriminator for learning a shared cross-lingual vector space (see \u00a72.3) has hyper-parameters identical to the AuxGAN. The generator instead is a linear layer initialized as an identity matrix and enforced to lie on the manifold of orthogonal matrices during training (Cisse et al., 2017). No dropout is used. The unsupervised validation metric for early stop-\n5The respective coverage for the 200K most frequent GLOVE-CC and FASTTEXT words is only 13.3% and 14.6%.\n6https://github.com/nmrksic/ attract-repel\n7The score is computed as the average cosine similarity between the original and specialized embeddings.\nping is the cosine distance between dictionary pairs extracted with the CSLS similarity metric."
        },
        {
            "heading": "4 Results and Discussion",
            "text": ""
        },
        {
            "heading": "4.1 Word Similarity",
            "text": "Evaluation Setup. We first evaluate adversarial post-specialization intrinsically, using two standard word similarity benchmarks for English: SimLex999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016), a dataset containing human similarity ratings for 3,500 verb pairs.8 The evaluation measure is Spearman\u2019s \u03c1 rank correlation between gold and predicted word pair similarity scores.\nWe evaluate word vectors in two settings, similar to Vulic\u0301 et al. (2018). a) In the synthetic DISJOINT setting, we discard all linguistic constraints that contain any of the words found in SimLex or SimVerb. This means that all test words from SimLex and SimVerb are effectively unseen words, and through this setting we are able to in vitro evaluate the model\u2019s ability to generalize the specialization function to unseen words. b) In the FULL setting we leverage all constraints. This is a standard \u201creallife\u201d scenario where some test words do occur in the constraints, while the mapping is learned for the remaining words. We use the FULL setting in all subsequent downstream applications (\u00a74.2).\nWe compare our model to ATTRACT-REPEL (AR), which specializes only the vectors of words occurring in the constraints. We also provide comparisons to a post-specialization model of Vulic\u0301 et al. (2018) which specializes the full vocabulary, but substitutes the AuxGAN architecture from \u00a72.2 with a deep 5-layer feed-forward neural net also based on the max-margin loss (see Eq. (4)) to learn the mapping function (POST-DFFN).\nResults and Analysis. The results are summarized in Table 1. The scores suggest that the proposed adversarial post-specialization model is universally useful and robust: we observe gains over input distributional word vectors for all three vector collections. The results in the DISJOINT setting illustrate the core limitation of the initial specialization/post-processing models and indicate the extent of improvement achieved when generalizing the specialization function to unseen words\n8Unlike WordSim-353 (Finkelstein et al., 2002) or MEN (Bruni et al., 2014), SimLex and SimVerb provide explicit guidelines to discern between true semantic similarity and (more broad) conceptual relatedness, so that related but nonsimilar words (e.g. tiger and jungle) have a low rating.\nthrough adversarial post-specialization. Moreover, the scores suggest that the more sophisticated adversarial post-specialization method (AUXGAN) outperforms POST-DFFN across a large number of experimental runs, verifying its effectiveness.\nWe observe only modest and inconsistent gains over ATTRACT-REPEL and POST-DFFN in the FULL setting. However, the explanation of this finding is straightforward: 99.2% of SimLex words and 99.9% of SimVerb words are present in the external constraints, making this an unrealistic evaluation scenario. The usefulness of the initial ATTRACTREPEL specialization is less pronounced in reallife downstream applications in which such high coverage cannot be guaranteed, as shown in \u00a74.2."
        },
        {
            "heading": "4.2 Downstream Tasks",
            "text": "We next evaluate the embedding spaces specialized with the AuxGAN method in two tasks in which discerning semantic similarity from semantic relatedness is crucial: lexical text simplification (LS) and dialog state tracking (DST)."
        },
        {
            "heading": "4.2.1 Lexical Text Simplification",
            "text": "The goal of lexical simplification is to replace complex words (typically words that are used less often in language and are therefore less familiar to readers) with their simpler synonyms, without infringing the grammaticality and changing the meaning of the text. Replacing complex words with related words instead of true synonyms affects the original meaning (e.g., Ferrari pilot Vettel vs Ferrari airplane Vettel) and often yields ungrammatical text (e.g., they drink all pizzas).\nLS Using Word Vectors. We use Light-LS, a publicly available LS tool based on word embeddings (Glava\u0161 and \u0160tajner, 2015). Light-LS generates and then ranks substitution candidates based on similarity in the input word vector space. The\nquality of the space thus directly affects LS performance: by plugging any word vector space into Light-LS, we extrinsically evaluate that embedding space for LS. Furthermore, the better the embedding space captures true semantic similarity, the better the substitutions made by Light-LS.\nEvaluation Setup. We use the standard LS dataset of Horn et al. (2014). It contains 500 sentences with indicated complex words (one word per sentence) that have to be substituted with simpler synonyms. For each word, simplifications were crowdsourced from 50 human annotators. Following prior work (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015), we evaluate the performance of LightLS using the metric that quantifies both the quality and the frequency of word replacements: Accurracy (Acc) metric is the number of correct simplifications made divided by the total number of complex words.\nResults and Analysis. Scores for all three pretrained vector spaces are shown in Table 2. Similar to the word similarity task, embedding spaces produced with post-specialization models outperform the vectors produced with AR and original distributional vectors. The gains are now more pronounced in the real-life FULL setup, as only 59.6 % of all indicated complex words and substitution candidates from the LS dataset are covered in the external con-\nstraints. Adversarial post-specialization (AUXGAN) has a slight edge over the post-specialization with a simple feed-forward network (POST-DFFN) for FASTTEXT and SGNS-W2 embeddings, but not for GLOVE-CC vectors. In general, the fact that both post-specialization methods outperform ATTRACTREPEL by a wide margin shows the importance of specializing the full word vector space for downstream NLP applications."
        },
        {
            "heading": "4.2.2 Dialog State Tracking",
            "text": "Finally, we evaluate the importance of fullvocabulary (adversarial) post-specialization in another language understanding task: dialog state tracking (DST) (Henderson et al., 2014; Williams et al., 2016), which is a standard task to measure the impact of specialization in prior work (Mrk\u0161ic\u0301 et al., 2017). A DST model is typically the first component of a dialog system pipeline (Young, 2010), tasked with capturing user\u2019s goals and updating the dialog belief state at each dialog turn. Distinguishing similarity from relatedness is crucial for DST (e.g., a dialog system should not recommend an \u201cexpensive restaurant in the west\u201d when asked for an \u201caffordable pub in the north\u201d).\nEvaluation Setup. To evaluate the effects of specialized word vectors on DST, following prior work we utilize the Neural Belief Tracker (NBT), a statistical DST model that makes inferences purely based on pre-trained word vectors (Mrk\u0161ic\u0301 et al., 2017).9 Again, as in prior work the DST evaluation is based on the Wizard-of-Oz (WOZ) v2.0 dataset (Wen et al., 2017; Mrk\u0161ic\u0301 et al., 2017), comprising 1,200 dialogues split into training (600 dialogues), development (200), and test data (400). We report the standard DST metric: joint goal accuracy (JGA), the proportion of dialog turns where all the user\u2019s search goal constraints were correctly identified, computed as average over 5 NBT runs.\n9https://github.com/nmrksic/ neural-belief-tracker; For full model details, we refer the reader to the original paper.\nResults and Analysis. We show English DST performance in the FULL setting in Table 3. Only NBT performance with GLOVE-CC vectors is reported for brevity, as similar performance gains are observed with the other two pre-trained vector collections. The results confirm our findings established in the other two tasks: a) initial AR specialization of distributional vectors is useful, but b) it is crucial to specialize the full vocabulary for improved performance (e.g., 57% of all WOZ words are present in the constraints), and c) the more sophisticated AUXGAN model yields additional gains."
        },
        {
            "heading": "4.3 Cross-Lingual Zero-Shot Specialization",
            "text": "Evaluation Setup. Large collections of linguistic constraints do not exist for many languages. Therefore, we test if the specialization knowledge from a resoure-rich language (i.e., English) can be transferred to resource-lean target languages (see \u00a72.3). We simulate resource-lean scenarios using two target languages: Italian (IT) and German (DE).10 We evaluate zero-specialized IT and DE FASTTEXT vectors, using English FASTTEXT vectors as the source, on the same three tasks as before. We report the same evaluation measures, using the following evaluation data: 1) IT and DE SimLex999 datasets (Leviant and Reichart, 2015) for word similarity; 2) IT lexical simplification data (SIMPITIKI) (Tonelli et al., 2016); 3) IT and DE WOZ data (Mrk\u0161ic\u0301 et al., 2017) for DST.\nResults and Analysis. The results are summarized in Table 4. The gains over the original distributional vectors are substantial across all three tasks and for both languages. This finding indicates that the semantic content of distributional vectors can be enriched even for languages without any readily available lexical resources.\nThe gap between performances of language transfer and the monolingual setting is explained\n10Note that the two languages are not resource-poor, but we treat them as such in our experiments. This choice of languages was determined by the availability of high-quality evaluation data to measure the effects of zero-shot specialization.\nby the noise introduced by the bilingual vector alignment and the different ways concepts are lexicalized across languages, as studied by semantic typology (Ponti et al., 2018). Nonetheless, in the long run, these transfer results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications."
        },
        {
            "heading": "5 Related Work",
            "text": "Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016).\nOther models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Sch\u00fctze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrk\u0161ic\u0301 et al., 2016; Cotterell et al., 2016; Mrk\u0161ic\u0301 et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrk\u0161ic\u0301 et al., 2016; Kim et al., 2016; Vulic\u0301 et al., 2017). By design, these methods fine-tune only vectors of words seen in external resources.\nVulic\u0301 et al. (2018) suggest that specializing the full vocabulary is beneficial for downstream applications. Comparing to their work, we show that a more sophisticated adversarial post-specialization can yield further gains across different tasks and boost full-vocabulary specialization in resourcelean settings through cross-lingual transfer.\nGenerative Adversarial Networks. GANs were originally devised to generate images from input noise variables (Goodfellow et al., 2014). The generation process is typically conditioned on discrete labels or data from other modalities, such as text (Mirza and Osindero, 2014). Otherwise, the condition can take the form of real data in input rather than (or in addition to) noise: in this case, the generator parameters are better conceived as a mapping function. For instance, it can bridge between pixelto-pixel (Isola et al., 2017) or character-to-pixel\n(Reed et al., 2016) transformations. The GAN objective can be mixed with more traditional loss functions: in these cases, apart from trying to fool the discriminator, the generator also minimizes the distance between input and target data (Pathak et al., 2016; Li and Wand, 2016; Ledig et al., 2017). The distance can be formulated as the mean squared error between the input and the target (Pathak et al., 2016), their feature maps (Li and Wand, 2016), both (Zhu et al., 2016), or a loss calculated on feature maps of a deep convolutional network (Ledig et al., 2017).\nIn the textual domain, adversarial models have been proven to support domain adaptation (Ganin et al., 2016) and language transfer (Chen et al., 2016) by learning domain/language-invariant latent features. Adversarial training also powers unsupervised mapping between monolingual vector spaces to learn cross-lingual word embeddings (Zhang et al., 2017; Conneau et al., 2018). In this work, we show how to apply adversarial techniques to the problem of vector specialization, which has a substantial impact on language understanding tasks."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We have presented adversarial post-specialization, a novel model supported by adversarial training which specializes word vectors for the full vocabulary of the input distributional vector space, including words unseen in external lexical resources. We have also introduced a method for zero-shot specialization of word vectors in languages without any external resources. The benefits of adversarial post-specialization and its zero-shot transfer have been demonstrated across three tasks (word similarity, lexical text simplification, and dialog state tracking) and for three languages.\nIn future work, we will explore more sophisticated adversarial models such as Cycle-GAN (Zhu et al., 2017). Moreover, we will experiment with bootstrapping approaches to extract new lexical constraints from post-specialized embeddings. We also plan to extend the method to asymmetric relations (e.g., hypernymy) and to more target (resource-lean) languages. The code is available at https://github.com/cambridgeltl/ adversarial-postspec."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the ERC Consolidator Grant LEXICAL (no 648909)."
        }
    ],
    "title": "Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization",
    "year": 2018
}