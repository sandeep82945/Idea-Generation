{
    "abstractText": "Background and Objective: Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. Methods: We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pretherapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement module to ensure the effectiveness of disease evolution learning and obtain realistic SD-OCT images by adversarial training. Results: SHENet is validated on 383 SD-OCT cubes of 22 nAMD patients based on three well-designed schemes (P-0, P-1 and P-M) based on the \u2217Qiang Chen is the corresponding author of this work. Email addresses: zhangyuhan@njust.edu.cn (Yuhan Zhang), huangkun@njust.edu.cn (Kun Huang), chaosli@njust.edu.cn (Mingchao Li), yuansongtao@vip.sina.com (Songtao Yuan), chen2qiang@njust.edu.cn (Qiang Chen) Preprint submitted to CMPB August 15, 2023 ar X iv :2 30 8. 06 43 2v 1 [ ee ss .I V ] 1 2 A ug 2 02 3 quantitative and qualitative evaluations. Three metrics (PSNR, SSIM, 1LPIPS) are used here for quantitative evaluations. Compared with other generative methods, the generative SD-OCT images of SHENet have the highest image quality (P-0: 23.659, P-1: 23.875, P-M: 24.198) by PSNR. Besides, SHENet achieves the best structure protection (P-0: 0.326, P-1: 0.337, P-M: 0.349) by SSIM and content prediction (P-0: 0.609, P-1: 0.626, PM: 0.642) by 1-LPIPS. Qualitative evaluations also demonstrate that SHENet has a better visual effect than other methods. Conclusions: SHENet can generate post-therapeutic SD-OCT images with both high prediction performance and good image quality, which has great potential to help ophthalmologists forecast the therapeutic effect of nAMD.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuhan Zhang"
        },
        {
            "affiliations": [],
            "name": "Kun Huang"
        },
        {
            "affiliations": [],
            "name": "Mingchao Li"
        },
        {
            "affiliations": [],
            "name": "Songtao Yuan"
        },
        {
            "affiliations": [],
            "name": "Qiang Chena"
        }
    ],
    "id": "SP:e4ed8f75882d7347ffa1dcd9fd257e2d6912c9d3",
    "references": [
        {
            "authors": [
                "A. Foss",
                "T. Rotsos",
                "T. Empeslidis",
                "V. Chong"
            ],
            "title": "Development of macular atrophy in patients with wet age-related macular degeneration receiving anti-vegf treatment",
            "venue": "Ophthalmologica 245 (3) ",
            "year": 2022
        },
        {
            "authors": [
                "R. Tadayoni",
                "L. Sararols",
                "G. Weissgerber",
                "R. Verma",
                "A. Clemens",
                "F.G. Holz"
            ],
            "title": "Brolucizumab: a newly developed anti-vegf molecule for the treatment of neovascular age-related macular degeneration",
            "venue": "Ophthalmologica 244 (2) ",
            "year": 2021
        },
        {
            "authors": [
                "P.S. Mettu",
                "M.J. Allingham",
                "S.W. Cousins"
            ],
            "title": "Incomplete response to anti-vegf therapy in neovascular amd: Exploring disease mechanisms and therapeutic opportunities",
            "venue": "Progress in Retinal and Eye Research 82 ",
            "year": 2021
        },
        {
            "authors": [
                "M.G. Maguire"
            ],
            "title": "D",
            "venue": "F. Martin, G.-s. Ying, G. J. Jaffe, E. Daniel, J. E. Grunwald, C. A. Toth, F. L. Ferris III, S. L. Fine, C. of Age-related Macular Degeneration Treatments Trials (CATT) Research Group, et al., Five-year outcomes with anti\u2013vascular endothelial growth factor treatment of neovascular age-related macular degeneration: the comparison of age-related macular degeneration treatments trials, Ophthalmology 123 (8) ",
            "year": 2016
        },
        {
            "authors": [
                "G. Lan",
                "J. Xu",
                "Z. Hu",
                "Y. Huang",
                "Y. Wei",
                "X. Yuan",
                "H. Liu",
                "J. Qin",
                "Y. Wang"
            ],
            "title": "Q",
            "venue": "Shi, et al., Design of 1300 nm spectral domain optical coherence tomography angiography system for iris microvascular imaging, Journal of Physics D: Applied Physics 54 (26) ",
            "year": 2021
        },
        {
            "authors": [
                "M. Gharbiya",
                "R. Giustolisi",
                "J. Marchiori",
                "A. Bruscolini",
                "F. Mallone",
                "V. Fameli",
                "M. Nebbioso",
                "S. Abdolrahimzadeh"
            ],
            "title": "Comparison of short-term choroidal thickness and retinal morphological changes after intravitreal anti-vegf therapy with ranibizumab or aflibercept in treatment-naive eyes",
            "venue": "Current eye research 43 (3) ",
            "year": 2018
        },
        {
            "authors": [
                "M. Saito",
                "M. Kano",
                "K. Itagaki",
                "T. Sekiryu"
            ],
            "title": "Efficacy of intravitreal aflibercept in japanese patients with exudative age-related macular degeneration",
            "venue": "Japanese journal of ophthalmology 61 (1) ",
            "year": 2017
        },
        {
            "authors": [
                "J. Yim",
                "R. Chopra",
                "T. Spitz",
                "J. Winkens",
                "A. Obika",
                "C. Kelly",
                "H. Askham",
                "M. Lukic",
                "J. Huemer"
            ],
            "title": "K",
            "venue": "Fasler, et al., Predicting conversion to wet age-related macular degeneration using deep learning, Nature Medicine 26 (6) ",
            "year": 2020
        },
        {
            "authors": [
                "S. Ajana",
                "A. Cougnard-Gr\u00e9goire",
                "J.M. Colijn",
                "B.M. Merle",
                "T. Verzijden"
            ],
            "title": "P",
            "venue": "T. de Jong, A. Hofman, J. R. Vingerling, B. P. Hejblum, J.-F. Korobelnik, et al., Predicting progression to advanced age-related macular degeneration from clinical, genetic, and lifestyle factors using machine learning, Ophthalmology 128 (4) ",
            "year": 2021
        },
        {
            "authors": [
                "I. Banerjee"
            ],
            "title": "L",
            "venue": "de Sisternes, J. A. Hallak, T. Leng, A. Osborne, P. J. Rosenfeld, G. Gregori, M. Durbin, D. Rubin, Prediction of age-related macular degeneration disease using a sequential deep learning approach on longitudinal sd-oct imaging biomarkers, Scientific reports 10 (1) ",
            "year": 2020
        },
        {
            "authors": [
                "Q. Yan",
                "Y. Jiang",
                "H. Huang",
                "A. Swaroop",
                "E.Y. Chew",
                "D.E. Weeks",
                "W. Chen",
                "Y. Ding"
            ],
            "title": "Genome-wide association studies-based machine learning for prediction of age-related macular degeneration risk",
            "venue": "Translational vision science & technology 10 (2) ",
            "year": 2021
        },
        {
            "authors": [
                "A. Bhuiyan",
                "T.Y. Wong",
                "D.S.W. Ting",
                "A. Govindaiah",
                "E.H. Souied",
                "R.T. Smith"
            ],
            "title": "Artificial intelligence to stratify severity of age-related 26 macular degeneration (amd) and predict risk of progression to late amd",
            "venue": "Translational vision science & technology 9 (2) ",
            "year": 2020
        },
        {
            "authors": [
                "U. Schmidt-Erfurth",
                "H. Bogunovic",
                "A. Sadeghipour",
                "T. Schlegl",
                "G. Langs",
                "B.S. Gerendas",
                "A. Osborne",
                "S.M. Waldstein"
            ],
            "title": "Machine learning to analyze the prognostic value of current imaging biomarkers in neovascular agerelated macular degeneration",
            "venue": "Ophthalmology Retina 2 (1) ",
            "year": 2018
        },
        {
            "authors": [
                "M. Rohm",
                "V. Tresp",
                "M. M\u00fcller",
                "C. Kern",
                "I. Manakov",
                "M. Weiss",
                "D.A. Sim",
                "S. Priglinger",
                "P.A. Keane",
                "K. Kortuem"
            ],
            "title": "Predicting visual acuity by using machine learning in patients treated for neovascular age-related macular degeneration",
            "venue": "Ophthalmology 125 (7) ",
            "year": 2018
        },
        {
            "authors": [
                "C. Diack",
                "D. Schwab",
                "V. Cosson",
                "V. Buchheit",
                "N. Mazer",
                "N. Frey"
            ],
            "title": "A baseline score to predict response to ranibizumab treatment in neovascular age-related macular degeneration",
            "venue": "Translational Vision Science & Technology 10 (6) ",
            "year": 2021
        },
        {
            "authors": [
                "F. Rossant",
                "M. Paques"
            ],
            "title": "Normalization of series of fundus images to monitor the geographic atrophy growth in dry age-related macular degeneration",
            "venue": "Computer Methods and Programs in Biomedicine 208 ",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "X. Zhang",
                "Z. Ji",
                "S. Niu",
                "T. Leng",
                "D.L. Rubin",
                "S. Yuan",
                "Q. Chen"
            ],
            "title": "An integrated time adaptive geographic atrophy prediction model for sd-oct images",
            "venue": "Medical Image Analysis 68 ",
            "year": 2021
        },
        {
            "authors": [
                "G.S. Reiter",
                "R. Told",
                "L. Baumann",
                "S. Sacu",
                "U. Schmidt-Erfurth",
                "A. Pollreisz"
            ],
            "title": "Investigating a growth prediction model in advanced age-related macular degeneration with solitary geographic atrophy using quantitative autofluorescence",
            "venue": "Retina 40 (9) ",
            "year": 2020
        },
        {
            "authors": [
                "K. Nattagh",
                "H. Zhou",
                "N. Rinella",
                "Q. Zhang",
                "Y. Dai",
                "K.G. Foote",
                "C. Keiner",
                "M. Deiner",
                "J.L. Duncan"
            ],
            "title": "T",
            "venue": "C. Porco, et al., Oct angiography to predict geographic atrophy progression using choriocapillaris flow void as a biomarker, Translational vision science & technology 9 (7) ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "Z. Ji",
                "S. Niu",
                "T. Leng",
                "D.L. Rubin",
                "Q. Chen"
            ],
            "title": "A multi-scale deep convolutional neural network for joint segmentation and prediction 27 of geographic atrophy in sd-oct images",
            "venue": "in: 2019 IEEE 16th International Symposium on Biomedical Imaging ",
            "year": 2019
        },
        {
            "authors": [
                "Q. Yang",
                "N. Anegondi",
                "V. Steffen",
                "C. Rabe",
                "D. Ferrara",
                "S.S. Gao"
            ],
            "title": "Multi-modal geographic atrophy lesion growth rate prediction using deep learning",
            "venue": "Investigative Ophthalmology & Visual Science 62 (8) ",
            "year": 2021
        },
        {
            "authors": [
                "H. Bogunovi\u0107",
                "S.M. Waldstein",
                "T. Schlegl",
                "G. Langs",
                "A. Sadeghipour",
                "X. Liu",
                "B.S. Gerendas",
                "A. Osborne",
                "U. Schmidt-Erfurth"
            ],
            "title": "Prediction of anti-vegf treatment requirements in neovascular amd using a machine learning approach",
            "venue": "Investigative ophthalmology & visual science 58 (7) ",
            "year": 2017
        },
        {
            "authors": [
                "Y. Liu",
                "J. Yang",
                "Y. Zhou",
                "W. Wang",
                "J. Zhao",
                "W. Yu",
                "D. Zhang",
                "D. Ding",
                "X. Li",
                "Y. Chen"
            ],
            "title": "Prediction of oct images of short-term response to anti-vegf treatment for neovascular age-related macular degeneration using generative adversarial network",
            "venue": "British Journal of Ophthalmology 104 (12) ",
            "year": 2020
        },
        {
            "authors": [
                "H. Lee",
                "S. Kim",
                "M.A. Kim",
                "H. Chung",
                "H.C. Kim"
            ],
            "title": "Post-treatment prediction of optical coherence tomography using a conditional generative adversarial network in age-related macular degeneration",
            "venue": "Retina 41 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "T.R.J. Forshaw",
                "H.J. Ahmed",
                "T.W. Kj\u00e6r",
                "S. Andr\u00e9asson",
                "T.L. S\u00f8rensen"
            ],
            "title": "Full-field electroretinography in age-related macular degeneration: can retinal electrophysiology predict the subjective visual outcome of cataract surgery",
            "venue": "Acta ophthalmologica 98 (7) ",
            "year": 2020
        },
        {
            "authors": [
                "Q.T. Pham",
                "S. Ahn",
                "J. Shin",
                "S.J. Song"
            ],
            "title": "Generating future fundus images for early age-related macular degeneration based on generative adversarial networks",
            "venue": "Computer Methods and Programs in Biomedicine 216 ",
            "year": 2022
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems 27 ",
            "year": 2014
        },
        {
            "authors": [
                "J. Johnson",
                "A. Alahi",
                "L. Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "in: European conference on computer vision, Springer",
            "year": 2016
        },
        {
            "authors": [
                "M. Arjovsky",
                "S. Chintala",
                "L. Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "in: International conference on machine learning, PMLR",
            "year": 2017
        },
        {
            "authors": [
                "S. Nowozin",
                "B. Cseke",
                "R. Tomioka"
            ],
            "title": "f-gan: Training generative neural samplers using variational divergence minimization",
            "venue": "in: Proceedings of the 30th International Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "M. Mirza",
                "S. Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "arXiv preprint arXiv:1411.1784 ",
            "year": 2014
        },
        {
            "authors": [
                "T.K. Yoo",
                "J.Y. Choi",
                "H.K. Kim"
            ],
            "title": "A generative adversarial network approach to predicting postoperative appearance after orbital decompression surgery for thyroid eye disease",
            "venue": "Computers in biology and medicine 118 ",
            "year": 2020
        },
        {
            "authors": [
                "D. Qiu",
                "Y. Cheng",
                "X. Wang"
            ],
            "title": "Improved generative adversarial network for retinal image super-resolution",
            "venue": "Computer Methods and Programs in Biomedicine 225 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "X. He",
                "L. Qing",
                "F. Gao",
                "B. Wang"
            ],
            "title": "Bpgan: Brain pet synthesis from mri using generative adversarial network for multi-modal alzheimer\u2019s disease diagnosis",
            "venue": "Computer Methods and Programs in Biomedicine 217 ",
            "year": 2022
        },
        {
            "authors": [
                "E. Schonfeld",
                "B. Schiele",
                "A. Khoreva"
            ],
            "title": "A u-net based discriminator for generative adversarial networks",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "T. Park",
                "M.-Y. Liu",
                "T.-C. Wang",
                "J.-Y. Zhu"
            ],
            "title": "Semantic image synthesis with spatially-adaptive normalization",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhang",
                "M. Li",
                "S. Yuan",
                "Q. Liu",
                "Q. Chen"
            ],
            "title": "Robust region encoding and layer attribute protection for the segmentation of retina with multifarious abnormalities",
            "venue": "Medical Physics 48 (12) ",
            "year": 2021
        },
        {
            "authors": [
                "P. Isola",
                "J.-Y. Zhu",
                "T. Zhou",
                "A.A. Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2017
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "D. Misra"
            ],
            "title": "Mish: A self regularized non-monotonic neural activation function",
            "venue": "arXiv preprint arXiv:1908.08681 4 ",
            "year": 2019
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Lio",
                "Y. Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "arXiv preprint arXiv:1710.10903 ",
            "year": 2017
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "in: International conference on machine learning, PMLR",
            "year": 2020
        },
        {
            "authors": [
                "N. Wang",
                "Y. Zhang",
                "L. Zhang"
            ],
            "title": "Dynamic selection network for image inpainting",
            "venue": "IEEE Transactions on Image Processing 30 ",
            "year": 2021
        },
        {
            "authors": [
                "T.-C. Wang",
                "M.-Y. Liu",
                "J.-Y. Zhu",
                "A. Tao",
                "J. Kautz",
                "B. Catanzaro"
            ],
            "title": "Highresolution image synthesis and semantic manipulation with conditional gans",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "S.A. Kamran",
                "K.F. Hossain",
                "A. Tavakkoli",
                "S. Zuckerbrod",
                "S.A. Baker",
                "K.M. Sanders"
            ],
            "title": "Fundus2angio: A conditional gan architecture for generating fluorescein angiography images from retinal fundus photography",
            "venue": "in: International Symposium on Visual Computing, Springer",
            "year": 2020
        },
        {
            "authors": [
                "S.A. Kamran",
                "K.F. Hossain",
                "A. Tavakkoli",
                "S.L. Zuckerbrod"
            ],
            "title": "Attention2angiogan: Synthesizing fluorescein angiography from retinal fundus images using generative adversarial networks",
            "venue": "in: 2020 25th International Conference on Pattern Recognition (ICPR), IEEE",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Background and Objective: Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk.\nMethods: We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pretherapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement module to ensure the effectiveness of disease evolution learning and obtain realistic SD-OCT images by adversarial training.\nResults: SHENet is validated on 383 SD-OCT cubes of 22 nAMD patients based on three well-designed schemes (P-0, P-1 and P-M) based on the\n\u2217Qiang Chen is the corresponding author of this work. Email addresses: zhangyuhan@njust.edu.cn (Yuhan Zhang),\nhuangkun@njust.edu.cn (Kun Huang), chaosli@njust.edu.cn (Mingchao Li), yuansongtao@vip.sina.com (Songtao Yuan), chen2qiang@njust.edu.cn (Qiang Chen)\nPreprint submitted to CMPB August 15, 2023\nar X\niv :2\n30 8.\n06 43\n2v 1\n[ ee\nss .I\nV ]\nquantitative and qualitative evaluations. Three metrics (PSNR, SSIM, 1- LPIPS) are used here for quantitative evaluations. Compared with other generative methods, the generative SD-OCT images of SHENet have the highest image quality (P-0: 23.659, P-1: 23.875, P-M: 24.198) by PSNR. Besides, SHENet achieves the best structure protection (P-0: 0.326, P-1: 0.337, P-M: 0.349) by SSIM and content prediction (P-0: 0.609, P-1: 0.626, PM: 0.642) by 1-LPIPS. Qualitative evaluations also demonstrate that SHENet has a better visual effect than other methods.\nConclusions: SHENet can generate post-therapeutic SD-OCT images with both high prediction performance and good image quality, which has great potential to help ophthalmologists forecast the therapeutic effect of nAMD.\nKeywords: nAMD, Generative Adversarial Network, Graph Neural Network, Predictive Generation"
        },
        {
            "heading": "1. Introduction",
            "text": ""
        },
        {
            "heading": "1.1. Application Background",
            "text": "Neovascular age-related macular degeneration (nAMD) is a main subtype of AMD. As the intravitreal vascular endothelial growth factor (VEGF) level elevates, choroidal neovasculars invade the avascular outer retina and severely damage photoreceptors, resulting in rapid vision loss [1]. At present, antiVEGF injection is considered the preferred nAMD therapy [2]. Although ophthalmologists always give anti-VEGF injections after nAMD diagnosis, nAMD does not always respond satisfactorily to treatment. Besides, due to the lack of uniform guidelines, it is difficult for ophthalmologists to predict the short-term therapeutic response after anti-VEGF injection according to their subjective experiences [3]. This results in huge economic pressures and waste of resources [4]. Therefore, based on the known pre-therapeutic status of nAMD at time point t1, predicting the post-therapeutic status of nAMD at time point t2 = t1 +\u2206t can effectively forecast the efficacy of anti-VEGF injection for each patient. This can promote better clinical decision making."
        },
        {
            "heading": "1.2. SD-OCT Imaging Brief",
            "text": "Spectral-domain optical coherence tomography (SD-OCT) is a noninvasive, depth-resolved, high-resolution, and volumetric imaging technique. SD-OCT\nhas become a pivotal diagnostic tool to visualize and quantitatively evaluate retinal morphological changes [5], including the diagnosis and tracing of nAMD [6, 7]. Each SD-OCT imaging can produce 3D volumetric images, also known as a cube. As shown in Fig. 1(a), we take Cirrus SD-OCT device as the example, each SD-OCT cube contains 1024\u00d7 512\u00d7 128 voxels with a corresponding trim size of 2mm\u00d7 6mm\u00d7 6mm on the retina. In a cube, each slice along the vertical direction, with the size of 1024\u00d7 512, is known as a B-scan. A complete SD-OCT cube contains 128 continuous B-scans in space."
        },
        {
            "heading": "1.3. Dilemma of Multi-horizon Predictions on Medical Images",
            "text": "Diseases-associated predictions are more restrictive than general predictions. In the field of pattern recognition, multi-horizon predictions have been widely applied for natural language predictions, action predictions, video predictions, traffic predictions and etc. Given Xt1:tN = [Xt1 ,Xt2 , \u00b7 \u00b7 \u00b7 ,XtN ] \u2208 R as the historical N observations, each observation is obtained at the different time point and the time interval between any two adjacent time point is uniform. The actual future M observations are formally expressed as XtN+1:tN+M = [XtN+1 ,XtN+2 , \u00b7 \u00b7 \u00b7 ,XtN+M ] \u2208 R. We expect multi-horizon pre-\ndictions to learn a mapping function F : Xt1:tN \u2192 X\u0303tN+1:tN+M to obtain the prediction result X\u0303tN+1:tN+M as close as XtN+1:tN+M , as shown in Fig. 2(a). However, when applying multi-horizon predictions on medical images, several actual challenges raise:\n\u2022 Obtaining long-series observations from the same patient is intractable in practical clinical scenes, and it also is necessary to make effective predictions for a new patient with only one observation at the current time point.\n\u2022 For serial medical data, given any two different time points ti, tj \u2208 (t2, tN ], time intervals from adjacent time points may be different, namely ti \u2212 ti\u22121 \u0338\u2261 tj \u2212 tj\u22121.\n\u2022 Most medical observations generate 3D data and it is expensive for GPUs to learn from serial 3D data.\n\u2022 Therapeutic intervention dependent on medicine injection at a random time point is a key factor that cannot be ignored for diseases-associated predictions, and most medicine injection treatments only work for a short period of time.\n\u2022 It is well-known that the prediction accuracy decreases over time and risk tolerance on medical predictions is lower than other predictions, so clinicians always pay more confidence on the short horizon than the longer horizon.\nThese difficulties make it difficult for multi-horizon predictions to be really applied to medical images, thus learning a single-horizon prediction for medical images is more practical and realistic. Given one historical observation Xti \u2208 R at time point ti, single-horizon prediction learns a mapping function F : Xti \u2192 X\u0303ti+1 to obtain the prediction result at time point ti+1, as shown in Fig. 2(b)."
        },
        {
            "heading": "1.4. Target of Our Work",
            "text": "Nowadays, most existing disease prediction methods in the field of medical image processing fall into two classes, namely classification-based image-tocategory (I2C) predictions, and regression-based image-to-parameter (I2P) predictions. Few works have focused on generation-based image-to-image (I2I)\npredictions, even though generative adversarial networks (GANs) have been widely used for modality transformation between medical images from different imaging devices. The aim of post-therapeutic prediction is to generate an image that explains how the anatomical appearance changes after treatment. We consider that the predictive post-therapeutic SD-OCT images would enable a better understanding of nAMD and clinical decision-making by presenting a visual post-therapeutic status of nAMD. According to the above, the target of our work is defined as:\nGiven a serial SD-OCT cubes with fixed time interval \u2206t, writing as X = [Xt1 ,Xt2 , \u00b7 \u00b7 \u00b7 ,XtN+M ] \u2208 R, and any time point ti can be represented as ti = t1 + (i \u2212 1)\u2206t, i \u2208 [1, N +M ]. Anti-VEGF injection is given at each time point. For any SD-OCT cube Xti , we hope to learn a mapping function F : Xti \u2192 X\u0303ti+1 to predictively generate X\u0303ti+1 as close as Xti+1 . However, learning the mapping function F directly based on a 3D SDOCT cube is difficult and cost-consuming. Thus, for the SD-OCT cube Xti = [x 1 ti ,x2ti , \u00b7 \u00b7 \u00b7 ,x 128 ti ], where xjti is j -th B-scan in Xti , we learn the 2D mapping function F2D : xjti \u2192 x\u0303 j ti+1\n. F2D is carried out 128 times until the SD-OCT cube is predicted completely."
        },
        {
            "heading": "1.5. Contributions",
            "text": "In this paper, we present a Single-Horizon disease Evolution Network (SHENet) to predictively generate the post-therapeutic SD-OCT images based on pre-therapeutic SD-OCT images with nAMD. SHENet can help forecast the short-term response of anti-VEGF injection for individual nAMD patients. The main contributions in this paper are summarized as:\n\u2022 We explore the possibility of predictively generating post-therapeutic\nSD-OCT images based on pre-therapeutic SD-OCT images with nAMD, and further propose SHENet to solve this problem.\n\u2022 Graph evolution module (GEM) is proposed to imprison the process of disease evolution in the high-dimensional latent space by graph representation learning.\n\u2022 Evaluation reinforcement module (ERM) is proposed to reinforce the disease evolution process by combining an additional reconstruction generator and contrastive learning.\n\u2022 We design targeted experimental schemes according to actual clinical realities, and the results demonstrate that SHENet has great potential to generate post-therapeutic SD-OCT images with both high prediction performance and good image quality."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Predictions on AMD",
            "text": "In clinical scenarios, several prediction requirements have been raised around AMD by ophthalmologists. In present, there are no uniform guidelines to be helpful for making predictions and ophthalmologists rely only on their own rich clinical experiences. Thus, ophthalmologists warrant the need for objective outcomes of subjective predictions. AMD typically develops from an early to an advanced form and advanced AMD is difficult to be cured effectively. When AMD is in its early stage, ophthalmologists predict the risk of progression to advanced AMD within the future short term in order to adapt therapies, recommendations, and follow-up frequency [8, 9, 10, 11, 12]. For advanced nAMD, ophthalmologists predict best-corrected visual acuity (BCVA) outcomes in patients receiving standard therapy [13, 14, 15]. For geographic atrophy (GA), as non-neovascular advanced AMD, there is a lack of effective treatments for retinal areas that have progressed to GA. But predicting the GA progression [16, 17, 18, 19, 20, 21] could allow for a better understanding of the pathogenesis and forewarn preventive treatment to normal retinal areas that are at high risk of developing GA in the future. Besides, several other predictions also reveal clinical needs. Bogunovic et al. [22] predicted low and high anti-VEGF injection requirements based on sets of SD-OCT images acquired during the initiation phase in nAMD. Liu et al. [23] and Lee et al. [24] exploringly generated individualized post-therapeutic\nSD-OCT images that could predict the short-term response of anti-VEGF injection for nAMD based on pre-therapeutic images using Pix2Pix. Forshaw et al. [25] predicted the visual gain from cataract surgery when the main cause of vision loss is nAMD. Pham et al. [26] generated future fundus images for early age-related macular degeneration based on generative adversarial networks."
        },
        {
            "heading": "2.2. Generative Adversarial Networks (GANs)",
            "text": "Generative adversarial networks (GANs) [27] have become one of the widely leveraged techniques for generating images that look like the real thing. Traditional L1 or L2 supervision often results in blurred images [28], but GANs introduce an additional discriminator to play a min-max game with the generator that enforces the generator to output more realistic images. The discriminator is mainly used to estimate the divergence difference between generated fake images and real images. Different adversarial losses estimate different divergences between Wasserstein divergence distribution [29] and f-divergence family distribution [30]. If target domains own multiple distributions, conditional GAN (cGAN) [31] uses conditional labels to guide the generator to produce the images by fitting the specified distribution. For example, Yoo et al. [32] proposed a postoperative appearance prediction model for orbital decompression surgery for thyroid ophthalmopathy using a conditional GAN. GANs are also used for image-to-image (I2I) translation, in which pixel-level losses or feature-level losses are embedded to ensure the quality and stability of the generated image [26, 33, 34]. Besides, literatures [35, 36] explored the impact of discriminator architecture on GANs."
        },
        {
            "heading": "3. Methods",
            "text": "This study was approved by the ethic committee of the First Affiliated Hospital with Nanjing Medical University."
        },
        {
            "heading": "3.1. Pre-processing: Voxel-wise Serial Alignment",
            "text": "The random deviation of rotation angle and displacement among SD-OCT cubes at different time points caused by human operations cannot be learned, which further limits the I2I prediction. In other words, for two original SD-OCT cubes from the same patient captured at different time points, the B-scan with the same index in two cubes may not correspond to the same anatomical tissue. Thus, we perform voxel-wise serial alignment, including\nimage flattening in vertical direction and image alignment in horizontal-axial directions, for all SD-OCT cubes before running SHENet. In this way, all SD-OCT cubes at different time points are aligned in the 3D space. The aligned SD-OCT cube can be seen in Fig. 1(b).\nImage Flattening in Vertical Direction. We first obtain the locations of Bruch\u2019s membrance (BM) of all B-scans by layer segmentation approach [37], and then all B-scans are flattened based on BM. Finally, we crop the B-scan restricted to the region from 0.75mm above BM to 0.25mm below BM. In this processing, negligible vitreum regions and sclera regions are removed as much as possible. Image flattening also reduces the size of the input SD-OCT images to alleviate the memory pressure of hardware.\nImage Alignment on Horizontal-Axial Directions. We first generate the 2D vessel fundus images of each SD-OCT cube by restricting the projection region between inner segments/outer segments (IS/OS) and BM. The vessel fundus images are aligned using a scale-invariant feature transform (SIFT) flow method to obtain the transformation matrixes. Lastly, the transformation matrixes are applied to horizontal-axial directions of SD-OCT cubes to obtain the aligned SD-OCT cubes."
        },
        {
            "heading": "3.2. Overview of SHENet",
            "text": "Pix2Pix [38] has been a great success to apply conditional GAN (cGAN) to supervised I2I translation. It regards input images as additional conditions to learn an I2I mapping, consequently producing specified output images. SHENet further extends Pix2Pix for our nAMD prediction task and its overview is illustrated in Fig. 3. In terms of model architecture, SHENet consists of:\n(1) Prediction Generator Gp is the core of SHENet, including a feature encoder, a graph evolution module (GEM), and a feature decoder, which can predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with nAMD.\n(2) Reconstruction Generator Gr is an auxiliary generator in the training process and will be removed in the model inference stage. Gr removes GEM from Gp and utilizes a reconstruction task to help Gp imprison the process of disease evolution in the high-dimensional latent space, and further to distill the function of feature encoder and decoder.\n(3) Evolution Reinforcement Module (ERM) reinforces the process of disease evolution by working with Gr based on contrastive learning.\nt2 \u2208 Xt2 as multi-channel inputs are sent to G\nr to reconstruct xjt2 .\nERM reinforces the disease evolution in the high-dimensional latent space. Dq ensures the image quality of x\u0303jt2 and x j t2 is realistic, and G p decides whether x\u0303jt2 and x j t2 are paired with xjt1 in terms of pathological characterization that reflects therapy response. In the inference stage of SHENet, only prediction generator Gp remains and other all components are removed.\n(4) Quality Discriminator Dq ensures that the predicted and reconstructed images look realistic.\n(5) Pair Discriminator Dp determines whether predicted and reconstructed images are paired with input images in terms of pathological characterization that reflects therapy response.\n3.2.1. Motivation: Why learn the disease evolution in the highdimensional latent space?\nLiu et al. [23] and Lee et al. [24] have preliminarily used Pix2Pix to generate post-therapeutic SD-OCT images and show the feasibility, but learning a pixel-to-pixel prediction on SD-OCT images seems not to be a delicate work. Severe speckle noise in SD-OCT images and the imbalanced proportion of foreground pixels (i.e. nAMD) relative to the background pixels (i.e. non-nAMD) degenerate the pixel-to-pixel prediction performance. Compared with Pix2Pix, SHENet imprisons the process of disease evolution\nin the high-dimensional latent space:\nPix2Pix : xjt1 Generator\u2212\u2192 x\u0303jt2 SHENet : xjt1 Enc\u2212\u2192 f jt1 Pred\u2212\u2192 f\u0303 j t2 Dec\u2212\u2192 x\u0303jt2 (1)\nwhere f jt1 is the encoding features of x j t1 and f\u0303\nj t2 is the predicted features of\nxjt1 . The high-dimensional features are more condensed and effective, because the invalid background and noise information are removed, and distinct disease information is retained. Therefore, we consider that learning the process of disease evolution after treatment in the high-dimensional latent space is more reasonable than performing a pixel-to-pixel prediction.\n3.2.2. Multiple B-scans As Model Input.\nGiven an SD-OCT cube at time point t1 as Xt1 = [x 1 t1 ,x2t1 , \u00b7 \u00b7 \u00b7 ,x 128 t1 ] and aligned SD-OCT cube at time point t2 = t1 +\u2206t as Xt2 = [x 1 t2 ,x2t2 , \u00b7 \u00b7 \u00b7 ,x 128 t2 ]. In general, we should train a 2D mapping F2D to predictively generate x\u0303jt2 as close as real xjt2 :\nF2D : xjt1 \u2192 x\u0303 j t2\n(2)\nHowever, in SD-OCT images, xjt1 with similar pathological characterization may develop to xjt2 with different pathological characterization, resulting in difficult model convergence and random prediction results. For example, for patient-1, a healthy SD-OCT B-scan at time point t1 evolves to an SD-OCT B-scan with nAMD at time point t2. However, for patient-2, a healthy SD-OCT B-scan at time point t1 may remain its healthy status at time point t2. Thus, to speed up the model convergence and improve the model robustness, we choose to stack a little piece of B-scans of Xt1 as X [j;\u2206s] t1 = [x j\u2212\u2206s t1 , \u00b7 \u00b7 \u00b7 ,x j t1 , \u00b7 \u00b7 \u00b7 ,x j+\u2206s t1 ] \u2208 Xt1 as multi-channel inputs to predict x\u0303jt2 :\nF2D : X[j;\u2206s]t1 \u2192 x\u0303 j t2\n(3)\nF2D slides on each B-scan of Xt1 until X\u0303t2 is predicted completely."
        },
        {
            "heading": "3.3. Prediction Generator Gp of SHENet",
            "text": "The prediction generator Gp of SHENet consists of three modules: feature encoder (GE), graph evolution module (GEM), and feature decoder (GD). The pre-therapeutic SD-OCT images are mapped to high-dimensional latent space\nby GE, then GEM predicts the process of disease evolution after treatment, and finally, GD recovers the predicted features to post-therapeutic SD-OCT images.\nFeature Encoder (GE). Fig. 4(a) illustrates the architecture of feature encoder GE that stacks several encoding blocks (EncBlock) and a mapping layer. Each EncBlock consists of two 3\u00d73 convolutional layers with ReLU activation, one channel attention block (CAB) [39], and one max-pooling\nlayer for down-sampling. CAB (Fig. 4(b)) improves the quality of features by explicitly modeling the interdependencies between the channels of its convolutional features. Through CAB, informative features are selectively emphasized and less useful ones are suppressed.\nGraph Evolution Module (GEM). Given the encoding features f jt1 \u2208 Rh\u2032\u00d7w\u2032\u00d7d\u2032 by GE, where P = h\u2032 \u00d7 w\u2032 is feature numbers and d\u2032 is feature dimension. We consider the evolution of each feature should be related to other features. Intuitively, a series of convolution layers or a linear regression is easier to implement to predict the change in the feature level. However, a series of convolution layers or a linear regression defaults that all other features contribute equally to the targeted feature. In fact, the contributions of all features are unequal due to their different semantic information. Graph neural networks can model the contributions of all features automatically, which is more reasonable than a series of convolution layers or a linear regression. Let each feature be the vertex of the graph, and we can represent f jt1 as a fully-connected undirected graph R = (V , E), where V is vertex set and E is edge set. Further given the adjacency matrix A, the diagonal degree matrix D and the identity matrix I, the relation of features can be extracted by the following formula:\nH l+1 = \u03c3(A\u0303H lW l), A\u0303 = D\u2212 1 2 (A+ I)D\u2212 1 2 (4)\nwhere W l \u2208 Rdin\u00d7dout is the learnable weight, and \u03c3 is the Mish function [40]. H l \u2208 RP\u00d7din , H l+1 \u2208 RP\u00d7dout are the input features and the updated features at l -th layer. Intuitively, for p-th vertex Vp, its all neighbors contribute unequally to the evolution of Vp. To model this, we introduce the multihead GAT [41] to explicitly consider the importance of the neighbors. Take independent GAT as an example, we calculate the attentive score \u03b3pq of the vertex pair (hp,hq):\n\u03b3pq = exp(LReLU(\u03b1T [Whp,Whq]))\u2211\nk\u2208Np exp(LReLU(\u03b1 T [Whp,Whk]))\n(5)\nwhereNp is the neighbors of p-th vertex in the graph, and [\u00b7] is a concatenation operation. W \u2208 Rdout\u00d7din indicates a linear mapping and \u03b1 \u2208 R2dout indicates a single-layer fully-connected layer. LReLU is the nonlinear activation. Then, we compute the average of multi-head GAT for the output of vertex Vp:\nh\u0303p = \u03c3( 1\nG G\u2211 g=1 \u2211 \u2211\nq\u2208Ni\n\u03b3gpqW ghp) (6)\nwhere G=5 is the number of heads. Similarly, we repeat the GAT computation on each vertex to obtain the complete output. Considering the powerful ability of graph neural networks for information inference, GEM is only composed of 3 multi-head GATs with the channel numbers 1024, 1024, 1024. The overview of GEM is shown in Fig. 4(c).\nFeature Decoder (GD). Fig. 4(d) illustrates the architecture of feature decoder GD that stacks several decoding blocks (DecBlock) and a mapping layer. Each DecBlock consists of one de-convolution layer with ReLU activation, and two convolutional layers with ReLU activation."
        },
        {
            "heading": "3.4. Evolution Reinforcement Module (ERM)",
            "text": "To reinforce the process of disease evolution, we introduce reconstruction generator Gr that removes GEM from Gp to reconstruct xjt2 by inputting X\n[j;\u2206s] t2 :\nGp : X[j;\u2206s]t1 GE\u2212\u2192 f jt1 GEM\u2212\u2192 f\u0303 j t2 GD\u2212\u2192 x\u0303jt2 Gr : X[j;\u2206s]t2 GE\u2212\u2192 f jt2 GD\u2212\u2192 xjt2\n(7)\nPrediction generator Gp and reconstruction generator Gr share feature encoder GE and feature decoder GD. We use the project head h(\u00b7) [42] to map f jt1 , f\u0303 j\nt2 , f jt2 to z j t1 , z\u0303 j t2 , zjt2 for evolution reinforcement learning:\nz = h(f) = W (2)(ReLU(W (1) \u00b7GAP (f))) (8)\nwhere W (1), W (2) are weight matrixes, GAP (\u00b7) indicates global average pooling. We hope predicted z\u0303jt2 and real z j t2 should be as similar as possible, while predicted z\u0303jt2 and real z j t1 should be different. Thus, we build the evolution reinforcement learning based on contrastive loss:\nLERM(Gp,Gr) = \u2212log exp(Sim(z\u0303jt2 , z j t2)/\u03c4)\nexp(Sim(z\u0303jt2 , z j t1)/\u03c4)\n(9)\nwhere \u03c4=1 is the temperature factor, Sim(\u00b7) is the cosine similarity metric. ERM also further distills the function of the feature encoder and feature decoder."
        },
        {
            "heading": "3.5. Discriminators of SHENet",
            "text": "For the predictively generated x\u0303jt2 and the reconstructed x j t2 , we firstly\nuse a quality discriminator Dq to ensure their image quality is realistic:\nDq(x\u0303jt2) \u2192 T/F, D q(xjt2) \u2192 T/F (10)\nFurthermore, we consider that pathological manifestation of x\u0303jt2 and x j t2 should show the response of xjt1 after anti-VEGF injection. In other words, x\u0303jt2 and x j t2 should be paired with x j t1 . Thus we also use a pair discriminator Dp to make the decision:\nDp(x\u0303jt2 ,x j t1) \u2192 T/F, D p(xjt2 ,x j t1) \u2192 T/F (11)\nIn SHENet, both Dq and Dp follow the popular PatchGAN [38]."
        },
        {
            "heading": "3.6. Adversarial Training",
            "text": "In the training process, we use LL1 measures the L1 distance between output images and ground truth:\nLL1(Gp) = E[||xjt2 \u2212 G p(X [j;\u2206s] t1 )||1] LL1(Gr) = E[||xjt2 \u2212 G r(X [j;\u2206s] t2 )||1]\n(12)\nA pair discriminator Dp makes the decision whether (x\u0303jt2 , x j t1) and (x j t2 , x j t1) are paired:\nLGANp(Gp,Dp) = E[logDp(xjt1 ,x j t2)] + E[log(1\u2212D p(xjt1 ,G p(X [j;\u2206s] t1 )))] LGANr(Gr,Dp) = E[logDp(xjt1 ,x j t2)] + E[log(1\u2212D p(xjt1 ,G r(X [j;\u2206s] t2 )))] (13)\nA quality discriminator Dq ensures x\u0303jt2 and x j t2 are realistic:\nLGANq(Gp,Dq) = E[logDq(xjt2)] + E[log(1\u2212D q(Gp(X[j;\u2206s]t1 )))] LGANq(Gr,Dq) = E[logDq(xjt2)] + E[log(1\u2212D q(Gr(X[j;\u2206s]t2 )))]\n(14)\nWe combine all losses together and the complete optimization can be represented as:\nGp =arg min Gp,Gr max Dp,Dq [LGANp(Gp,Dp) + LGANp(Gr,Dp)\n+ LGANq(Gp,Dq) + LGANq(Gr,Dq) + \u03bb(LL1(Gp) + LL1(Gr)) + \u00b5LERM(Gp,Gr)]\n(15)"
        },
        {
            "heading": "4. Results",
            "text": ""
        },
        {
            "heading": "4.1. Data Acquisition",
            "text": "According to clinical research, the therapeutic effect of anti-VEGF injection for nAMD lasts about one month, so nAMD patients treated should be followed up one month later to evaluate the therapeutic response of nAMD. Then, ophthalmologists can make the decision on whether further treatment is needed. However, to the best of our knowledge, there are no large-scale, public, and annotated SD-OCT datasets that can be acquired for our model validation due to the differences in imaging protocols, privacy problems, and lack of medical integration. This is also a common problem in the field of medical image processing.\nThe experimental data includes 46208 paired SD-OCT images obtained from 383 SD-OCT cubes of 22 nAMD patients. Only one eye per nAMD patient is included in the dataset, namely 22 eyes are included. Each patient contains about 17 serial SD-OCT cubes captured at different time points. During the SD-OCT imaging performed once a month, ophthalmologists gave anti-VEGF injections for nAMD treatment. The time interval between any adjacent time points is about one month, that is to say, the predictive single horizon is fixed to one month. These SD-OCT cubes were captured by Cirrus SD-OCT device Carl Zeiss Meditec, Inc., Dublin, CA. The size of an SD-OCT cube is 1024 \u00d7 512 \u00d7 128. Detailed attribute descriptions are presented in Table 1."
        },
        {
            "heading": "4.2. Evaluation & Comparison",
            "text": "For the predictively generated post-therapeutic SD-OCT images, qualitative evaluation using human subjective judgment is the most straightforward and effective way to verify the prediction performance of SHENet. Besides, we also use three metrics, peak signal-to-noise ratio (PSNR), structural similarity (SSIM) and learned perceptual image patch similarity (LPIPS), to evaluate the prediction performance quantitatively. These three metrics have been previously used for the evaluation of image generation [43]. PSNR measures the image quality and higher PSNR means less distortion. SSIM measures the structure similarity and higher SSIM means higher structure similarity. LPIPS measures the similarity of deep visual features and lower LPIPS means higher feature similarity. To make LPIPS keep the consistency with PSNR and SSIM, namely higher value indicates better performance, here we use 1-LPIPLS to replace LPIPS.\nTo highlight the advantages of SHENet in terms of I2I prediction performance, we choose four GANs-based methods for competitive comparison, namely Pix2Pix [38], Pix2PixHD [44], Fundus2Angio [45], Att2Angio [46]. Pix2Pix and Pix2PixHD have become popular baselines and are widely used for image generation. Fundus2Angio and Att2Angio are the latest GANsbased methods for modality transformation in the field of medical image processing."
        },
        {
            "heading": "4.3. Experimental Designs",
            "text": "In order to conform to real clinical application scenes, we design our experiments from three sub-evaluations:\nP-0 Evaluation. For the new nAMD patients that only have one SDOCT imaging at time point t1, we want to predict post-therapeutic SD-OCT images at time point t2. Thus, the SD-OCT images from all other patients are used for training SHENet. We use five-fold cross-validation until all patients are tested, as shown in Fig. 5(a).\nP-1 Evaluation. For the nAMD patients that have two SD-OCT imaging at time points t1 and t2, we would like to predict post-therapeutic SD-OCT images at time point t3. Thus, we transfer the model parameters from P-0 evaluation and fine-tune SHENet only using (Xt1 ,Xt2). In the model inference stage, we take Xt2 as model input to predictively generate Xt3 . We use five-fold cross-validation until all patients are tested, as shown in Fig. 5(b).\nP-M Evaluation. For the nAMD patients that have owned multiple regular SD-OCT imaging, we would like to predict consequent post-therapeutic SD-OCT images. Thus, we remain the last two SD-OCT cubes of all patients for model validation and the remaining SD-OCT cubes are used for training model, as shown in Fig. 5(c)."
        },
        {
            "heading": "4.4. Implementation Details",
            "text": "The experiment is constructed in a hardware condition with Intel Xeon CPU, one GeForce RTX 3090 GPU and 128 GB RAM, and a software condition with Python3.5 and Pytorch.\nFor the input multiple B-scans, we choose \u2206s = 3 and zero-padding operation is used if j \u2212\u2206s < 0 or j +\u2206s > 128, thus the model input is a three-channel SD-OCT image. The number of encoding blocks and decoding blocks is 5. The output dimensions of 5 encoding blocks are {128, 256, 512, 1024, 2048} respectively. The output feature size of the feature encoder is 16\u00d7 16\u00d7 2048, which means the vertex number of a fully-connected graph in GEM is 16 \u00d7 16. In the loss function, \u03bb = 100 and \u00b5 = 10 balance the weights among GAN losses, L1 losses and contrastive loss. Flip and rotation operations are used here for data augmentation.\nAdam optimizer with initial learning rate of 0.0001 and weight decay of 0.1 is chosen for model optimization. The batch-size is set to 2. SHENet was trained for 100 epochs and the training was properly completed."
        },
        {
            "heading": "4.5. Qualitative Evaluation.",
            "text": "We qualitatively compare our SHENet with competing methods by visualizing the examples from two different nAMD patients based on P-0, P-1 and\nt1 to Pix2Pix [38], Pix2PixHD\n[44], Fundus2Angio [45], Att2Angio [46] and the proposed SHENet to predictively generate x\u0303jt2 based on three experimental designs (P-0, P-1 and P-M), and further evaluate them with ground truth xjt2 .\nP-M evaluations, as shown in Fig. 6. In terms of image quality, benefiting from adversarial training, all methods can produce clear and unblurred SDOCT images. Besides, SHENet can stably maintain the structural integrity of the retina to achieve the better visual effects and actually predict the status of nAMD one month later after anti-VEGF injections."
        },
        {
            "heading": "4.6. Quantitative Evaluation.",
            "text": "We quantitatively evaluate the prediction performance based on P-0, P-1 and P-M evaluations using PSNR, SSIM and 1-LPIPS metrics, as shown in Table 2. For P-0 and P-1 evaluations, we calculate the average of five-fold crossvalidation as the final results. Overall, SHENet obtains consistently superior results than competing methods on three experimental designs. Among three experimental designs, SHENet achieves the best prediction performance on P-M evaluation (PSNR: 24.198, SSIM: 0.349, 1-LPIPS: 0.642), followed by P-1 evaluation (PSNR: 23.875, SSIM: 0.337, 1-LPIPS: 0.626), and P-0 evaluation (PSNR: 23.659, SSIM: 0.326, 1-LPIPS: 0.609) has the worst results, which shows the consistency with qualitative evaluation in Fig. 6. That is because the difference among patients is significantly greater than that among serial SD-OCT observations from the same patient. We consider that the gap will narrow by further collecting more samples from more patients."
        },
        {
            "heading": "4.7. Ablation Analysis",
            "text": "Evaluation of Hyper-parameter Values. We analyze the influence of hyper-parameters \u03bb and \u00b5 in loss function with the experiment. We first fix \u00b5 to be 1 and vary \u03bb from 10 to 190 with interval 10. As shown in Fig. 7(a), 1-LPIPS metrics of three evaluations are consistently increasing when rising the of \u03bb from 10 to 100 and consistently decreasing when further rising \u03bb. We then fix the value of \u03bb to 100 and vary \u00b5 from 1 to 19 with interval 1. As shown in Fig. 7(b), the performance of SHENet achieves the highest values when \u00b5 = 10.\nEvaluation of Model Inputs. In terms of model input, we investigate the difference between single B-scan and multiple B-scans, and the quantitative\ncomparisons are recorded in Table 3. We find that the prediction results using multiple B-scans as model input demonstrate significant improvements to those using a single B-scan as model input on three experimental designs.\nEvaluation of Model Architecture. In the training process, we build our single-horizon disease evolution in the high-dimensional latent space based on the cooperation of GEM, Gr and ERM. We investigate the impact of three components by stacking them one by one. As shown in Table 4, SHENet achieves better results than others when considering these three components jointly. This evidences that SHENet effectively imprisons the process of disease evolution in GEM and further reinforces the process by Gr+ERM. Therefore, adopting all three can improve prediction performance.\nEvaluation of Discriminator. To verify the relevance of two discriminators (Dq, Dp), we separately analyze the prediction results of SHENet when one of them is reserved. When only using one of the two, quantitative metrics in Table 5 show performance degradation. Besides, we also observe that single Dp performs better than single Dq, as Dp can also control the image quality to a certain extent. Thus, it shows the necessity to use two independent discriminators to respectively control the image quality and the pathological characterization."
        },
        {
            "heading": "5. Discussion",
            "text": "In this paper, according to the actual clinical requirement, we explore the possibility of predictively generating post-therapeutic SD-OCT images based on pre-therapeutic SD-OCT images with nAMD, and propose a singlehorizon disease evolution network (SHENet) to solve it. SHENet learns the process of disease evolution in the high-dimensional latent space, rather than performing pixel-to-pixel prediction. This has the advantage of eliminating the influence of speckle noise and redundant background context on the prediction. Considering several inherent characteristics of medical images different from other modal data, we choose single-horizon prediction rather\nthan multi-horizon prediction to simplify the problem. In other words, we only predictively generate post-therapeutic SD-OCT images with a one-month time interval, and would not continue to predict the nAMD status of the third month.\nFrom Fig. 6, we can observe structural damage of the retina from competing methods, but SHENet can stably maintain the structural integrity of the retina to achieve a better visual effect. This benefits from two improvements to the proposed model: 1) SHENet uses two discriminators to respectively manage image quality and pathological characterization, but other competing methods only use one discriminator; 2) SHENet imprisons the process of disease evolution in the high-dimensional latent space, which makes the feature encoder and feature decoder concentrate on the restoration of image details.\nFurthermore, we should pay more attention to the correctness of predictive generation of nAMD, namely whether SHENet can actually predict the status of nAMD one month later after anti-VEGF injection. Generally speaking, reduction of nAMD volume is the best indicator of therapeutic response, but texture change and improvement of additional diseases also need to be taken into account. In example-1 in Fig. 6, by comparing the pre-therapeutic SD-OCT image with the post-therapeutic ground truth, nAMD is significantly reduced in size, which shows an excellent therapeutic response. In example-2 in Fig. 6, although the nAMD volume does not change significantly, the thickness of the retina becomes thinner, which still indicates an effective treatment as additional diseases are improved. By qualitative comparisons, the predicted visual nAMD of SHENet is closer to ground truth than that of competing methods, demonstrating that SHENet owns a more powerful ability to learn the disease evolution. We also visualize the latent space of Pix2Pix and our SHENet from sequential observations for further comparison. As observed in Fig. 8, the image features generated by our SHENet are more high-density than those generated by Pix2Pix. That is because our designed GEM and ERM can filter out disease-unrelated information effectively. Highquality image features can provide a good basis for disease evolution learning in the latent space.\nFrom Tables 2-5, we also observe lower PSNR & SSIM metrics than 1-LPIPS metric, and we consider this is caused by inherent severe speckle noise of SD-OCT images. Although denoising techniques can reduce the impact of speckle noise, the image quality would also become more blurry and the image details would be lost. Considering that the feature encoder owns the denoising ability, we do not apply any denoising technique during\nthe pre-processing, avoiding an overly prediction model. In SHENet, to make the predicted images look realistic, the feature decoder adds random speckle noise to the generated SD-OCT images. PSNR & SSIM metrics are evaluated based on the whole SD-OCT images, thus speckle noise can result in low quantitative values, but 1-LPIPS metric is evaluated based on the deep visual features that show higher values than PSNR & SSIM.\nTo explore the influence of the number of historical observations on predictive results, we conducted P-0 evaluation, P-1 evaluation and P-M evaluation in our experiments. These three evaluations used zero, one and multiple historical observations respectively from the testing patients for model training. From Table 2, we can find that all methods achieve the best results on P-M evaluation and have the worst results on P-0 evaluation, demonstrating that more historical observations of the same patients can improve their own prediction performance. Although our dataset contains only 22 nAMD patients, each patient contains about 17 sequential SD-OCT cubes captured at different time points with regular medicine injections. A total of 46208 SD-OCT image pairs are used for our model validation. Besides, common data augmentation operations (rotation and horizontal flipping) are used. Therefore, our method does not occur the overfitting phenomenon, which could be observed from the results of P-0 evaluation. In the future, we will validate our method on other medical images, such as fundus photos for AMD and OCT for DME.\nFig. 9 shows more cases of predicted post-therapeutic SD-OCT images from P-0 evaluation, where green dotted box denotes the best results and crimson dotted box denotes the worst results. Overall, SHENet has the\nability to predict the change trend of nAMD status with high image quality, but several flaws still appear. First, SHENet fails to predict the dramatic texture change of nAMD precisely, as shown in Fig. 9(5). Second, due to individual differences, SHENet is hard to model the personalized change rate of nAMD after medicine injection. For example, SHENet overestimates the effect of medicine injection in Figs. 9(6-7) and underestimates the effect of medicine injection in Fig. 9(8). We consider the main reason is the limited number of patients, leading to SHENet cannot learn comprehensive information due to the complexity of nAMD. We believe that SHENet can be improved significantly after learning from more nAMD patients, and we will also continually validate SHENet by collecting more data in the future.\nAlthough SHENet can produce high-quality SD-OCT images to visually reflect the status of nAMD one month later after anti-VEGF injection, it also has limitations. First, SD-OCT cubes at all time points need to be aligned by manual or automated alignment methods, since SHENet cannot learn the\nrandom deviations caused by man-made operations. Second, the time interval between the model input and ground truth must be consistent in the training process. Thus, SHENet can only predict the nAMD status after a single horizon and is incapable to predict longer horizons. Third, each time point for model training must ensure the same treatment intervention. SHENet can also be extended to train on the serial SD-OCT cubes without treatment intervention for predicting the single-horizon progression of nAMD."
        },
        {
            "heading": "6. Acknowledgment",
            "text": "This work was supported in part by the National Natural Science Foundation of China (62172223, 61671242), and the Fundamental Research Funds for the Central Universities (30921013105)."
        }
    ],
    "title": "Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration",
    "year": 2023
}