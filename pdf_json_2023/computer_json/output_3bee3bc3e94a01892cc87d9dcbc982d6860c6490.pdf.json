{
    "abstractText": "In this work, we show that learning the output distributions of brickwork random quantum circuits is average-case hard in the statistical query model. This learning model is widely used as an abstract computational model for most generic learning algorithms. In particular, for brickwork random quantum circuits on n qubits of depth d , we show three main results: \u2022 At super logarithmic circuit depth d = !(log(n)), any learning algorithm requires super polynomially many queries to achieve a constant probability of success over the randomly drawn instance. \u2022 There exists a d = O(n), such that any learning algorithm requires \u03a9(2n) queries to achieve a O(2\u2212n) probability of success over the randomly drawn instance. \u2022 At in nite circuit depth d \u2192 \u221e, any learning algorithm requires 22 many queries to achieve a 2\u22122 probability of success over the randomly drawn instance. As an auxiliary result of independent interest, we show that the output distribution of a brickwork random quantum circuit is constantly far from any xed distribution in total variation distance with probability 1 \u2212 O(2\u2212n), which con rms a variant of a conjecture by Aaronson and Chen. \u2217Author list in pseudorandom order. All authors contributed equally. Corresponding authors: a.nietner@fu-berlin.de, marios.ioannou@fu-berlin.de, m.hinsche@fu-berlin.de 1 ar X iv :2 30 5. 05 76 5v 1 [ qu an tph ] 9 M ay 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Alexander Nietner"
        },
        {
            "affiliations": [],
            "name": "Marios Ioannou"
        },
        {
            "affiliations": [],
            "name": "Ryan Sweke"
        },
        {
            "affiliations": [],
            "name": "Richard Kueng"
        },
        {
            "affiliations": [],
            "name": "Jens Eisert"
        },
        {
            "affiliations": [],
            "name": "Marcel Hinsche"
        },
        {
            "affiliations": [],
            "name": "Jonas Haferkamp"
        }
    ],
    "id": "SP:9b24a266c16af6b3a0b72c80b7c2e759a0ad2e5f",
    "references": [
        {
            "authors": [
                "Naman Agarwal",
                "Pranjal Awasthi",
                "Satyen Kale"
            ],
            "title": "A deep conditioning treatment of neural networks. 2021",
            "venue": "arXiv: 2002.01523",
            "year": 2002
        },
        {
            "authors": [
                "Scott Aaronson"
            ],
            "title": "The learnability of quantum states",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences",
            "year": 2007
        },
        {
            "authors": [
                "Scott Aaronson",
                "Lijie Chen"
            ],
            "title": "Complexity-theoretic foundations of quantum supremacy experiments",
            "venue": "Computational Complexity Conference (CCC 2017)",
            "year": 2017
        },
        {
            "authors": [
                "Scott Aaronson",
                "Daniel Gottesman"
            ],
            "title": "Improved simulation of stabilizer circuits",
            "venue": "In: Phys. Rev. A",
            "year": 2004
        },
        {
            "authors": [
                "Srinivasan Arunachalam",
                "Alex Bredariol Grilo",
                "Aarthi Sundaram"
            ],
            "title": "Quantum hardness of learning shallow classical circuits",
            "venue": "SIAM Journal on Computing",
            "year": 2021
        },
        {
            "authors": [
                "Eddie Aamari",
                "Alexander Knop"
            ],
            "title": "Statistical query complexity of manifold estimation",
            "venue": "Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing. Virtual Italy: ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Emmanuel Abbe",
                "Colin Sandon"
            ],
            "title": "Poly-time universality and limitations of deep learning",
            "venue": "en. arXiv:2001.02992",
            "year": 2020
        },
        {
            "authors": [
                "Srinivasan Arunachalam",
                "Ronald de Wolf"
            ],
            "title": "A survey of quantum learning theory. 2017",
            "year": 2017
        },
        {
            "authors": [
                "Boaz Barak"
            ],
            "title": "The complexity of public-key cryptography. Cryptology ePrint Archive, Paper 2017/365",
            "venue": "url: https://eprint.iacr.org/",
            "year": 2017
        },
        {
            "authors": [
                "Boaz Barak",
                "Chi-Ning Chou",
                "Xun Gao"
            ],
            "title": "Spoo ng linear cross-entropy benchmarking in shallow quantum circuits",
            "venue": "Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl\u2013LeibnizZentrum fu\u0308r Informatik,",
            "year": 2021
        },
        {
            "authors": [
                "Fernando G.S.L. Brand\u00e3o",
                "Wissam Chemissany",
                "Nicholas Hunter-Jones",
                "Richard Kueng",
                "John Preskill"
            ],
            "title": "Models of quantum complexity growth",
            "venue": "PRX Quantum 2",
            "year": 2021
        },
        {
            "authors": [
                "Bonnie Berger"
            ],
            "title": "The fourth moment method",
            "venue": "SIAM Journal on Computing",
            "year": 1997
        },
        {
            "authors": [
                "Avrim Blum",
                "Merrick Furst",
                "Je rey Jackson",
                "Michael J. Kearns",
                "Yishay Mansour",
                "Steven Rudich"
            ],
            "title": "Weakly learning DNF and characterizing statistical query learning using Fourier analysis",
            "venue": "Proceedings of the twenty-sixth annual ACM symposium on Theory of computing",
            "year": 1994
        },
        {
            "authors": [
                "Fernando G.S.L. Brandao",
                "Aram W. Harrow",
                "Micha\u0142 Horodecki"
            ],
            "title": "Local random quantum circuits are approximate polynomial-designs",
            "venue": "Communications in Mathematical Physics",
            "year": 2016
        },
        {
            "authors": [
                "Michael J. Bremner",
                "Richard Jozsa",
                "Dan J. Shepherd"
            ],
            "title": "Classical simulation of commuting quantum computations implies collapse of the polynomial hierarchy",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences",
            "year": 2011
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems",
            "year": 2020
        },
        {
            "authors": [
                "Anna Choromanska",
                "Mikael Hena",
                "Michael Mathieu",
                "G\u00e9rard Ben Arous",
                "Yann LeCun"
            ],
            "title": "The loss surfaces of multilayer networks",
            "year": 2015
        },
        {
            "authors": [
                "Sitan Chen",
                "Jerry Li",
                "Yuanzhi Li"
            ],
            "title": "Learning (very) simple generative models is hard",
            "venue": "arXiv preprint arXiv:2205.16003",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Dankert"
            ],
            "title": "E cient simulation of random quantum states and operators",
            "venue": "arXiv preprint quant-ph/0512217",
            "year": 2005
        },
        {
            "authors": [
                "Ilias Diakonikolas",
                "Daniel M. Kane",
                "Alistair Stewart"
            ],
            "title": "Statistical query lower bounds for robust estimation of high-dimensional Gaussians and Gaussian mixtures",
            "venue": "IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)",
            "year": 2017
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Frank McSherry",
                "Kobbi Nissim",
                "Adam Smith"
            ],
            "title": "Calibrating noise to sensitivity in private data analysis",
            "venue": "Theory of Cryptography. Ed. by Shai Halevi and Tal Rabin. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer,",
            "year": 2006
        },
        {
            "authors": [
                "Amit Daniely",
                "Gal Vardi"
            ],
            "title": "Hardness of learning neural networks with natural weights",
            "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS\u201920. Red Hook, NY, USA: Curran Associates Inc.,",
            "year": 2020
        },
        {
            "authors": [
                "Vitaly Feldman"
            ],
            "title": "Evolvability from learning algorithms",
            "venue": "Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing. Victoria British Columbia Canada: ACM,",
            "year": 2008
        },
        {
            "authors": [
                "Vitaly Feldman"
            ],
            "title": "A general characterization of the statistical query complexity",
            "venue": "url: http://arxiv.org/abs/1608.02198 (pages",
            "year": 2017
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Elena Grigorescu",
                "Lev Reyzin",
                "Santosh S Vempala",
                "Ying Xiao"
            ],
            "title": "Statistical algorithms and a lower bound for detecting planted cliques",
            "venue": "Journal of the ACM (JACM)",
            "year": 2017
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Cristobal Guzman",
                "Santosh Vempala"
            ],
            "title": "Statistical query algorithms formean vector estimation and stochastic convex optimization",
            "venue": "en. arXiv:1512.09170",
            "year": 2016
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Will Perkins",
                "Santosh Vempala"
            ],
            "title": "On the complexity of random satis ability problems with planted solutions",
            "venue": "en. arXiv:1311.4821",
            "year": 2018
        },
        {
            "authors": [
                "David Gross",
                "Koenraad Audenaert",
                "Jens Eisert"
            ],
            "title": "Evenly distributed unitaries: On the structure of unitary designs",
            "venue": "Journal of Mathematical Physics",
            "year": 2007
        },
        {
            "authors": [
                "Daniel Gottesman"
            ],
            "title": "The Heisenberg representation of quantum computers",
            "year": 1998
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems",
            "year": 2014
        },
        {
            "authors": [
                "Aram W. Harrow"
            ],
            "title": "The church of the symmetric subspace",
            "year": 2013
        },
        {
            "authors": [
                "Bogdan Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Oriol Vinyals",
                "Jack W. Rae",
                "Laurent Sifre"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Haferkamp",
                "Nicholas Hunter-Jones"
            ],
            "title": "Improved spectral gaps for random quantum circuits: large local dimensions and all-to-all interactions",
            "venue": "Physical Review A",
            "year": 2021
        },
        {
            "authors": [
                "Marcel Hinsche",
                "Marios Ioannou",
                "Alexander Nietner",
                "Jonas Haferkamp",
                "Yihui Quek",
                "Dominik Hangleiter",
                "Jean-Pierre Seifert",
                "Jens Eisert",
                "Ryan Sweke"
            ],
            "title": "A single T gate makes distribution learning hard",
            "venue": "url: http://arxiv.org/ abs/2207.03140",
            "year": 2022
        },
        {
            "authors": [
                "Aram W. Harrow",
                "Richard A. Low"
            ],
            "title": "Random quantum circuits are approximate 2-designs",
            "venue": "In: Communications in Mathematical Physics",
            "year": 2009
        },
        {
            "authors": [
                "Nicholas Hunter-Jones"
            ],
            "title": "Unitary designs from statistical mechanics in random quantum circuits. 2019",
            "venue": "doi: 10.48550/ARXIV.1905.12053. url: https://arxiv.org/abs/1905",
            "year": 1905
        },
        {
            "authors": [
                "Russell Impagliazzo",
                "Leonid A. Levin"
            ],
            "title": "No better ways to generate hard NP instances than picking uniformly at random",
            "venue": "Annual Symposium on Foundations of Computer Science",
            "year": 1990
        },
        {
            "authors": [
                "John Jumper",
                "Richard Evans",
                "Alexander Pritzel",
                "Tim Green",
                "Michael Figurnov",
                "Olaf Ronneberger",
                "Kathryn Tunyasuvunakool",
                "Russ Bates",
                "Augustin \u017d\u00eddek",
                "Anna Potapenko"
            ],
            "title": "Highly accurate protein structure prediction with AlphaFold",
            "venue": "Nature",
            "year": 2021
        },
        {
            "authors": [
                "Majid Janzamin",
                "Hanie Sedghi",
                "Anima Anandkumar"
            ],
            "title": "Beating the perils of nonconvexity: Guaranteed training of neural networks using tensor methods",
            "year": 2016
        },
        {
            "authors": [
                "Michael Kearns"
            ],
            "title": "E cient Noise-Tolerant Learning from Statistical Queries",
            "venue": "Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing. STOC \u201993. San Diego, California, USA: Association for Computing Machinery,",
            "year": 1993
        },
        {
            "authors": [
                "Richard Kueng",
                "David Gross"
            ],
            "title": "Qubit stabilizer states are complex projective 3designs",
            "year": 2015
        },
        {
            "authors": [
                "Michael Kharitonov"
            ],
            "title": "Cryptographic hardness of distribution-speci c learning",
            "venue": "Proceedings of the twenty- fth annual ACM symposium on Theory of computing",
            "year": 1993
        },
        {
            "authors": [
                "Michael J. Kearns",
                "Yishay Mansour",
                "D. Ron",
                "Ronitt Rubinfeld",
                "Robert E. Schapire",
                "Linda Sellie"
            ],
            "title": "On the learnability of discrete distributions",
            "venue": "Proceedings of the Twenty-sixth Annual ACM Symposium on Theory of Computing. STOC \u201994",
            "year": 1994
        },
        {
            "authors": [
                "Stefan Knabe"
            ],
            "title": "Energy gaps and elementary excitations for certain VBS-quantum antiferromagnets",
            "venue": "Journal of Statistical Physics",
            "year": 1988
        },
        {
            "authors": [
                "Michael J. Kearns",
                "Umesh Vazirani"
            ],
            "title": "An introduction to computational learning theory",
            "venue": "MIT press,",
            "year": 1994
        },
        {
            "authors": [
                "Nathan Linial",
                "Yishay Mansour",
                "Noam Nisan"
            ],
            "title": "Constant depth circuits, Fourier transform, and learnability",
            "venue": "Journal of the ACM (JACM)",
            "year": 1993
        },
        {
            "authors": [
                "Roi Livni",
                "Shai Shalev-Shwartz",
                "Ohad Shamir"
            ],
            "title": "On the computational e ciency of training neural networks",
            "year": 2014
        },
        {
            "authors": [
                "Kosuke Mitarai",
                "Makoto Negoro",
                "Masahiro Kitagawa",
                "Keisuke Fujii"
            ],
            "title": "Quantum circuit Learning",
            "venue": "en. In: Physical Review A 98.3 (2018)",
            "year": 2018
        },
        {
            "authors": [
                "Ashley Montanaro"
            ],
            "title": "Learning stabilizer states by Bell sampling",
            "year": 2017
        },
        {
            "authors": [
                "Bruno Nachtergaele"
            ],
            "title": "The spectral gap for some spin chains with discrete symmetry breaking",
            "venue": "In: Communications in Mathematical Physics",
            "year": 1996
        },
        {
            "authors": [
                "Mikito Nanashima"
            ],
            "title": "A theory of heuristic learnability",
            "venue": "Proceedings of Thirty Fourth Conference on Learning Theory. Conference on Learning Theory. optissn: 2640-3498",
            "year": 2021
        },
        {
            "authors": [
                "Ohad Shamir"
            ],
            "title": "Distribution-speci c hardness of learning neural networks. 2017",
            "year": 2017
        },
        {
            "authors": [
                "James C. Spall"
            ],
            "title": "An overview of the simultaneous perturbation method for e cient optimization",
            "venue": "en. In: Johns Hopkins APL Technical Digest",
            "year": 1998
        },
        {
            "authors": [
                "Jacob Steinhardt",
                "Gregory Valiant",
                "Stefan Wager"
            ],
            "title": "Memory, communication, and statistical queries",
            "venue": "COLT",
            "year": 2016
        },
        {
            "authors": [
                "Barbara M. Terhal",
                "David P. DiVincenzo"
            ],
            "title": "Classical simulation of noninteractingfermion quantum circuits",
            "venue": "Physical Review A",
            "year": 2002
        },
        {
            "authors": [
                "Leslie G. Valiant"
            ],
            "title": "Quantum circuits that can be simulated classically in polynomial time",
            "venue": "SIAM Journal on Computing",
            "year": 2012
        },
        {
            "authors": [
                "Leslie G. Valiant"
            ],
            "title": "A theory of the learnable",
            "venue": "Proceedings of the sixteenth annual ACM symposium on Theory of computing. STOC \u201984",
            "year": 1984
        }
    ],
    "sections": [
        {
            "text": "\u2022 At super logarithmic circuit depth d = !(log(n)), any learning algorithm requires super polynomially many queries to achieve a constant probability of success over the randomly drawn instance. \u2022 There exists a d = O(n), such that any learning algorithm requires \u03a9(2n) queries to achieve a O(2\u2212n) probability of success over the randomly drawn instance. \u2022 At in nite circuit depth d \u2192 \u221e, any learning algorithm requires 22\u03a9(n) many queries to achieve a 2\u22122\u03a9(n) probability of success over the randomly drawn instance. As an auxiliary result of independent interest, we show that the output distribution of a brickwork random quantum circuit is constantly far from any xed distribution in total variation distance with probability 1 \u2212 O(2\u2212n), which con rms a variant of a conjecture by Aaronson and Chen.\n\u2217Author list in pseudorandom order. All authors contributed equally. Corresponding authors: a.nietner@fu-berlin.de, marios.ioannou@fu-berlin.de, m.hinsche@fu-berlin.de\nar X\niv :2\n30 5.\n05 76\n5v 1\n[ qu\nan t-\nph ]\nContents"
        },
        {
            "heading": "1 Introduction 3",
            "text": "1.1 Set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Proof overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.4.1 Bounding f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.4.2 Bounding the far from uniform probability . . . . . . . . . . . . . . . . . . 10\n1.5 Discussion and future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
        },
        {
            "heading": "2 Notation and preliminaries 12",
            "text": "2.1 Statistical query learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2 Random quantum circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3 Unitary designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15"
        },
        {
            "heading": "3 Haar random unitaries 16",
            "text": "3.1 Maximally distinguishable fraction . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.2 Far from uniform probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "4 Random quantum circuits of linear depth 20",
            "text": "4.1 Maximally distinguishable fraction . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2 Far from uniformity via unitary designs . . . . . . . . . . . . . . . . . . . . . . . . 22"
        },
        {
            "heading": "5 Random quantum circuits of sub-linear depth 27",
            "text": "5.1 Maximally distinguishable fraction via restricted depth moments . . . . . . . . . . 27 5.2 Far from uniformity for constant-depth circuits . . . . . . . . . . . . . . . . . . . 28"
        },
        {
            "heading": "A Omitted proofs for Haar random unitaries 29",
            "text": "A.1 Haar random state averages via Gaussian integration . . . . . . . . . . . . . . . . 29 A.2 Lipschitz constants for function evaluations and TV distances . . . . . . . . . . . 35"
        },
        {
            "heading": "B Unitary designs 37",
            "text": ""
        },
        {
            "heading": "C Moment calculations 39",
            "text": "C.1 Haar moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 C.2 Restricted depth moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40"
        },
        {
            "heading": "D Random Cli ord unitaries 41",
            "text": ""
        },
        {
            "heading": "E Deterministic algorithms 42",
            "text": ""
        },
        {
            "heading": "F Quantum and probabilistic algorithms 46",
            "text": "G Far from any xed distribution 49"
        },
        {
            "heading": "1 Introduction",
            "text": "Quantum circuits are of central importance in quantum computing and serve as a discrete toy model for the physical world. Understanding the intrinsic properties of quantum circuits is thus of fundamental interest. One such property is quantum circuit complexity, which corresponds to the minimum number of elementary gates necessary to implement a given quantum circuit. Another such property is the complexity of simulating quantum circuits, which is the basis for many quantum advantage proposals. There, one asks what computational resources are required for sampling from the output distribution of a given quantum circuit when applied to a xed input product state and when measured in the computational basis. In this work, we take the perspective of learning theory, and study the complexity of learning the output distribution of quantum circuits. At a high level, this amounts to the resources required to reproduce samples according to the output distribution of a quantum circuit when given black-box access to the corresponding output distribution.\nMore speci cally, we study the average case complexity of learning the output distributions of quantum circuits. This amounts to the cost of learning when the quantum circuit is drawn randomly according to some measure. We note that in the setting of quantum circuit complexity the average-case setting has been the subject of intense work, due to connections between randomly drawn quantum circuits and holographic models in high-energy physics [BCH+21]. Similarly, the established average case hardness of classically simulating quantum circuits has been fundamental to proposals for quantum advantage [HE22]. In our setting, we ask the following question:\nWhat is the complexity of learning generic quantum circuit output distributions?\nWe answer this question within the statistical query (SQ) framework by proving lower bounds on the query complexity required to learn only a fraction of the output distributions of random quantum circuits. Apart from being a natural question from the perspective of learning theory, it also has a physical interpretation. On a high level, the learning problem we consider corresponds to the setting where an observer living in a world governed by quantum physics is to learn a model of its environment with respect to the outcomes of measurements in a xed basis. We note that this contrasts the setting often considered in quantum learning theory where the learner is granted access to the quantum state on which measurements are made [AW17].\nOur results also have implications in the context of quantum machine learning. Firstly, many quantum machine learning proposals today operate within the setting of classical data, and therefore such a restriction is motivated not only from a theoretical but also from a practical perspective. Secondly, the output distributions of parameterized local quantum circuits are often used as a model class for probabilistic modelling, and are referred to as quantum circuit Born machines (QCBMs). Understanding the extent to which learning algorithms for QCBMs may o er advantages over purely classical approaches, such as those based on deep neural networks, is currently the subject of much interest [HIN+22; CMD+20]. By investigating the average-case learnability of the model class of QCBMs, our work can be seen as a rigorous rst step towards characterizing the limitations of QCBM based algorithms.\nAdditionally, a core technical ingredient of our results has rami cations for quantum advantage proposals based on sampling from the output distributions of random quantum circuits. We\nprove that with overwhelming probability, the output distribution of a random quantum circuit will have at least some constant total-variation distance from the uniform distribution. This resembles a conjecture by Aaronson and Chen [AC17] where they conject the same statement with di erent constants. They required and proved a weaker form of this statement for establishing the complexity theoretic foundations of quantum advantage proposals. We provide more details on the connections between our work and the variety of related works in Section 1.3."
        },
        {
            "heading": "1.1 Set-up",
            "text": "General framework: We say that a class  of distributions can be learned by an algorithm  if, when given access to any P \u2208 , the algorithm returns a description of some close distribution Q. In particular, for an accuracy > 0 we say that  -learns P if it returns a description of a Q which is -close in total variation distance. An algorithm is said to be a statistical query algorithm if it has access to P only via approximate expectation values instead of individual samples, where the approximation is promised to be within a tolerance . This is not only a handy restriction on the algorithm that makes analysis simpler, but also practically inspired since most heuristic algorithms are of this form [FGR+17]. In particular, for = \u03a9(1/poly(n)) the statistical query oracle can be simulated from polynomially many samples. Formal de nitions are given in Section 2.\nAverage case complexity: The average case complexity of an algorithm characterizes the cost to achieve a certain probability of success with respect to a measure over the instances1. The (deterministic) average case query complexity with respect to and corresponds to the minimal number of queries any algorithm must make in order to have at least a success probability of with respect to P \u223c . The randomized average case query complexity is de ned in the same manner, though introducing another parameter capturing the success probability over the internal (classical or quantum) randomness of the algorithm.\nQuantum and probabilistic algorithms: As explained in more detail in Appendix F the bounds for deterministic algorithms apply almost exactly to random, i.e., probabilistic or quantum, algorithms. In particular, the randomized average case complexity for -learning is lower bounded by the deterministic average case complexity of -learning up to a prefactor of 2( \u2212 1/2) (c.f. Lemma 1 in comparison with Theorem 34), where we denote by the success probability with respect to the internal randomness of the algorithm. Thus, for the sake of ease of presentation, throughout this work we focus on the deterministic case. We refer to Appendix F for the extension to probabilistic and quantum algorithms.\nRandomquantum circuits: The distribution class  we consider is given by QCBMs, the set of Born distributions corresponding to brickwork quantum circuits at depth d and with gates from a gate set . In particular, we consider distributions of the form\nPU (x) = | \u27e8x |U |0n\u27e9|2 , (1)\nwith U being some unitary brickwork circuit composed of gates from the gate-set . While our 1Thus, we work in the Monte Carlo framework of random algorithms. One can likewise characterize the average case complexity by the expectation of success, which then corresponds to the Las Vegas framework. The Monte Carlo framework is more general in our case as the task we consider can in general not be veri ed e ciently.\nmain focus is on  = U(4), the set of unitary two qubit gates, our techniques will carry over and give similar results for discrete approximations of U(4).\nAverage case bounds and their interpretation depend on the choice of the underlying measure. In this work, we consider the measure over distributions PU that is induced by sampling a random quantum circuit U . This is, each gate is sampled individually from the the uniform measure over the gate set . For  = U(4) this measure is well studied and known as the unitary Haar measure. One practical aspect of this measure is that it corresponds to a natural notion of a generic distribution that would potentially be sampled in the lab by individually sampling each local gate. Other interesting measures would be the uniform distribution over the actual distribution class . However, in case of  = U(4), this class is continuous. Hence, de ning the uniform measure would require a suitable discretization, e.g., by means of an -net. On the other hand, as we will show, the measure induced by random quantum circuits will be bias free in the sense that for any distribution P \u2208 , the probability of sampling some distribution close to P is exponentially small. Thus, similar to the uniform distribution, the measure we consider does not introduce any bias towards any distribution."
        },
        {
            "heading": "1.2 Our results",
            "text": "The main contribution of this work consists in characterizing the average-case complexity of learning the output-distributions of random quantum circuits for di erent circuit depths. Here we provide an informal overview of these results.\nOur main focus is on the scaling of the average case complexity with respect to both circuit depth d and the success probability over the randomly drawn instance :\n1. Circuit depth d : As the circuit depth increases, the expressivity of the set of distributions we consider increases as well. At d = 1 all output distributions of local quantum circuit are product distributions and hence, can be learned trivially. At in nite depth, in contrast, local quantum circuit output distributions can represent any distribution. Thus, they are not learnable in the worst case. As such, the complexity of learning must scale with the circuit depth. We we are interested in understanding this dependence.\n2. Probability over instances : Intuitively the larger , the harder we expect the averagecase learning task to be. Speci cally, setting = 1 recovers the worst-case setting, where we require the learning algorithm to succeed on all instances. Setting < 1 implies that we only require the learning algorithm to succeed on a fraction of instances, which makes the average-case learning problem easier. In this work we investigate how small can be made, while still guaranteeing hardness of average-case learning.\nIn addition to the dependency of average-case query-complexity on d and , we also investigate the dependence on both the tolerance of the statistical queries and the desired accuracy . For ease of presentation, in the informal results below we suppress the dependencies on and and focus on the case where = \u03a9(1/poly(n)) and is a su ciently small constant. We refer to the formal statements for details.\nFinally, we stress again that while the results given below are stated for deterministic algorithms, they immediately translate to both probabilistic and quantum algorithms. This correspondence\nis sketched in Section 1.1 while the details can be found in Appendix F. With this in mind, we state the main results of this work as follows:\nInformal Theorem 1: Let small and n large enough and let = \u03a9(1/poly(n)). Let  be an algorithm for -learning the output distributions of brickwork random quantum circuits of depth d from q many -accurate statistical queries. Then it holds\n1. In nite depth:When d \u2192 \u221e, q = 22\u03a9(n) queries are necessary for any > 2 exp(\u22122n\u22122/9 3) = 2\u22122\u03a9(n) (c.f. Theorem 2).\n2. Linear depth: There is a d \u2032 = O(n) such that for any d \u2265 d \u2032, q = \u03a9(2n) queries are necessary for any > 3200 \u22c5 2\u2212n = O(2\u2212n) (c.f. Theorem 6).\n3. Sublinear depth: for any c log n \u2264 d \u2264 c(n+log n), q = 2\u03a9(d) = 2!(log(n)) queries are necessary for any > 4/5 + + = O(1), where c = 1/ log(5/4) (c.f. Theorem 12).\nIn particular, we nd that the average case problem at superlogarithmic depth is hard with constant probability over the random instance. Moreover, at linear depth we nd hardness with probability exponentially close to one. At in nite depth, the problem becomes hard with probability double exponentially close to one.\nAs a side result we show that the output distribution of a random quantum circuit is, with overwhelming probability, at least constantly far in total variation distance from any xed distribution. This resolves a variant of Aaronson and Chen\u2019s [AC17, Conjecture 1], made with the goal of clarifying the hardness of verifying random circuit sampling procedures. As such we believe this auxiliary result to be of independent interest.\nInformal Theorem 2: (Informal version of Theorem 36) There exists some d \u2032 = O(n) such that for any depth d \u2265 d \u2032, for any \u2264 1/450 and for any distribution Q we have\nPr U\u223c\n[dTV(PU , Q) > ] \u2265 1 \u2212 O (2\u2212n) , (2)\nwhere is the measure induced by random brickwork quantum circuits."
        },
        {
            "heading": "1.3 Related work",
            "text": "Our work touches on and combines a variety of well studied elds. In order to provide further context and motivation, we provide below a discussion of relevant related work.\nStatistical queries: We work in the statistical query (SQ) framework which was introduced by Kearns [Kea93] as a restriction of Valiants theory of learning [Val84]. Kearns original motivation was the intrinsic robustness of SQ learners with respect to random classi cation noise. However, the SQ model is also highly relevant in the context of statistical problems [FGR+17], which includes distribution learning, as originally formulated by Kearns et al. [KMR+94]. In this context, SQ algorithms are those which only have access to coarse statistical properties of the data generating distribution. While this is a restriction of the oracle access it turns out, with the famous\nexception of Gaussian elimination, that almost all known learning algorithms can be recast as SQ algorithm [Kea93; Fel17].\nThe SQ framework is particularly interesting in the context of variational quantum machine learning, such as QCBM based algorithms. To see this we note that all current methods for optimizing the QCBM\u2019s parameters use noisy evaluations of gradients, or gradient-like quantities, along individual directions. Examples for this include stochastic gradient descent via the parameter shift rule [MNK+18; SBG+19] and simultaneous perturbations and stochastic approximation [Spa98]. Thus, the update in QCBM based algorithms can be directly implemented via statistical queries.\nEquivalences to other statistical oracles, such as the \u201chonest SQ\u201d oracle and the statistical query oracle with respect to Bernoulli noise are worked out in [FGR+17; Fel17]. Interestingly, SQ learning has been shown to be equivalent to learning with restricted memory [SVW16; FPV18], as well as di erentially private learning [DMN+06; KLN+11]. Evolutionary algorithms can be recast as SQ algorithms and in fact were shown to be equivalent to \u201ccorrelational statistical query\u201d (CSQ) algorithms [Val09; Fel08]. A particularly nice feature of the SQ framework is that it allows for unconditional lower bounds. As such, SQ lower bounds are often taken as evidence for computational hardness if the underlying problem does not admit a linear structure, as is the case for parities. Additionally, many statistical query lower bounds asymptotically match complexity theoretic upper bounds, such as learning DNFs [BFJ+94], learning mixtures of Gaussians [DKS17] and the planted clique problem [FGR+17]. Other results via the SQ framework contain positive results for k-means clustering, principle component analysis and the perceptron algorithm [BDM+05] and manifold estimation [AK21], negative results for learning simple neural networks [CLL22], average case hardness for learning neural networks at super-logarithmic depth [AAK21], as well as lower and upper bounds for optimization and distributional search problems [FGV16; FGR+17; FPV18].\nComplexity of quantum circuits and computational learning theory: A circuit class is a collection of quantum circuits. A well-established way to assess the complexity of such a circuit class from the viewpoint of classical computing is to study the resources required to classically simulate the circuit class. Examples of circuit classes that have been studied in this regard include Cli ord circuits, Cli ord+T circuits, matchgate circuits and IQP circuits [Got98; AG04; Val12; TD02; BJS11; BMS17]. Note that in all these examples the description of the circuit class includes a speci cation of the input state and a xed measurement basis as the simulatability may crucially depend on these choices. In this work, we analogously assess the complexity of a quantum circuit class from the viewpoint of computational learning theory. Indeed, a long line of research has been aimed at characterizing the complexity of learning classical circuit classes, in various learning models [LMN93; Kha93; KMR+94; AGS21]. This complexity is indeed a fundamental property of such circuit classes. Our work provides insight into this fundamental property of quantum circuits. We stress here that, analogous to the case of classical simulation, we consider learning with respect to a xed input state and xed measurement basis. In that sense, our learning model di ers from those employed in other works on learning quantum states [Aar07; Mon17] as we require learning the action of the circuit only with respects to measurements in a xed product basis.\nHeuristic algorithms for distribution learning: Recent years have seen major advances in the\ndevelopment of heuristic neural-network based methods for probabilistic modelling. Generative adversarial networks [GPM+14] and generative transformer models [BMR+20] have managed to achieve impressive results ranging from predicting protein structure to atomic accuracy [JEP+21] to achieving human-level language comprehension [HBM+22]. Learning the underlying model classes is known to be worst case hard [CLL22]. This inspired a series of works in order to better understand these successes from a theoretical perspective (see for example [ABG+; LSS14; CHM+15; JSA16; Dan17; Sha17; AS20; DV20; AAK21]). Our results can be seen in a similar light for QCBM based algorithms. In particular, our average case hardness results for learning super logarithmic depth quantum circuit distributions is the QCBM equivalent to [AAK21, Contribution 3].\n\u201cFar fromuniform\u201d property and quantum advantage In [AC17], the authors propose heavy output generation as a particularly natural task for separating classical and quantum computers. More precisely, quantum computers can be used to output sets of bit strings z1,\u2026 , zk such that more than 2/3 of them have probability larger than the median of the output probabilities. This can be achieved by a subroutine, which uses a conditional probability distribution that samples instances of random quantum circuits as long as necessary for a su ciently non-uniform probability distribution to appear.\nA key result is that far-from-uniform output distributions are not rare for random quantum circuits. We adapted the resulting bound as Corollary 16. Aaronson and Chen moreover conjectured that this far from uniform conjecture not only holds with constant probability but with probability exponentially close to one. This would imply that it su ces to directly sample from a random quantum circuit instance without the subroutine. While we prove that a far from uniform property holds with exponential probability in Theorem 9, the constants in our result do not imply Conjecture 1 in [AC17]. At the same time, it is possible to de ne a weaker version of heavy output generation, for which our bounds su ce.\nFrom average-case complexity in learning to cryptography: There is a rich correspondence between computational learning theory and cryptography. On the one hand, cryptographic assumptions are often used to prove conditional lower bounds for learning problems [KV94]. On the other hand, the assumed hardness of learning problems can sometimes be used for the construction of cryptographic primitives. For this latter direction, it is well known that the existence of cryptographic primitives such as one-way functions require the existence of learning problems which are average-case hard [IL90; Bar17]. As concrete examples, Blum, Furst, Kearns and Lipton [BFK+94] have shown that an e cient average case learner for polynomial size circuits in the distribution speci c PAC model, would imply the non-existence of one-way functions. This result has been recently extended by Nanashima [Nan21] to give a characterization of auxillary-input one way functions, based on the hardness of PAC learning polynomial size circuits in a modi ed average-case variant of PAC learning. In light of these results, it is natural to ask whether one can characterize either classical or quantum cryptographic primitives in terms of the complexity of average-case learners for quantum circuit output distributions in the distribution learning setting. While we do not address this question in this work, and while our restriction to the SQ model is a major restriction in this regard, we believe that the connection to cryptography merits further study and hope that the insights gained from our results can be helpful in this regard."
        },
        {
            "heading": "1.4 Proof overview",
            "text": "For ease of presentation, we use the notation P[ ] to denote Ex\u223cP [ (x)] and denote by  the uniform distribution. The starting point of all our results is a lower bound on the average case query complexity in terms of properties of the measure . Suppose there is a deterministic algorithm  that -learns a fraction of  with respect to from q many -accurate statistical queries. Then, it holds (c.f. Lemma 1)\nq + 1 \u2265 \u2212 PrP\u223c [dTV(P, ) \u2264 + ] max PrP\u223c [|P[ ] \u2212 [ ]| > ] , (3)\nwhere the max is over all bounded functions | (x)| \u2264 1.\nThe above bound is obtained by rst reducing a suitable worst-case uniformity test to the averagecase learning problem, and then lower bounding the complexity of the uniformity test. Given some set of distributions \u0303 \u2286 , we consider the decision problem of testing \u0303 versus  de ned via:\nGiven statistical query access to some P \u2208 \u0303 \u222a { } decide whether \u201cP =  \u201d or \u201cP \u2208 \u0303\u201d.\nWe then note:\n1. An average-case learner for  which succeeds on a fraction of instances with respect to , implies the existence of a worst-case learning algorithm for some \u2032 \u2286  with (\u2032) = . This trivially implies a worst-case learning algorithm for \u0303 = \u2032/B + ( ).\n2. A worst-case learning algorithm for \u0303 using at most q queries implies an algorithm for deciding \u0303 versus  using q + 1 queries.\nAs such, it is su cient for us to lower bound the complexity the uniformity test. To this end, we use a counting argument to show that for any measure over \u0303 it holds that the number of queries necessary to decide \u0303 versus  satis es\nq \u2265 (max PrP\u223c [|P[ ] \u2212 [ ]| > ])\n\u22121\n. (4)\nWe then obtain Equation (3) by considering the measure de ned by conditioning on \u0303.\nGiven this, from Equation (3) it is clear that in our context, in order to obtain the desired lower bounds we require:\n\u2022 An upper bound on the maximal fraction distinguishable from uniform f = max PrU [|PU [ i]\u2212  [ i]| \u2265 ].\n\u2022 An upper bound on the mass of the -ball around the reference distribution. The complement of this probability is often referred to as the probability of being far from uniform.\nIn this work we give bounds on both quantities for random quantum circuits of various depths. We begin in the limit of in nitely deep circuits, and thus of Haar-random unitaries and then partially derandomize our results using concentration inequalities based on higher moments of the\nHaar measure. This will allow us to prove results about random quantum circuits of comparably low depth, which are far from the Haar measure but quickly generate the same moments."
        },
        {
            "heading": "1.4.1 Bounding f",
            "text": "For Haar random unitaries, the concentration of measure phenomenon in the form of Levy\u2019s lemma produces tight bounds on tails, which implies bounds on f.\nFor linear depth circuits we can use Chebyshev\u2019s inequality in order to bound f\nmax Pr U [|PU [ i] \u2212 [ i]| \u2265 ] \u2264 VarU [PU [ ]] 2\n(5)\nwhere we have used EU [PU [ ]] =  [ ] since for any depth d \u2265 1 is a 1-design. The variance is a second moment and can be exactly computed for the Haar measure via standard symmetry arguments or from the Weingarten calculus.\nFor sublinear circuits, the distribution over unitaries does not form a unitary design, however, we can still bound the second moments involved in the variance by adapting the statistical physics mapping from [Hun19] in a similar way as in [BCG21]. Many of our bounds will depend on such moment bounds over the Haar measure that we detail in Appendix C."
        },
        {
            "heading": "1.4.2 Bounding the far from uniform probability",
            "text": "To obtain a lower bound on the far from uniform probability (or equivalently an upper bound on the probability mass of an -ball around the uniform distribution), we start by writing the total variation distance in terms of the 1-norm.\nIn the in nite circuit depth regime we make use of Gaussian integration in order to obtain upper and lower bounds on the expected distance between a random output distribution PU and the uniform distribution. Then we use Levy\u2019s Lemma once again to obtain a bound on the probability of PU being far from the uniform distribution.\nIn the linear depth regime we apply a variant of Berger\u2019s inequality for p-norms, c.f. Lemma 8, in order to obtain\nPr U [\u2016PU \u2212 \u20161 < 2 ] \u2264 PrU [ \u2016PU \u2212 \u201632 \u2016PU \u2212 \u201624 < 2 ] . (6)\nTreating the numerator and denominator separately we can thus apply the union bound and obtain\nPr U [\u2016PU \u2212 \u20161 < 2 ] \u2264 PrU [\u2016PU \u2212 \u2016 3 2 < 2 1] + PrU [\u2016PU \u2212 \u2016 2 4 > 2 2] , (7)\nfor suitable 1 2 \u2265 . Both terms can be bounded separately with the same strategy. Using Chebyshev\u2019s inequality for the random variable X (p, q) = \u2016PU \u2212 \u2016qp we can bound the deviation of X (p, q) from its mean E [X (p, q)]. In particular, due to the variance term in Chebyshev\u2019s inequality we nd that an (approximate) 2p-design is su cient for an exponential concentration of X (p, q).\nSince E[X (2, 3)] is exponentially small it is thus crucial, that X (4, 2) itself is su ciently small and sharply concentrated such that we can nd 1 and 2 that cancel to a constant. Again, we con rm this ingredient via a variance bound provided that our measure is induced by an 8-design. We conclude from [BHH16; Haf22] that random linear depth Born distributions are far from uniform.\nIn the sublinear depth regime we use a result by Aaronson and Chen [AC17] which lower bounds the expected distance between a randomly drawn PU and the uniform distribution even for d = 1. We then use Markov\u2019s inequality to translate this to a bound the probability of PU being far from the uniform distribution."
        },
        {
            "heading": "1.5 Discussion and future work",
            "text": "In this work we give lower bounds for the average case query complexity of learning the output distributions of random quantum circuits in di erent depth regimes. In particular, we show that the problem of learning the output distribution of random quantum circuits is hard with constant probability over the instance already at super logarithmic depth. Moreover, we prove that the problem becomes hard with probability exponentially close to one over the instance at linear depth. Our analysis is accompanied by the corresponding results for both Haar random unitary output distributions and Haar random Cli ord output distributions. While the former gives hardness with probability doubly exponentially close to 1, the latter only gives hardness with a constant probability over the instance.\nThere are multiple natural avenues to continue this work:\n1. While we prove a strong asymptotic average-case complexity bound for linear depth circuits, the explicit depth at which our bounds apply comes with a rather large prefactor (d = 1020n). This prefactor is likely an artefact of the proof techniques in [BHH16; Haf22], which bound the depth at which unitary designs are well-approximated. There are at least two promising approaches to lowering this constant and consequently making our bound more directly applicable to a practial regime. One could directly compute the moments using the statistical physics mapping from [Hun19]. Alternatively, extensive numerical calculations of nite-size spectral gaps in combination with Knabe bounds might lower the explicit depth at which unitary designs are generated [HH21]. While these calculations are beyond the scope of this paper, we believe that it would be worthwhile to address this issue.\n2. Moreover, when relaxing the assumption on from constant to inverse polynomial and on from inverse exponential to inverse polynomial, thus making the learning task harder, one can use Markov\u2019s inequality instead of Chebyshev\u2019s in the far from uniform proof (c.f. Theorem 10). This already improves the asymptotics to hold at d = 690n. This might simplify the corresponding statistical physics mapping since it makes use of fourth instead of eighth moments. We expect that such a calculation implies average-case hardness with probability 1 \u2212 o(1) over the instances for any depth d = !(log(n)).\n3. In this work, we rule out e cient algorithms at super logarithmic depth. It is thus natural to ask what happens at logarithmic depth and below. For example, are the output distributions of constant depth quantum circuits e cient to learn?\n4. Last, we emphasize that all our results hold in the statistical query model. Another prominent model is learning from samples. With the famous exception of parities, most known tight upper bounds for learning can be realized in the statistical query model. We ask whether our average-case complexity results carry over to average-case hardness of learning from samples."
        },
        {
            "heading": "2 Notation and preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Statistical query learning",
            "text": "Let  be a class of distributions over a domain X . For two distributions P, Q \u2208  we denote by dTV(P, Q) \u2236= 12 \u2211x\u2208X |P (x) \u2212 Q(x)| the total variation distance between them. The open -ball B (P ) around any distribution P over the domain X is given by the set of all distributions Q over X such that dTV(P, Q) < . For a distribution P over X and a function \u2236 X \u2192 [\u22121, 1] we use the short hand notation\nP[ ] \u2236= E x\u223cP [ (x)] (8)\nto refer to the expectation value of with respect to P . We denote by X the set of distributions over X and by n the set of all distributions over the domain {0, 1}n. The uniform distribution is denoted by  .\nA well studied model in learning theory is the statistical query learning model. Here we assume that the learner has access to expectation values of functions with respect to the underlying probability distribution. This can be formalized by considering access to a statistical query oracle.\nDe nition 1: (Statistical query oracle) For > 0 and a distribution P over X we denote by Stat (P ) the statistical query (SQ) oracle of P with tolerance . When queried with some function \u2236 X \u2192 [\u22121, 1] the oracle returns some v such that |v \u2212 P[ ]| \u2264 .\nRemark 1: On immediate consequence of De nition 1 which is useful for the interpretation of our formal result statements is as follows: For any < , any SQ oracle Stat (P ) is also a valid SQ oracle Stat (P ). Thus, lower bounds for SQ algorithms with respect to trivially imply the same lower bound for SQ algorithms with respect to .\nA prominent special case is to consider to be lower bounded by an inverse polynomial, since this re ects the scenario where the statistical query oracle can be run e ciently with a polynomial number of samples. Up to polynomial corrections, the complexity of the oracle is then given by the complexity of computing the query function .\nIn order to learn a distribution it is crucial to x the representation of the distribution to be learned. Here, a representation can be thought of as an algorithm that speci es the distribution. For the sake of clarity typical representations are generators and evaluators.\n\u2022 A generator of a probability distribution P is a probabilistic algorithm that produces samples\naccording to x \u223c P .\n\u2022 An evaluator of a probability distribution P \u2208 n is an algorithm EvalP \u2236 {0, 1}n \u2192 [0, 1] that outputs the probability amplitude EvalP [x] = P (x).\nWe will refer to a representation (e.g. a generator or an evaluator) of a distribution Q as an -approximate representation of P if dTV(P, Q) < . We would like to stress that, due to fundamental limitations, our results hold for any -approximate representation even if we allow the corresponding algorithms to be computationally ine cient.\nDistribution learning in the statistical query framework is made formal by the following de nition.\nProblem 1: ( -learning of  from statistical queries) Let \u2208 (0, 1) be an accuracy parameter, \u2208 (0, 1) be the tolerance and let  be a distribution class. For a xed representation of the distribution, the task of -learning  from statistical queries with tolerance is de ned as given access to Stat for any unknown P \u2208 , output an -representation of P .\nIn [HIN+22] the authors have studied the worst case query complexity of Problem 1 for  being the class of output distributions of local quantum circuits. Here we want to consider the average case query complexity for the same distribution class. This is a strictly easier task as we do not require the learner to succeed for each and every distribution in the class. Rather, we want to characterize the number of statistical queries needed to succeed in solving Problem 1 on a fraction of distributions in  with respect to a measure over the distributions in . Similarly, when one considers a quantum or probabilistic learner one can ask about the number of statistical queries needed to succeed in the aforementioned task with some xed probability with respect to the algorithm\u2019s randomness. Thus, the randomized average case complexity is de ned with respect to a measure and the two parameters and . By we denote the success probability of the algorithm and by the size, with respect to , of the fraction on which the algorithm is successful.\nDe nition 2: (Average case complexity) Let be a class of distributions, a probability measure over  and , \u2208 (0, 1). The deterministic average case query complexity of Problem 1 is de ned as the minimal number q of queries any learning algorithm  must make in order to achieve\nPr P\u223c [\u201c Stat(P ) -learns P from q queries\u201d] \u2265 . (9)\nLikewise, the randomized average case query complexity is de ned as the minimal number q of queries any random learning algorithm  must make in order to achieve\nPr P\u223c [Pr [\u201c Stat(P ) -learns P from q queries\u201d] \u2265 ] \u2265 , (10)\nwhere Pr denotes the probability over the internal randomness of .\nThe deterministic and randomized average case complexities in the framework of statistical query learning are closely related. As advertised in Section 1.1, one can directly translate our lower\nbounds for deterministic learning to lower bounds of randomized learning by means of a global prefactor 2( \u2212 1/2). Thus, for the sake of ease we focus on the deterministic average case complexity throughout the main text and refer to Appendix F for the details about random algorithms.\nAs discussed in Section 1.4 the average case query complexity of deterministic algorithms for learning in the SQ framework can now be lower bounded as follows.\nLemma 1: (Deterministic average case complexity) Suppose there is a deterministic algorithm that -learns a fraction of with respect to from q many -accurate statistical queries. Then for any Q it holds\nq + 1 \u2265 \u2212 PrP\u223c [dTV(P, Q) \u2264 + ] max PrP\u223c [|P[ ] \u2212 Q[ ]| > ] , (11)\nwhere again, the max is over all functions \u2236 X \u2192 [\u22121, 1].\nWe refer to Appendix E for the proof of Lemma 1. To provide a simpli ed expression we make the following remark.\nRemark 2: Note that without loss of generality we can take \u2264 which leads to the bound\nq + 1 \u2265 \u2212 PrP\u223c [dTV(P, Q) \u2264 2 ]\nmax PrP\u223c [|P[ ] \u2212 Q[ ]| > ] . (12)\nTo see why we can do so, consider instead the case > . Given P, Q such that > dTV(P, Q) > we can see that these distributions are indistinguishable with respect to -accurate queries and thus there cannot exist an -learner.\nFrom Lemma 1 it is clear that a crucial gure of merit is the fraction of distributions that can be distinguished from a single query. Following [Fel17] we de ne.\nDe nition 3: (Maximally distinguishable fraction) Let  be a distribution class over the domain X and let be some probability measure over . The maximally distinguishable fraction with tolerance parameter and with respect to the measure and the reference distribution Q is de ned as\nfrac( , Q, ) \u2236= max Pr P\u223c [|P[ ] \u2212 Q[ ]| > ] , (13)\nwhere the maximum is over all functions \u2236 X \u2192 [\u22121, 1].\nIn the special case that the reference distribution is the uniform distribution, as is the case in the remainder of this paper,  we will refer to this by the short hand\nf = frac( , , ) , (14)\nwhere the measure and the tolerance will be clear from context.\nSummary: A lower bound for average case query complexity for learning in the statistical query model is determined by:\n\u2022 The size of the + -ball of any xed reference distribution PrP\u223c [dTV(P, Q) \u2264 + ].\n\u2022 The maximally distinguishable fraction with respect to the same reference distribution frac( , Q, ).\nNote 1: For the sake of ease of presentation we give the derivation of bounds on the weights of the ball around the reference distribution in terms of . We then translate the corresponding result to Lemma 1 substituting by + ."
        },
        {
            "heading": "2.2 Random quantum circuits",
            "text": "Given some n-qubit unitary U \u2208 U(2n), we denote by PU (x) = | \u27e8x |U |0n\u27e9|2 the quantum circuit output, or Born distribution. We denote by U the unitary Haar measure, or simply the uniform measure, over U(D). Similarly, we denote by S the spherical Haar measure, or likewise the uniform measure, over the complex unit sphere SD\u22121, where in both cases the dimensionality D will be clear from context.\nDe nition 4: (Brickwork architecture) An n-qubit brickwork quantum circuit of depth d (with periodic boundary conditions) is a quantum circuit that is of the form\nU = (U (d)2,3 \u2297\u22ef \u2297 U (d) n,1 ) \u22c5 (U (d\u22121) 1,2 \u2297\u22ef \u2297 U (d\u22121) n\u22121,n )\u22ef (U (2) 2,3 \u2297\u22ef \u2297 U (2) n,1 ) \u22c5 (U (1) 1,2 \u2297\u22ef \u2297 U (1) n\u22121,n) (15)\nwhere U (k)i,j \u2208 U(4) is the unitary in the k\u2019th layer acting on neighboring qubits i and j. For the sake of ease we have assumed d and n to be even.\nWhile we give de nitions and analysis only for periodic boundary conditions, we note that all our results will carry over to open boundary conditions at the price of slightly worse prefactors.\nDe nition 5: (Random quantum circuits) A random brickwork quantum circuit of depth d on n qubits is formed by drawing \u230an/2\u230b \u22c5 d many 2-qubit unitaries U (k)i,j i.i.d. Haar randomly and contracting them. We denote the resulting probability distribution on U(2n) by C , where n and d will be clear from context.\nGiven a two-qubit gate set  \u2286 U(4). We denote by (n, d) the set of Born distributions which can be realized by brickwork quantum circuits on n qubits of depth d ."
        },
        {
            "heading": "2.3 Unitary designs",
            "text": "Unitaries generated by random quantum circuits quickly mimic Haar random unitaries for many practical purposes. The reason for this is that they generate unitary t-designs. These are \"evenly\"\nspread probability distributions over the unitary group that have the same t\u2019th moments as the Haar measure [Dan05; GAE07]. This is often expressed in terms of t-fold twirls: Let be a probability measure on the unitary group U(D). Then we de ne for any matrix A \u2208 CDt\u00d7Dt .\n\u03a6(t)( )(A) \u2236= \u222b U \u2297tA(U \u2020)\u2297td (U ) . (16)\nWe call an approximate unitary t-design if, for U being the Haar measure.\n\u03a6(t)( ) \u2248 \u03a6(t)( U ) . (17)\nWe provide a detailed de nition in Appendix B. Moreover, see Appendix B for the relation to state designs and bounds on the generation of approximate designs by random quantum circuits."
        },
        {
            "heading": "3 Haar random unitaries",
            "text": "In this section, we bound the two key quantities, namely f and the far from uniform probability, for random quantum circuits of in nite depth, corresponding to Haar random unitaries. Plugging these bounds into Lemma 1, we obtain the following lower bound on the average case query complexity.\nTheorem 2: (Formal version of in nite depth part of Informal Theorem 1) Let > 0, \u2264 1/e \u2212 2\u2212n/2\u22121 \u2212 and set = 1/e \u2212 2\u2212n/2\u22121 \u2212 \u2212 . Any algorithm that succeeds in -learning a fraction of the output distributions of in nitely deep random brickwork quantum circuits requires q many -accurate statistical queries, with\nq + 1 \u2265 \u2212 2 exp(\u2212\n2n+2 2 9 3 )\n2 exp (\u2212 2 n 2 9 3 )\n. (18)\nRemark 3: Note that, for any\n\u2265 2\u2212n/4 and any \u2264 1 e \u2212 2\u2212n/2\u22121 \u2212 2\u2212n/4+2\nwhich corresponds to \u2265 2\u2212n/4+2, we nd by Theorem 2 the query complexity for learning any fraction > 2 exp(\u22122n/2+4/9 3) = 2\u22122\n\u03a9(n) requires q = 22\u03a9(n) many queries. In words: learning a doubly exponentially small fraction takes doubly exponentially many, inverse exponentially accurate statistical queries.\nIn the case of in nitely deep circuits, it is known that the distribution of circuit unitaries converges to the unique, rotationally invariant Haar measure on the full unitary group in D = 2n dimensions U . If we apply such a Haar-random unitary to a designated (arbitrary) pure starting state, say | 0\u27e9 = |0,\u2026 , 0\u27e9, we obtain a pure state | \u27e9 that is sampled from the spherical Haar\nmeasure S , i.e. uniformly from the set of all pure states in D dimensions:\n| \u27e9 = U | 0\u27e9 unif\u223c { |u\u27e9 \u2208 CD \u2236 \u27e8u|u\u27e9 = 1 } \u2282 CD (and D = 2n for n qubits).\nSuch (Haar) uniform distributions of pure states have two remarkable features: (i) we can use powerful frameworks like Weingarten calculus and Gaussian integration to compute complicated expectation values and (ii) concentration of measure (Levy\u2019s lemma) asserts that concrete realizations concentrate very sharply around this expected behavior. These two features can be combined into a powerful strategy to obtain very sharp bounds on deviation probabilities, like the two essential ingredients in Lemma 1:\nPr U\u223c U [dTV(PU , ) \u2265 ] and Pr U\u223c U [|PU [ ] \u2212 [ ]| > ]\nfor any xed function \u2236 {0, 1}n \u2192 [\u22121, 1]. To apply this formalism, our strategy will be to rst reformulate the arguments in both probabilities as functions in the (Haar) random pure state | \u27e9 = U | 0\u27e9. Then we will use Levy\u2019s lemma to obtain bounds on both probabilities. Let\u2019s focus on this last step as it applies to both probabilities.\nLevy\u2019s lemma asserts that every reasonably well-behaved function concentrates very sharply around its expectation value if we choose a random vector uniformly from a (real- or complexvalued) unit sphere SD\u22121 in D \u226b 1 dimensions. Note that Haar-random state | \u27e9 = U | 0\u27e9 with U \u223c U meet this sampling requirement by de nition. The well-behavedness of functions is measured by their Lipschitz constant. A function f \u2236 SD\u22121 \u2192 R is Lipschitz with respect to the 2-norm in CD with constant L \u2265 0 if\n|f (| \u27e9) \u2212 f (| \u27e9)| \u2264 L \u2016| \u27e9 \u2212 | \u27e9\u20162 for all | \u27e9 , | \u27e9 \u2208 S D\u22121.\nHere is a variant of Levi\u2019s Lemma (concentration of measure) that directly applies to pure quantum states inD dimensions. It readily follows from identifying the complex unit sphere SD\u22121 \u2282 CD with a real-valued unit sphere in 2D dimensions isometric embedding, see, e.g. [BCH+21, proof of Proposition 29].\nTheorem 3: (Levy\u2019s lemma for Haar-random pure states) Let f \u2236 SD\u22121 \u2192 R be a function from D-dimensional pure states to the real numbers that is Lipschitz with Lipschitz constant L. Then,\nPr | \u27e9\u223c S [ |||| f (| \u27e9) \u2212 E | \u27e9\u223c S [f (| \u27e9)] |||| > ] \u2264 2 exp(\u2212 4D 2 9 3L2) for any > 0.\nIn words, Levy\u2019s lemma suppresses the probability of a -deviation from the expectation value. This bound diminishes exponentially in Hilbert space dimensionD = 2n, i.e. doubly exponentially in qubit size n. In the following two sections we will use theorem 3 to obtain bounds on the maximally distinguishable fraction and the probability of being far from uniform. We do so by explicitly upper bounding the Lipschitz constants of the involved functions and computing the Haar expectation values."
        },
        {
            "heading": "3.1 Maximally distinguishable fraction",
            "text": "Let us start with PrU\u223c U [|PU [ ]\u2212 [ ]| > ]. We will use the short-hand notation x = (x1,\u2026 , xn) \u2208 {0, 1}n to enumerate all possible outcome strings of n parallel computational basis measurements:\nPU [ ] = \u2211 x\u2208{0,1}n (x0,\u2026 , xn) |\u27e8x | \u27e9|2 = \u27e8 |( \u2211 x\u2208{0,1}n (x) |x\u27e9\u27e8x | ) | \u27e9 = \u27e8 |\u03a6| \u27e9,\nwhere we have introduced the diagonal matrix \u03a6 = \u2211x (x)|x\u27e9\u27e8x | \u2208 CD\u00d7D whose spectral norm obeys \u2016\u03a6\u2016\u221e = maxx\u2208{0,1}n | (x)| \u2264 1 regardless of the underlying function in question. This is a very simple and highly structured quadratic form in | \u27e9 = U | 0\u27e9. Its expectation value over all Haar-random states produces the uniform distribution  [ ], in formulas\nE U\u223c U [PU [ ]] = \u2211 x\u2208{0,1}n\n1 2n (x) =  [ ] . (19)\nTo see this, note that the uniform average over all pure states in D dimensions produces the maximally mixed state, i.e. E| \u27e9\u223c S [| \u27e9\u27e8 |] = 1/D. Linearity of the expectation value then ensures\nE U\u223c U [PU [ ]] = E | \u27e9\u223c S [\u27e8 |\u03a6| \u27e9] = Tr( E| \u27e9\u223c S [| \u27e9\u27e8 |] \u03a6) = tr (1/D \u03a6) = 1 D tr (\u03a6)\n=\u2211 x\n1 2n (x) =  [ ],\nas claimed. We also provide an alternative derivation based on Gaussian integration in the Appendix (Theorem 18). We can now obtain an explicit bound on the maximally distinguishable fraction by theorem 3.\nTheorem 4: Consider n-qubit Haar-random unitaries U \u223c U (D = 2n) and x a function \u2236 {0, 1}n \u2192 [\u22121, 1]. Then,\nPr U\u223c U [|PU [ ] \u2212 [ ]| > ] \u2264 2 exp(\u2212 2n 2 9 3 ) for any > 0.\nIn words: for each xed function , its evaluation PU [ ] concentrates very sharply doublyexponentially in qubit size n around the uniform average. Hence, it is extremely unlikely to distinguish PU from  with only a single .\nProof. Rewrite PU [ ] = \u27e8 |\u03a6| \u27e9 =\u2236 f\u03a6(| \u27e9) with | \u27e9 = U | 0\u27e9 (Haar random state) and diagonal matrix \u03a6 that obeys \u2016\u03a6\u2016\u221e \u2264 1. This reformulation highlights that the quadratic form function f\u03a6 \u2236 SD\u22121 \u2192 R is Lipschitz with constant L \u2264 2\u2016\u03a6\u2016\u221e \u2264 2, we refer to Lemma 22 in the appendix for a detailed statement and proof. Next, recall from Equation (19) that E| \u27e9\u223c S [f\u03a6(| \u27e9)] = EU\u223c U [PU [ ]] =  [ ] and apply Theorem 3 to deduce the claim."
        },
        {
            "heading": "3.2 Far from uniform probability",
            "text": "Moving now to the quantity PrU\u223c U [dTV(PU , ) \u2265 ], we note that the expectation value required is a bit more involved by comparison. Let us start by reformulating the total variation distance between PU and  as\ndTV(PU , ) = 1 2\n\u2211 x\u2208{0,1}n\n|PU (x) \u2212 (x)| = 1 2\n\u2211 x\u2208{0,1}n |||| |\u27e8x \u2223 U | 0\u27e9|2 \u2212 1 D |||| = 1 2D \u2211 x\u2208{0,1}n ||||D\u27e8x \u2223 \u27e9| 2 \u2212 1||| ,\nwhere D = 2n and | \u27e9 = U | 0\u27e9. Linearity of the expectation value and unitary invariance of the spherical Haar measure then implies\nE U\u223c U [dTV(PU , )] = 1 2D\n\u2211 x\u2208{0,1}n E | \u27e9\u223c S\n[||D |\u27e8x | \u27e9| 2 \u2212 1||] = 1 2 E | \u27e9\u223c S [||D |\u27e80,\u2026 , 0| \u27e9| 2 \u2212 1||] .\nThe expression on the right hand side is not a polynomial in the overlap |\u27e80,\u2026 , 0| \u27e9|2 which prevents us from using Weingarten calculus to compute it. Another averaging technique, known as Gaussian integration, does the job, however. The key idea is to view the uniform expectation over pure states as a uniform integral over all points that are contained in the complex-valued unit sphere in D dimensions. Up to a normalization factor (scaling), this integral can then be re-cast as an expectation value over the directional degrees of freedom in a 2n-dimensional complexvalued Gaussian random vector with independent entries gj + i\u210ej , 1 \u2264 j \u2264 2n and gj , \u210ej\niid\u223c  (0, 1). A detailed argument is provided in the appendix and yields\n1 e \u2212 1 2n/2+1 \u2264 E U\u223c U [dTV(PU , )] \u2264 1 e + 1 2n/2+1 , (20)\nwhere e denotes Euler\u2019s constant. Note that the approximation errors on the left and right decay exponentially in qubit size n. This is the content of Theorem 19 in the appendix and the proof uses a precise version of the approximate identity\n1 2 E | \u27e9\u223c S [|D\u27e80,\u2026 , 0| \u27e9 \u2212 1|] \u2248 1 4 E gj ,\u210ej iid\u223c (0,1) [|||g1 + i\u210e1| 2 \u2212 2||] = 1 4 E g1,\u210e1 iid\u223c (0,1) [||g 2 1 + \u210e 2 1 \u2212 2||]\n= 1 4 \u222c\n\u221e\n\u221e\n||g 2 1 + \u210e 2 1 \u2212 2|| exp (\u2212(g21 + \u210e21)/2) 2 dg1dg2 = 1 e .\nThe nal equality follows from switching into polar coordinates and solving the resulting integral analytically \u2013 this is why the technique is called Gaussian integration. The exponentially small o sets \u00b11/2n/2+1 in Rel. (20) bound the approximation error that is incurred in the rst step of this argument. As before, we can now obtain a bound on the probability of a Haar randomly drawn unitary giving rise to a distribution that is far from uniform by use of Theorem 3.\nTheorem 5: Consider n-qubit Haar-random unitaries U \u223c U (D = 2n). Then, the TV distance\nbetween PU and the uniform distribution  is guaranteed to obey\nPr U\u223c U [ |||| dTV(PU , ) \u2212 1 e |||| \u2265 + 1 2n/2+1 ] \u2264 2 exp(\u2212 2n+2 2 9 3 ) for any > 0.\nIn words: this TV distance concentrates very sharply (doubly-exponentially in qubit size n) around the remarkable value 1/e \u2265 0.367. The exponentially small in n additive correction term 1/2n/2+1 is a consequence of the slight mismatches in Rel. (20). Note, however, that this does not qualitatively change the concentration statement. The mismatch is of the same order as the smallest for which the exponential tail bound still provides meaningful results: 2n+2 2 \u2273 1 requires \u2273 1/2n/2+1.\nProof. The proof is conceptually very similar to the proof of Theorem 4. We rst recast the expectation over Haar-random unitaries U as an expectation value over Haar-random state | \u27e9 = U | 0\u27e9. The function dTV(PU , ) in question becomes g(| \u27e9) = (2D)\u22121\u2211x\u2208{0,1}n ||D |\u27e8x | \u27e9|\n2 \u2212 1|| and can be shown to be Lipschitz with constant L \u2264 1. Again, we refer to Lemma 23 in the appendix for a precise statement and proof. Next, we use Rel. (20) to infer\nPr [ |||| dTV(PU , ) \u2212 1 e |||| \u2265 + 1 2n/2+1 ] \u2264 Pr [ |||| dTV(PU , ) \u2212 E U\u223c U [dTV(PU , )] |||| \u2265 ]\nand apply Levy\u2019s Lemma (Theorem 3) with Lipschitz constant L = 1 to the right hand side of this display."
        },
        {
            "heading": "4 Random quantum circuits of linear depth",
            "text": "In this section, we bound the two key quantities, namely f and the far from uniform probability, for random quantum circuits of linear depth. We will nd that the strong convergence of these circuit ensembles to unitary t-designs su ces to show exponentially small upper bounds on both quantities.\nPlugging these bounds into Lemma 1, we obtain the following lower bound on the average case query complexity.\nTheorem 6: (Formal version of linear depth part of Informal Theorem 1) Let > 0. Further, let d \u2265 1.2 \u00d7 1020n, let \u2264 1/150 \u2212 and let n be large enough. Then, the average case query complexity q of -learning any -fraction of brickwork random quantum circuit output distributions is lower bounded by\nq + 1 \u2265 ( \u2212 3200 \u00d7 2\u2212n) 2n\u22122 2 . (21)\nRemark 4: Note that for any \u2265 2\u2212n/4 and, say any \u2264 1/160 by Theorem 6 learning a fraction > 3200 \u22c5 2\u2212n = O(2\u2212n) requires at least q = 2\u03a9(n) many queries.\nMoreover, in the practically inspired regime 1/poly(n) \u2264 \u2264 1/150 \u2212 , we obtain q = \u03a9(2n)."
        },
        {
            "heading": "4.1 Maximally distinguishable fraction",
            "text": "We begin with bounding the maximally distinguishable fraction.\nLemma 7: Let > 0, n \u2265 2 and d \u2265 3.2((2 + ln(2))n + ln(n) + ln(1/ )). Then for all \u2236 {0, 1}n \u2192 [\u22121, 1] it holds\nPr U\u223c C [|PU [ ] \u2212 [ ]| > ] \u2264 (2 + ) 2n 2 . (22)\nProof. First, we show the result for an exact unitary 2-design. Using the rst moment from Equation (75), we nd that\nE U\u223c U [PU [ ]] = \u2211 x\u2208{0,1}n ( EU\u223c U [PU (x)] (x)) =  [ ]. (23)\nThus, by Chebyshev\u2019s inequality, for any > 0,\nPr U\u223c U\n[|PU [ ] \u2212 [ ]| > ] \u2264 Var [PU [ ]]\n2 . (24)\nThe variance is given by\nVar U\u223c U [PU [ ]] = E U\u223c U [PU [ ]2] \u2212( EU\u223c U [PU [ ]])\n2\n= \u2211 x \u2211 y (x) (y)( EU\u223c U\n[PU (x)PU (y)] \u2212 1 22n) .\n(25)\nInserting the second moment from Equation (76) and bounding (x) (y) \u2264 1, we nd\nVar U\u223c U [PU [ ]] = \u2211 x \u2211 y (x) (y)(\n1 2n(2n + 1) [1 + x,y] \u2212 1 22n) \u2264 1 2n\u22121 = O(2\u2212n) (26)\nwhich holds for any exact unitary 2-design.\nLet us now turn back to random quantum circuits. At depth d \u2265 3.2((2 + ln(2))n + ln(n) + ln(1/ )), the measure C forms an \u22c52\u2212n-approximate 2-design [HH21]. Notice that by H\u00f6lder\u2019s inequality and Equation (68), it holds\nE U\u223c C [PU (x)PU (y)] \u2212 E U\u223c U [PU (x)PU (y)]\n\u2264 ||||| Tr [|x\u27e9\u27e8x | \u2297 |y\u27e9\u27e8y | ( EU\u223c C [ U |0n\u27e9\u27e80n|U \u2020] \u22972 \u2212 E U\u223c U [U |0n\u27e9\u27e80n|U \u2020] \u22972 )] ||||| \u2264 \u2016\u2016\u2016\u2016 E U\u223c C [U |0n\u27e9\u27e80n|U \u2020] \u22972 \u2212 E U\u223c U [U |0n\u27e9\u27e80n|U \u2020] \u22972\u2016\u2016\u2016\u20161\n\u2264 23n .\n(27)\nThen, as in Equation (26), we arrive at\nVar U\u223c C [PU [ ]] = \u2211 x \u2211 y (x) (y)( EU\u223c C\n[PU (x)PU (y)] \u2212 1 22n)\n\u2264 \u2211 x \u2211 y (x) (y)( EU\u223c U [PU (x)PU (y)] + 23n\n\u2212 1 22n) \u2264 1 2n\u22121 + 2n = 2 + 2n\n, (28)\nwhich completes the proof."
        },
        {
            "heading": "4.2 Far from uniformity via unitary designs",
            "text": "In this section, we will use higher moments to show a far from uniform property that holds with probability 1 \u2212 exp(\u2212\u03a9(n)). Notice, that third moments cannot su ce to prove such a statement as the Cli ord group is a 3-design but a constant fraction (\u2248 0.4) of stabilizer states yield uniform output distributions as shown in Appendix D.\nA standard technique for lower bounding expectation values of 1-norms is Berger\u2019s inequality [Ber97]\nE [|S|] \u2265 ( E [S2])\n3 2\n(E [S4]) 1 2 , (29)\nfor a random variable S. However, applying this to the expected total variation distance yields another constant lower bound on the expectation value, which is not su cient to prove a bound with probability 1 \u2212 o(1). Instead, we will use that, very similarly, we have:\nLemma 8:\n\u2016f \u20161 \u2265 \u2016f \u201632 \u2016f \u201624\n(30)\nfor functions f \u2236 {0, 1}n \u2192 R.\nProof. This follows from H\u00f6lder\u2019s inequality\n\u2016f \u201622 = \u27e8f a, f b\u27e9 \u2264 ( \u2211 x f pa(x) )\n1 p\n( \u2211 x f qb(x) )\n1 q\n, (31)\nfor a + b = 2 and 1p + 1 q = 1. Choosing a = 4 3 , b = 2 3 , p = 3 and q = 3 2 , yields the result.\nWe will prove concentration inequalities both for the numerator and the denominator using eighth moments and then apply a union bound to show that both scale as desired with high probability. This will allow us to prove a qualitative version of the conjecture by Aaronson and Chen [AC17] in the a rmative.\nTheorem 9: Let be a 110002 \u221210n-approximate unitary 8-design and n \u2265 2, then:\nPr U\u223c [dTV (PU , ) \u2265 1 150] \u2265 1 \u2212 3200 \u00d7 2\u2212n. (32)\nApplications of the 8-design property are rare and we pose it as an open problem whether Theorem 9 can be further derandomized.\nWe know that 3-designs are not su cient as the Cli ord group is one. Can the same scaling be shown using only the (approximate) 4-design property? To this end, we prove the following theorem, which does not yield exponential concentration, but a weaker trade-o between the probability and the total variation distance. This property therefore still allows for meaningful average-case hardness statements with probability 1 \u2212 o(1), which separates 4-designs from the Cli ord group. Moreover, far better constants are known for the generation of approximate 4- designs [HH21].\nTheorem 10: Let be a 110002 \u221210n-approximate unitary 4-design and n \u2265 2, then for any c > 0 we have:\nPr U\u223c [dTV (PU , ) \u2265 1 20 \u221a c + 18] \u2265 1 \u2212 100 \u00d7 2\u2212n \u2212 25 c . (33)\nBefore we prove Theorem 9 and Theorem 10, we state the following corollary of Theorem 9 which is due to the bounds from [HH21]:\nCorollary 11: Denote by C the distribution on U(2n) obtained from brickwork random quantum circuits of depth d . For d \u2265 1.2 \u00d7 1020n and n \u2265 2, we have\nPr U\u223c C [ dTV (PU , ) \u2265 1 150] \u2265 1 \u2212 3200 \u00d7 2\u2212n. (34)\nProof of Theorem 9. Applying Lemma 8 to the function f (x) = PU (x) \u2212 (x) we get\n\u2016PU \u2212 \u20161 \u2265 \u2016PU \u2212 \u201632 \u2016PU \u2212 \u201624 . (35)\nThe proof strategy is to show concentration inequalities for the numerator and the denominator, independently. Then, we apply the union bound to show that both events are realized simultaneously with high probability.\nWe apply Chebyshev\u2019s inequality to the collision probability Z \u2236= \u2211x PU (x)2 in order to estimate \u2016PU \u2212 \u201622 = Z \u2212 1/D. We will use the notation D = 2n. In the following, we will make an error of size 10\u22123D\u221210, compared to the Haar value when evaluating monomials E [PU (x1) 1 \u2026 PU (xk) k] with \u2211k k \u2264 8. In the following calculations, we denote by Ei \u2208 R with i = 1, 2, 3, 4 error terms with |Ei | \u2264 10\u22123D\u221210. We have chosen this error such that it is negligible for any of the below calculations. The reader may ignore it, but needs to keep in mind that it mildly a ects the constants.\nUsing Lemma 25, we compute the rst and second moment of Z by\nE U\u223c\n[Z ] = 2\nD + 1 + DE1 (36)\nand\nE U\u223c [Z 2] = EU\u223c [ \u2211 x\u2260y PU (x)2PU (y)2 +\u2211 x PU (x)4]\n= 4(D \u2212 1)\n(D + 1)(D + 2)(D + 3) + 24 (D + 1)(D + 2)(D + 3) + D2E2\n= 4\n(D + 1)(D + 2) + 8 (D + 1)(D + 2)(D + 3) + D2E2\n\u2264 4D\u22122 + 8D\u22123 .\n(37)\nWe readily compute the variance 2 of Z:\n2 = E U\u223c [Z 2] \u2212( EU\u223c [Z ])\n2\n\u2264 4D\u22122 + 8D\u22123 \u2212 4 1\n(D + 1)2 + 2D|E1| + D2|E1|2\n\u2264 17 \u00d7 D\u22123 ,\n(38)\nwhere we have used in the last inequality that 1 \u2212 x2 < 1 for x > 0 implies 11+x > 1 \u2212 x and hence\n( 1 D + 1) 2 = ( 1 1 + D\u22121) 2 D\u22122 \u2265 (1 \u2212 D\u22121) 2 D\u22122 \u2265 (1 \u2212 2 \u00d7 D\u22121)D\u22122 , (39)\nwhere again the last step is Bernoulli\u2019s inequality. Plugging this into Chebyshev\u2019s inequality, we nd\nPr U\u223c [ ||||| \u2016PU \u2212 \u201622 \u2212( D \u2212 1 D + 1) D\u22121 \u2212 DE1 ||||| \u2265 k ] = Pr U\u223c [ |||| Z \u2212 2 D + 1 \u2212 DE1 |||| \u2265 k] = 2 k2 \u2264 17D\u22123 k2 (40)\nfor the probability of Z being outside an interval of radius k centered at its mean Choosing\nk = 12 D\u22121 D+1 \u22c5 D \u22121 \u2212 DE1, this implies\nPr U\u223c [\u2016PU \u2212 \u2016 2 2 \u2264 1 2 D \u2212 1 D + 1 D\u22121] \u2264 17 \u00d7( D \u2212 1 D + 1 D\u22121 \u2212 DE1)\n\u22122\n\u00d7 D\u22123\n\u2264 18 \u00d7( D + 1 D \u2212 1)\n2\nD\u22121\n\u2264 18 \u00d7( 21 + 1 21 \u2212 1)\n2\nD\u22121\n\u2264 50D\u22121 .\n(41)\nHere we used in the second inequality that 11\u2212x \u2264 1 + 2x for 0 \u2264 x \u2264 1 2 , which, after multiplying it with 1 \u2212 x is equivalent to 0 \u2264 x(1 \u2212 2x). This bound will be su cient to bound the numerator of Equation (35).\nNext, we will nd a concentration inequality for the denominator of Equation (35) by essentially the same strategy. We nd\n\u2016PU \u2212 \u201644 = \u2211 x (PU (x) \u2212 D\u22121) 4 = \u2211 x (PU (x)4 \u2212 4PU (x)3D\u22121 + 6PU (x)2D\u22122 \u2212 4PU (x)D\u22123 + D\u22124)\n\u2264 \u2211 x PU (x)4\n\u23df\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23df\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23df =\u2236X\n+6D\u22122\u2211 x PU (x)2\n\u23df\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23df\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23df =Z\n,\n(42)\nwhere we have used \u2211x (D\u22124 \u2212 4PU (x)D\u22123) \u2264 0.\nWe will prove probability inequalities for the two terms in Equation (42) independently. In fact, the second term can be handled similarly to Equation (41). We apply Equation (40) with k = 1 D+1 \u2212 DE1 to obtain\nPr U\u223c [Z \u2265 3 D + 1 \u00d7 D\u22121] \u2264 18(D + 1) 2D\u22123 \u2264 30D\u22121. (43)\nFor the rst term in Equation (42) we use Lemma 25 to compute the rst\nE U\u223c [X ] = \u2211 x E U\u223c [PU (x)\n4] = 4!\nD\u22ef (D + 3) + DE3 = 24 (D + 1)(D + 2)(D + 3) + DE3 , (44)\nand second moments\nE U\u223c [X 2] = \u2211 x E U\u223c [PU (x) 8] +\u2211 x\u2260y E U\u223c [PU (x) 4PU (y)4]\n= 8!\n(D + 1)\u22ef (D + 7) + 242(D \u2212 1) (D + 1)\u22ef (D + 7) + D2E4\n= 8! \u2212 2 \u00d7 242\n(D + 1)\u22ef (D + 7) +\n242\n(D + 2)\u22ef (D + 7) + D2E4 .\n(45)\nFor the variance this implies\n2X \u2264 8!\n(D + 1)\u22ef (D + 7) +\n242\n(D + 2)\u22ef (D + 7) + D2E4\n\u2212 242\n(D + 1)2(D + 2)2(D + 3)2 + 2\n242\n(D + 1)2(D + 2)2(D + 3)2 D|E3| + D2E23\n\u2264 41000D\u22127 + 242( 1 (D + 2)\u22ef (D + 7) \u2212 1 (D + 1)2(D + 2)2(D + 3)2)\n\u23df\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23df\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23de\u23df <0\n\u2264 41000D\u22127.\n(46)\nVia Chebyshev\u2019s inequality, we obtain\nPr U\u223c [ |||| X \u2212 24 (D + 1)(D + 2)(D + 3) + DE3 |||| \u2265 k] \u2264 41000D\u22127 k2 . (47)\nChoosing k = 12(D+1)(D+2)(D+3) \u2212 DE3, this implies\nPr U\u223c [X \u2265 36 (D + 1)(D + 2)(D + 3)] \u2264 41001 122 D\u22127(D + 1)2(D + 2)2(D + 3)2 \u2264 3100D\u22121 , (48)\nfor n \u2265 2.\nWe can now put these bounds together via a union bound applied twice: For any 1, 2 > 0 such that 1 2 \u2265 , we nd\nPr U\u223c [\u2016PU \u2212 \u20161 \u2264 ] \u2264 PrU\u223c [ \u2016PU \u2212 \u201632 \u2016PU \u2212 \u201624 \u2264 ]\n\u2264 Pr U\u223c [(\u2016PU \u2212 \u2016 3 2 \u2264 1) \u2228 (\u2016PU \u2212 \u2016 2 4 \u2265 2)]\n\u2264 Pr U\u223c [\u2016PU \u2212 \u2016 3 2 \u2264 1] + PrU\u223c [\u2016PU \u2212 \u2016 2 4 \u2265 2]\n= Pr U\u223c [\u2016PU \u2212 \u2016 2 2 \u2264 2 3 1 ] + PrU\u223c [\u2016PU \u2212 \u2016 4 4 \u2265 2 2]\n\u2264 Pr U\u223c [\u2016PU \u2212 \u2016 2 2 \u2264 2 3 1 ] + PrU\u223c [X + 6D 2(Z + 2\u2212n) \u2265 22]\n\u2264 Pr U\u223c [\u2016PU \u2212 \u2016 2 2 \u2264 2 3 1 ] + PrU\u223c [X \u2265 a1] + PrU\u223c [6D \u22122Z \u2265 a2] ,\n(49)\nwith a1 + a2 = 22 . These probabilities are bounded in Equations (41), (43) and (48). Choosing 2 3 1 = 12 D\u22121 D+1D\n\u22121 \u2265 10\u2212 23D\u22121 (for n \u2265 2), a1 = 36 \u00d7D\u22123 and a2 = 18 \u00d7D\u22123 implies 22 = 54 \u00d7D\u22123. Plugging this into Equation (49) yields Theorem 9, where dTV (PU , ) = 12 ||PU \u2212 ||1.\nProof of Theorem 10. The proof of Theorem 10 follows analogously to the proof of Theorem 9. However, instead of proving concentration of the random variable X = \u2211x PU (x)4 to bound\nPr[X \u2265 a1] as in Equation (49), we instead apply Markov\u2019s inequality, to get\nPr U\u223c [X \u2265 a1] \u2264 EU\u223c [X ] a1 \u2264 24 (D + 1)(D + 2)(D + 3) \u22c5 1 a1 + DE3 \u2264 25D\u22123 a1 . (50)\nTherefore, the result follows by choosing a1 = cD\u22123 for c > 0."
        },
        {
            "heading": "5 Random quantum circuits of sub-linear depth",
            "text": "In this section, we bound the two key quantities, namely f and the far from uniform probability, for random quantum circuits in the regime of sub-linear depth. This regime of circuit depth includes the shortest circuits for which we can still show super-polynomial query complexity lower bounds and hence hardness of learning. Note that in this regime, the random circuit ensembles that we consider do not yet form even a unitary 2-design requiring di erent techniques to obtain these bounds. Plugging these bounds into Lemma 1, we obtain the following lower bound on the average case query complexity.\nTheorem 12: (Formal version of sub-linear depth part of Informal Theorem 1) Let c = 1/ log(5/4). Further, let c log n \u2264 d \u2264 c(n+log n), > 0 and \u2264 1/4\u2212 . Then, for su ciently large n, the average case query complexity q of -learning any -fraction of brickwork random quantum circuits is lower bounded by\nq + 1 \u2265 ( \u2212 3/4 \u2212 \u2212 ) 2 3n \u22c5 ( 4 5) d , (51)\nMoreover, if d > c(n + log n) then it holds\nq + 1 \u2265 ( \u2212 3/4 \u2212 \u2212 ) 2n\u22122 2 . (52)\nRemark 5: Note that Theorem 12 implies that in the practically relevant regime of = 1/poly(n), learning circuits of depth !(log(n)) to some constant precision requires a super-polynomial number of queries. In particular, already for any constant fraction slightly greater than 3/4 there exists no e cient statistical query algorithm for any accuracy = \u03a9(1/poly(n))."
        },
        {
            "heading": "5.1 Maximally distinguishable fraction via restricted depth moments",
            "text": "We begin with bounding the maximally distinguishable fraction.\nLemma 13: For all d \u2265 log nlog 5/4 and for all \u2236 {0, 1} n \u2192 [\u22121, 1] it holds\nPr U\u223c C [|PU [ ] \u2212 [ ]| > ] \u2264 1 2 [ n ( 4 5)\nd\n(1 + 1 2n) + 1 2n ] . (53)\nRemark 6: Note that Equation (53) can be simpli ed as follows, which lead to the two cases in\nTheorem 12:\nPr U\u223c C [|PU [ ] \u2212 [ ]| > ] \u2264\n{ 3n 2 \u22c5 ( 4 5)\nd , for d \u2264 n+log(n)log(5/4) 3\n2n 2 , for d > n+log(n) log(5/4) .\n(54)\nProof of Lemma 13: The proof strategy is essentially identical to the rst part of the proof of Lemma 7 bounding the same quantity for linear depth circuits. However, one replaces the moments over the Haar measure with the moments of restricted depth models obtained in Lemma 26. In particular, the second moment in Equation (25) gets replaced by the one given in Equation (78). The moments in Lemma 26 hold for restricted depth circuits in 1D. They are obtained via a mapping to a statistical mechanics model [Hun19] also used in [BCG21] to bound a similar quantity, for more details see Appendix C."
        },
        {
            "heading": "5.2 Far from uniformity for constant-depth circuits",
            "text": "A direct approach to bound Pr [dTV(PU , ) \u2265 ] is to lower bound the expectation E [dTV(PU , )]. Then, using Markov\u2019s inequality, a far-from-uniform-bound follows immediately as made explicit by the following lemma:\nLemma 14: For any random variable 0 \u2264 Y \u2264 1 and any 0 < < 1 we have\nPr (Y \u2265 ) \u2265 E [Y ] \u2212 1 \u2212 . (55)\nIn [AC17], Aaronson and Chen show the following lower bound on EU\u223c C [dTV(PU , )].\nLemma 15: (Section 3.5, [AC17]) For any n \u2265 2, d \u2265 1, it holds that\nE U\u223c C [dTV(PU , )] \u2265 1 4 . (56)\nCuriously, their proof only takes into account the randomness in drawing the very last two-qubit gate which is why the bound holds already at depth d = 1. By Lemma 14, we immediately nd:\nCorollary 16: For any n \u2265 2, d \u2265 1, \u2208 [0, 1/4], it holds that\nPr U\u223c C [dTV(PU , ) \u2265 ] \u2265 1 4 \u2212 1 \u2212 \u2265 1 4 \u2212 . (57)"
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Yihui Quek, Dominik Hangleiter, Jean-Pierre Seifert, Scott Aaronson, and Lijie Chen for many helpful discussions and insights. J. H. acknowledges funding from the Harvard Quantum Initiative. The work of R. K. is supported by the SFB BeyondC (Grant No. F7107- N38), the Project\nQuantumReady (FFG 896217) and the BMWK (ProvideQ). The Berlin team thanks the BMWK (PlanQK, EniQmA), the BMBF (Hybrid), and the Munich Quantum Valley (K-8). This work has also been funded by the Deutsche Forschungsgemeinschaft (DFG) under Germany\u2019s Excellence Strategy \u2013 The Berlin Mathematics Research Center MATH+ (EXC-2046/1, project ID: 390685689) as well as CRC 183 (B1)."
        },
        {
            "heading": "A Omitted proofs for Haar random unitaries",
            "text": "In this appendix section, we provide the technical details for studying the Haar random case. We rst introduce Gaussian integration \u2013 a powerful way to compute expectation values of functions over uniformly random pure states Haar random states. This technique is complementary to the more widely used Weingarten formalism. The two approaches have di erent strengths and weaknesses. One core advantage of Gaussian integration is that it can be readily applied to functions that are not well approximated by homogeneous polynomials. The expected TV distance between a Haar random outcome distribution and the uniform distribution is one such function which features prominently in this work.\nSubsequently, we will also provide tight bounds on the Lipschitz constants of these functions. This is the only missing ingredient to show exponentially strong concentration around the previously computed expectation values by means of Levy\u2019s lemma.\nWe emphasize that this appendix section covers the extreme case of global Haar-random unitaries. Such evolutions scramble information across all subsystems and we therefore present our results directly in terms of total Hilbert space dimension D, an abstract computational basis {|1\u27e9,\u2026 , |D\u27e9} and global (pure) states | \u27e9 = U | 0\u27e9 \u2208 CD , where | 0\u27e9 is a designated (simple) starting state, e.g., a product state. For n-qubit systems, this would amount to setting D = 2n and equating the d-th basis state |d\u27e9 with the n-bit string xdy = x1(d) \u2026 xn(d) \u2208 {0, 1}n that corresponds to the binary representation of d \u2208 N.\nA.1 Haar random state averages via Gaussian integration Gaussian integration is a standard technique in mathematical signal processing and data science that allows us to recast an expectation value over the unit sphere as an expectation value over standard Gaussian random vectors. The latter has the distinct advantage that individual vector components become statistically independent from each other and follow the very simple normal distribution. These features are very useful for computing non-polynomial functions that only depend on comparatively few vector entries. We will showcase this technique based on two exemplary expectation values that feature prominently in this work:\n(i) the function value PU [ ] = \u2211Dd=1 (d) |\u27e8d |U | 0\u27e9| 2 averaged uniformly over all unitaries U ,\nsee Theorem 18 below and\n(ii) the expected TV distance between the outcome distribution of a Haar random pure state | \u27e9 = U | 0\u27e9 and the uniform distribution, see Theorem 19 below.\nAt the heart of both computations is the following powerful meta-theorem, known as Gaussian integration.\nTheorem 17: (Gaussian integration) Let f \u2236 CD \u2192 C be a homogeneous function with even degree 2k \u2208 2N, i.e., f (ax) = |a|2kf (x) for x \u2208 CD and a \u2208 C. Then, we can rewrite the Haar expectation value as\nE | \u27e9\u223c S\n[f (| \u27e9)] = 1 k!2k( D + k \u2212 1 k )\n\u22121\nE gi ,\u210ei iid\u223c (0,1) [f (g1 + i\u210e1,\u2026 , gD + i\u210eD)] , (58)\nwhere gi , \u210ei iid\u223c  (0, 1) denote independent standard Gaussian random variables ( = 0, 2 = 1).\nIn words: the left hand side features the Haar average over all pure states (complex unit vectors), while the right hand side features an expectation value over 2D statistically independent standard Gaussian random variables gj , \u210ej \u223c  (0, 1) with probability density function exp (\u2212x2/2) / \u221a 2 . This reformulation can be extremely helpful in concrete calculations, because the individual vector components on the right hand side are all statistically independent.\nProof. Let us start by considering the complex-valued standard Gaussian vector g + ih \u2208 CD with g = (g1,\u2026 , gD), h = (\u210e1,\u2026 , \u210eD) \u2208 RD and gj , \u210ej\niid\u223c  (0, 1). Switching to polar coordinates allows us to rewrite this vector as\ng + ih = r(g, h)|g\u0302 + ih\u27e9 with |g\u0302 + ih\u27e9 \u2208 SD\u22121 (direction) and r(g, h)2 = D\n\u2211 j=1 (g2j + \u210e 2 j ) (radius).\nWe can now use homogeneity of the function f \u2236 CD \u2192 C to rewrite the Gaussian expectation value on the right hand side of Equation (58) as\nE g,h\u223c (0,I) [f (g + ih)] = E g,h [f (r(g, h)|g\u0302 + ih\u27e9)] = Eg,h [r(g, h) 2kf (|g\u0302 + ih\u27e9)] ,\nwhere we have succinctly accumulated all Gaussian random variables into two vectors g and h. Now, something interesting happens. The particular form of the standard Gaussian probability density ensures that radius (r(g, h)) and direction (|g\u0302 + ih\u27e9) can be decomposed into two statistically independent random variables/vectors. The square r2 of the former follows a 22Ddistribution with 2D degrees of freedom while the latter must be a unit vector that is sampled uniformly from the complex unit sphere SD\u22121. The latter is a consequence of the fact that the distribution of standard Gaussian vectors g + ih is invariant under unitary transformations. Statistical independence, on the other hand, can be deduced from transforming the probability density function (pdf) of a Gaussian random vector into generalized spherical coordinates. Under such a transformation, the pdf decomposes into a product of a radial part (the radius) and an angular part. We can now use these insights to decompose the original expectation value into a product of two expectation values:\nE g,hiid\u223c (0,I) [r(g, h) 2kf (|g\u0302 + ih\u27e9)] = Er2\u223c 22D , g\u0302+ih\u223c S [ r2kf (|g\u0302 + ih\u27e9)]\n= E r2\u223c 22D [r2k] E\u0302 g+ih\u223c S [f (g\u0302 + ih\u27e9)] .\nThe second expression describes a uniform integral of f over the complex unit sphere in D dimensions. This is exactly the Haar expectation value on the left hand side of Equation (58). The other expression is the k-th moment of a 2 random variable with 2D degrees of freedom. These are well-known and amount to\nE r2\u223c 22D [r2k] = \u0393(2D + k/2) \u0393(k/2) = (2D)(2D + 2)\u22ef (2D + 2(k \u2212 1)) = k!2k( D + k \u2212 1 k ) .\nPutting everything together, allows us to conclude\nE g,hiid\u223c (0,I) [f (g + ih)] = E gi ,\u210ei iid\u223c (0,1) [f (g1 + i\u210e1,\u2026 , gD + i\u210eD)]\n= E r2\u223c 22D [r2k] E | \u27e9\u223c S [f (| \u27e9)] =k!2k( D + k \u2212 1 k ) E | \u27e9\u223c S [f (| \u27e9)] .\nThe claim in Equation (58) is an immediate reformulation of this equality.\nWe now have the essential tool at hand to compute the Haar expectation values that matter for this work.\nTheorem 18: (Haar average of bounded function expectation values) Let \u2236 {1,\u2026 , D} \u2192 [\u22121, 1] be a function and de ne PU [ ] = \u2211Dd=1 (d) |\u27e8d |U | 0\u27e9|\n2. Then, the expectation value of PU [ ] over Haar random unitaries becomes\nE U\u223c U [PU [ ]] = E | \u27e9\u223c S [\nD\n\u2211 d=1 (d) |\u27e8d | \u27e9|2 ] =\nD\n\u2211 d=1\n1 D (d) =  [ ] .\nThis is a standard result that readily follows from Haar integration via Weingarten calculus and the representation theory of the unitary group. Let us now show how to achieve the same result with Gaussian integration (Theorem 17).\nProof. Let us start by using linearity to rewrite the desired expectation value as\nE U\u223c U\n[PU [ ]] = D\n\u2211 d=1 (d) E | \u27e9\u223c S\n[|\u27e8d | \u27e9|2] .\nThe claim then follows from the following equality:\nE | \u27e9\u223c S [|\u27e8d | \u27e9|2] = 1 D for all d = 1,\u2026 , D. (59)\nwhich we now prove for any basis vector d . Once d is xed, we can interpret this as the expectation value of the function fd (| \u27e9) = |\u27e8d | \u27e9|2 which selects the d\u2019th vector entry (amplitude) and outputs its magnitude squared. Every such function is homogeneous with even degree 2 (k = 1)\nand we can use Theorem 17 to rewrite the expectation value as\nE | \u27e9\u223c S [|\u27e8d | \u27e9|2] = E | \u27e9\u223c S [fd (| \u27e9)] = 1 1!21( D 1)\n\u22121\nE gi ,\u210ei iid\u223c (0,1) [fd (g1 + i\u210e1,\u2026 , gD + i\u210eD)]\n= 1 2D E gi ,\u210ei iid\u223c (0,1) [|gd + i\u210ed |2] = 1 2D ( E gd\u223c (0,1) [g2d] + E \u210ed\u223c (0,1) [\u210e2d]) = 1 2D (1 + 1) = 1 D .\nHere, we have used statistical independence (we can forget all Gaussian random variables which do not feature in the expression), as well as the fact that Gaussian standard variables obey E [g2d] = E [\u210e2d] = 1 (unit variance).\nThe next expectation value is more intricate by comparison. There, Weingarten calculus would not work, because the function involved is not a well-behaved polynomial. Gaussian integration, however, can handle such non-polynomial expectation values and yields the following elegant display.\nTheorem 19: (Haar expectation of the TV distance) Let PU (d) = | \u27e8d |U | 0\u27e9|2 be the output distribution of a D level quantum system in the computational basis and  the D dimensional uniform distribution. The expectation value of the total variation distance dTV(PU , ) over Haar random unitaries U obeys\n1 e \u2212 1 2 \u221a D \u2264 E U\u223c U [dTV (PU , )] \u2264 1 e + 1 2 \u221a D .\nThe approximation error is controlled by 1/(2 \u221a D)which diminishes exponentially for n-qubit systems (D = 2n).\nProof. Let us once more start by using linearity of the expectation value to rewrite the desired expression as\nE U\u223c U [dTV (PU , )] = 1 2\nD\n\u2211 d=1 E | \u27e9\u223c S [\n1 2\nD\n\u2211 d=1 |||| |\u27e8d | \u27e9|2 \u2212 1 D ||||] = 1 2D\nD\n\u2211 d=1 E | \u27e9\u223c S\n[||D |\u27e8d | \u27e9| 2 \u2212 1||] .\nNext, note that unitary invariance of | \u27e9 \u223c S ensures that each of the summands on the right hand side must yield the same expectation value. This allows us to simplify further and obtain\nE U\u223c U [dTV (PU , )] = 1 2D\nD\n\u2211 d=1 E | \u27e9\u223c S\n[||D |\u27e8d | \u27e9| 2 \u2212 1||] = 1 2 E | \u27e9\u223c S [||D |\u27e81| \u27e9| 2 \u2212 1||] .\nNote that this is not a polynomial function, but we can still use Gaussian integration to accurately bound this expression. To this end, we rst use \u27e8 | \u27e9 = 1 to rewrite the expression of interest as\nthe uniform average over a homogeneous function with even degree 2 (k = 1):\nf (| \u27e9) = 1 2 ||D |\u27e81| \u27e9| 2 \u2212 \u27e8 | \u27e9|| 2 .\nWe can now use Theorem 17 to conclude\nE U\u223c U [dTV (PU , )] = E | \u27e9\u223c S [ 1 2 ||D |\u27e81| \u27e9| 2 \u2212 \u27e8 | \u27e9||] = E| \u27e9\u223c S [f (| \u27e9)]\n= 1 1!21( D 1)\n\u22121\nE gi ,\u210ei iid\u223c (0,1) [f (g1 + i\u210e1,\u2026 , gd + i\u210ed )]\n= 1 2D E gj ,\u210ej iid\u223c (0,1) [ 1 2 |||D\u27e81|g + ih\u27e9| 2 \u2212 \u27e8g + ih|g + ih\u27e9||]\n= 1 4D E gj ,\u210ej iid\u223c (0,1) [ ||||| D (g21 + \u210e 2 1) \u2212 D \u2211 j=1 (g2j + \u210e 2 j ) |||||]\n= 1 2 E gj ,\u210ej iid\u223c (0,1) [ ||||| 1 2 ( g21 + \u210e 2 1) \u2212 1 + 1 \u2212 1 2D D \u2211 j=1 (g2j + \u210e 2 j ) |||||] . (60)\nIt seems possible to compute this expectation value directly by rewriting each expectation value as an integral weighted by the Gaussian probability density function exp (\u2212g2j /2), but doing so would incur a total of 2D nested integrations. Here, we instead simplify the derivation considerably by providing reasonably tight upper and lower bounds. We have already suggestively decomposed the expression within the absolute value into two terms that are easier to control individually:\nM = 1 2 E g1,\u210e1 iid\u223c (0,1) [ |||| 1 2 ( g21 + \u210e 2 1) \u2212 1 ||||] (asymptotic mean value),\n\u0394 = 1 2 E gj ,\u210ej iid\u223c (0,1) [ ||||| 1 \u2212 1 2D D \u2211 j=1 (g2j + \u210e 2 j ) |||||]\n(approximation error).\nApplying the triangle inequality to Equation (60) readily allows us to infer\nM \u2212 \u0394 \u2264 E U\u223c U [dTV (PU , )] \u2264 M + \u0394 (61)\nand the two remaining parameters can now be computed independently. We defer the actual calculations to the end of this subsection and only state the results here:\nM =1/e (see Lemma 21 below),\n\u0394 \u22641/ (2 \u221a D) (see Lemma 20 below).\nInserting these numerical values into Equation (61) yields the claim.\nLet us now supply the technical calculations that are essential for completing the proof of Theorem 19.\nLemma 20: Let g1,\u2026 , gd , \u210e1,\u2026 , \u210ed be 2D independent standard Gaussian random variables. Then,\n\u0394 = 1 2 E gi ,\u210ei iid\u223c (0,1) [ ||||| 1 \u2212 1 2D D \u2211 j=1 (g2j + \u210e 2 j ) |||||] \u2264 1 2 \u221a D .\nProof. There is no conceptual di erence between the gj and \u210ej random variables. So, we may replace them with 2D independent standard Gaussian random variables g\u03031,\u2026 , g\u03032D\niid\u223c  (0, 1). This reformulation yields\n\u0394 = 1 4D E g\u0303i iid\u223c (0,1) [ ||||| 2D \u2212 2D \u2211 j=1 g\u03032j |||||] \u2264 1 4D E g\u0303i iid\u223c (0,1) [( 2D \u2212 2D \u2211 j=1 g\u03032j )\n2\n]\n1/2\n, (62)\nwhere the last inequality is Jensen\u2019s. The remaining expectation value is the variance of a 2 random variable with 2D degrees of freedom. Standard textbooks tell us that this variance is equal to 2(2D) = 4D. We do, however, believe that it is instructive to compute this variance directly, because it showcases an important subroutine when computing Haar integrals via Gaussian integration. Note that the random variables involved obey\nE g\u0303j ,g\u0303k iid\u223c (0,1) [g\u03032j g\u0303 2 k] = \u23a7\u23aa\u23aa \u23a8\u23aa\u23aa\u23a9 Eg\u0303j\u223c (0,1) [g\u0303 2 j ]Eg\u0303k\u223c (0,1) [g\u0303 2 k] = 1 , whenever j \u2260 k and E [g\u03034j ] = 3 , else if j = k.\nCombining them yields Eg\u0303j ,g\u0303k iid\u223c (0,1) [g\u0303 2 j g\u03032k] = 1 + 2 j,k .\nIf we also recall Eg\u0303j\u223c (0,1) [g\u0303 2 j ] = 1, we can readily conclude\nE g\u0303i iid\u223c (0,1) [( 2D \u2212\n2D\n\u2211 j=1 g\u03032j )\n2\n] =4D2 \u2212 4D\n2D\n\u2211 j=1 E g\u0303j\u223c (0,1)\n[g\u03032j ] + 2D\n\u2211 j=1\n2D\n\u2211 k=1 E g\u0303j ,g\u0303k iid\u223c (0,1) [g\u03032j g\u0303 2 k]\n=4D2 \u2212 4D 2D\n\u2211 j=1 1 +\n2D\n\u2211 j=1\n2D\n\u2211 k=1 (1 + 2 j,k)\n=4D2 \u2212 8D2 + 4D2 + 4D = 4D.\nInserting this variance expression into Equation (62) yields the claim.\nLemma 21: Let g, \u210e be two independent standard Gaussian random variables. Then\nM = 1 2 E g,\u210eiid\u223c (0,1) [ |||| 1 2 ( g2 + \u210e2) \u2212 1 ||||] = 1 e .\nProof. This is the one location in our derivation, where we really utilize the power of Gaussian integration. We start by rewriting the expectation value as an integral over two independent standard Gaussian random variables with (standard Gaussian) probability density functions\nexp (\u2212g2/2) / \u221a 2 and exp (\u2212\u210e2/2) / \u221a 2 , respectively:\nM = 1 2 E g,\u210eiid\u223c (0,1) [ |||| 1 2 ( g2 + \u210e2) \u2212 1 ||||] = 1 4 \u222c \u221e \u2212\u221e ||g 2 + \u210e2 \u2212 2|| exp (\u2212(g2 + \u210e2)/2) 2 dgd\u210e.\nNext, we view (g, \u210e) \u2208 R2 as a 2-dimensional vector and switch into polar coordinates: (r cos( ), r sin( )). Note that g2 +\u210e2 = r2 and there is no angle dependence in the integral. Accordingly the volume element changes from dgd\u210e to rdrd and we obtain\nM = 1 8 \u222b\n2\n0 \u222b\n\u221e\n0\n||r 2 \u2212 2|| e \u2212r2/2rdrd = 1 4 \u222b\n\u221e\n0\n||r 2 \u2212 2|| re \u2212r2/2rdr ,\nwhere we have carried out the integral over the angle which cancels the 1/(2 )-term in front of the expression. Next, we note that the sign of the absolute value changes as we change the integration range. For r \u2208 [0, \u221a 2], we have ||r 2 \u2212 2|| = (2\u2212r 2)while ||r 2 \u2212 2|| = (r 2\u22122) for r \u2208 [ \u221a 2,\u221e). This implies\nM = 1 4 \u222b\n\u221e\n0\n||r 2 \u2212 2|| re \u2212r2/2rdr = 1 4 (\u222b\n\u221a 2\n0 (2 \u2212 r2) re\u2212r\n2/2dr + \u222b \u221e\n\u221a 2 (r2 \u2212 2) re\u2212r 2/2dr )\n= 1 4 ( 2 \u222b\n\u221a 2\n0 re\u2212r 2/2dr \u2212 \u222b\n\u221a 2\n0 r3e\u2212r\n2/2dr + \u222b \u221e\n\u221a 2 r3e\u2212r\n2/2 \u2212 2 \u222b \u221e\n\u221a 2 re\u2212r 2/2dr ) . (63)\nThese four remaining integrals can be determined from the following well-known Gaussian integration formulas for a \u2264 b:\n\u222b b\na rer 2/2 = e\u2212a 2/2 \u2212 e\u2212b\n2/2 and \u222b b\na r3e\u2212r 2/2dr = (a2 + 2)e\u2212a 2/2 \u2212 (b2 + 2)e\u2212b 2/2.\nThe limit b \u2192 \u221e produces a vanishing contributions, because limb\u2192\u221e e\u2212b 2/2 = 0 and limb\u2192\u221e(b2 + 2)e\u2212b2/2 = 0 Inserting these values into Equation (63) yields\nM = 1 4 ( 2 (e\u22120 \u2212 e\u22121) \u2212 ((2 + 0)e\u22120 \u2212 (2 + 2)e\u22121) + ((2 + 2)e\u22121 \u2212 0) \u2212 2 (e\u22121 \u2212 0))\n= 1 4 (2 \u2212 2/e \u2212 2 + 4/e + 4/e \u2212 2/e) = 1 e .\nA.2 Lipschitz constants for function evaluations and TV distances In this appendix section, we derive Lipschitz constants for the functions whose expectation value value we computed in the previous subsection. We will see that these Lipschitz constants are small (L = 2 and L = 1, respectively) which is the only requirement we need to invoke Levy\u2019s lemma to show exponential concentration around these expectation values.\nLemma 22: Fix \u2236 {1,\u2026 , D} \u2192 [\u22121, 1] and reinterpret PU [ ] = \u2211Dd=1 (d) |\u27e8d |U | 0\u27e9| 2 as a function in the pure state | \u27e9 = U | 0\u27e9, namely P (| \u27e9) = \u2211Dd=1 (d) |\u27e8d | \u27e9| 2. This function has Lipschitz constant L = 2, namely\n||P (| \u27e9) \u2212 P (| \u27e9)|| \u2264 2 \u2016| \u27e9 \u2212 | \u27e9\u2016 2 for all pure states | \u27e9, | \u27e9 \u2208 C D .\nProof. Let us start by rewriting P (| \u27e9) as a linear function in the (pure) density matrix | \u27e9\u27e8 |:\nP (| \u27e9) = D\n\u2211 d=1 (d) |\u27e8d | \u27e9|2 = \u27e8 | (\nD\n\u2211 d=1 (d)|d\u27e9\u27e8d | ) | \u27e9 = tr (\u03a6 | \u27e9\u27e8 |) ,\nwhere we have introduced the diagonal D \u00d7 D matrix \u03a6 = \u2211Dd=1 (d)|d\u27e9\u27e8d |. Note that this matrix has operator norm \u2016\u03a6\u2016\u221e = max1\u2264d\u2264D | (d)| \u2264 1, because the function values (d) are con ned to [\u22121, 1] by assumption. The matrix Hoelder inequality then implies\n||P (| \u27e9) \u2212 P (| \u27e9)|| = |tr (\u03a6 | \u27e9\u27e8 |) \u2212 tr (\u03a6 | \u27e9\u27e8 |)| = |tr (\u03a6 (| \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |))| \u2264 \u2016\u03a6\u2016\u221e || \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |\u20161 ,\nwhere \u2016 \u22c5 \u20161 denotes the trace norm. Since \u2016\u03a6\u2016\u221e \u2264 1, the claim \u2013 Lipschitz constant L = 2 \u2013 then follows from the following relation between trace distance of pure states and Euclidean distance of the state vectors involved:\n1 2 \u2016| \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |\u20161 \u2264 \u2016| \u27e9 \u2212 | \u27e9\u20162 for pure states | \u27e9, | \u27e9 \u2208 C D . (64)\nLet us now derive this useful relation. One way is to use the Fuchs-van de Graaf inequalities (which are tight for pure states) to relate the trace distance to a pure state delity:\n1 2 \u2016| \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |\u20161 = \u221a 1 \u2212 F (| \u27e9, | \u27e9) = \u221a 1 \u2212 |\u27e8 | \u27e9|2.\nFinally, we can use |\u27e8 | \u27e9| \u2264 1, as well as \u27e8 | \u27e9 = \u27e8 | \u27e9 = 1 and 2 |\u27e8 | \u27e9| \u2265 2Re (\u27e8 | \u27e9) = \u27e8 | \u27e9 + \u27e8 | \u27e9 to obtain\n\u221a 1 \u2212 |\u27e8 | \u27e9|2 = \u221a 1 + |\u27e8 | \u27e9| \u221a 1 \u2212 |\u27e8 | \u27e9| \u2264 \u221a 1 + 1 \u221a 1 \u2212 Re (\u27e8 | \u27e9) = \u221a 1 \u2212 \u27e8 | \u27e9 \u2212 \u27e8 | \u27e9 + 1\n= \u221a \u27e8 | \u27e9 \u2212 \u27e8 | \u27e9 \u2212 \u27e8 | \u27e9 + \u27e8 | \u27e9 = \u221a (\u27e8 | \u2212 \u27e8 |) (| \u27e9 \u2212 | \u27e9) = \u2016| \u27e9 \u2212 | \u27e9\u20162.\nLemma 23: Let PU (d) = | \u27e8d |U | 0\u27e9|2 be the output distribution of a D level quantum system in the computational basis and let  be the D dimensional uniform distribution. The total variation distance between both distributions de nes a function in the pure state | \u27e9 = U | 0\u27e9, namely fTV(| \u27e9) = 12 \u2211d |||\u27e8d | \u27e9| 2 \u2212 1/D||. This function has Lipschitz constant L = 1, i.e.,\n|fTV (| \u27e9) \u2212 fTV (| \u27e9)| \u2264 \u2016| \u27e9 \u2212 | \u27e9\u20162 for all pure states | \u27e9, | \u27e9 \u2208 C D .\nProof. Let us start by rewriting the absolute value of the di erence of di erent function values as\n|fTV (| \u27e9) \u2212 fTV (| \u27e9)| = 1 2 ||||| D \u2211 d=1 (|||\u27e8d | \u27e9| 2 \u2212 1/D|| \u2212 |||\u27e8d | \u27e9| 2 \u2212 1/D||) |||||\n= 1 2 ||||| D \u2211 d=1 ( ||| ||(\u27e8d | \u27e9| 2 \u2212 1/D) + (|\u27e8d | \u27e9|2 \u2212 |\u27e8d | \u27e9|2) ||| \u2212 |||\u27e8d | \u27e9| 2 \u2212 1/D|| 2 ) |||||\n\u2264 1 2\nD\n\u2211 d=1\n|||\u27e8d | \u27e9| 2 \u2212 |\u27e8d | \u27e9|2|| ,\nwhere the last inequality follows from applying the triangle inequality to each summand in order to break up the two contributions in the rst absolute value of the second line. The rst contribution then cancels with the nal term and we obtain the advertised display. We can now rewrite this new expression as\n1 2\nD\n\u2211 d=1\n|||\u27e8d | \u27e9| 2 \u2212 |\u27e8d | \u27e9|2|| = 1 2\nD\n\u2211 d=1 |\u27e8d | (| \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |) |d\u27e9| ,\nwhich accumulates the sum of the absolute values of the diagonal entries of the (pure) state di erence (| \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |). This sum of absolute diagonal entries is always smaller than the trace norm of the matrix in question2. This relation implies\n|fTV (| \u27e9) \u2212 fTV (| \u27e9)| \u2264 1 2\nD\n\u2211 d=1\n|\u27e8d | (| \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |) |d\u27e9| \u2264 1 2 \u2016| \u27e9\u27e8 | \u2212 | \u27e9\u27e8 |\u20161 ,\nand the claim \u2013 Lipschitz constant L = 1 \u2013 now follows from reusing Equation (64) to convert this trace norm distance into a Euclidean distance of the state vectors involved."
        },
        {
            "heading": "B Unitary designs",
            "text": "In this appendix we provide context for approximate unitary designs; a key tool for the results in this work. Moreover, we discuss recent bounds on the generation of designs by random quantum circuits. Recall the de nition of the moment operator:\n\u03a6(t)( )(A) \u2236= \u222b U \u2297tA(U \u2020)\u2297td (U ). (65)\nDe nition 6: (\"-Approximate Design) A probability distribution overU(D) is an \"-approximate unitary design if\n\u2016\u2016\u2016\u03a6 (t)( ) \u2212 \u03a6(t)( U ) \u2016\u2016\u2016\u25ca \u2264 \" Dt , (66)\nwhere \u2016\u2219\u2016\u25ca denotes the diamond norm, or channel distinguishability, de ned as the stabilized 1\u2192 1 norm.\n2This relation is well known in matrix analysis and follows, for instance, from Helstrom\u2019s theorem.\nIn this work we will only be concerned with averages over states, that is the case A = (| \u27e9\u27e8 |)\u2297t . In this case we have the standard formula (see e.g. [Har13]).\n\u03a6(t)( U )(A) = \u222b U \u2297t (| \u27e9\u27e8 |)\u2297t (U \u2020)\u2297td U (U ) = Psym,t (D+t\u22121t ) , (67)\nwhere Psym,t we denote the projector onto the symmetric subspace St(CD). With the above de - nition of an approximate unitary design, we obtain that for an \"-approximate unitary design, we have\n\u2016\u2016\u2016\u2016\u2016 \u03a6(t)( ) ((| \u27e9\u27e8 |)\u2297t) \u2212 Psym,t (D+t\u22121t ) \u2016\u2016\u2016\u2016\u20161 \u2264 \" Dt , (68)\nwhere \u2016\u2219\u20161 denotes the Schatten 1-norm, or trace norm.\nThe key result for the following is that random quantum circuits are in fact approximate unitary t-designs in polynomial depth [BHH16; HL09; Haf22]. These bounds come with large explicit constants. For small values of t = 2, 4, we even have good explicit constants. We present the bound from Haferkamp [Haf22]:\nTheorem 24: For n \u2265 \u23082 log2(4t) + 1.5 \u221a log2(4t) \u2309, random quantum circuits in a brickwork architecture are \"-approximate unitary t-designs in depth\nT \u2265 C ln5(t)t4+3 1\u221a\nlog2(t) (2nt + log2(1/\")), (69)\nwhere C can be taken to be 1013.\nNote that the large constants are likely an artefact of the proof technique based on the martingale technique [Nac96] in [BHH16; Haf22], which focus on the scaling in t .\nUsing instead nite-size criteria [Kna88] combined with numerics, one can greatly improve these constants for t \u2264 5. Compare [HH21, Table I]. It is likely that we could obtain comparable constants for t = 8 as well. Unfortunately, this seems to require numerics for daunting system sizes."
        },
        {
            "heading": "C Moment calculations",
            "text": "C.1 Haar moments To begin we give explicit formulas for Haar random moments. We will make use of the following standard formula repeatedly:\nE \u223c S [|\u27e8 | \u27e9|2t] = E \u223c S [Tr [(| \u27e9\u27e8 |)\u2297t (| \u27e9\u27e8 |)\u2297t]]\n= Tr [ E \u223c S [| \u27e9\u27e8 |]\u2297t (| \u27e9\u27e8 |)\u2297t]\n= Tr [ Psym,t ,D (D+t\u22121t ) (| \u27e9\u27e8 |)\u2297t ]\n= ( D + t \u2212 1 t )\n\u22121\n.\nIn fact, we will need a more general formula for the proof of Theorem 9, which we state as the following lemma.\nLemma 25: Let |i1\u27e9,\u2026 , |ik\u27e9 with i1,\u2026 , D \u2208 {1,\u2026 , D} be mutually orthogonal state vectors and = ( 1,\u2026 , k) a partition of t for t \u2264 D. Then, we nd the formula\nE | \u27e9\u223c S [\nk\n\u220f l=1 |\u27e8 |il\u27e9|2 l] = \u220fkl=1 l! D\u22ef (D + t \u2212 1) . (70)\nProof. The proof follows directly from the following calculation:\nE | \u27e9\u223c S\nk\n\u220f l=1 |\u27e8 |il\u27e9|2 l = E \u223c S [ Tr [ (| \u27e9\u27e8 |)\u2297t\nk\n\u2a02 l=1 (|il\u27e9\u27e8il |)\u2297 l]]\n= ( D + t \u2212 1 t )\n\u22121\nTr [ Psym,t ,D\nk\n\u2a02 l=1 (|il\u27e9\u27e8il |)\u2297 l]\n= 1\nD\u22ef (D + t \u2212 1) \u2211 \u2208St Tr [ r( )\nk\n\u2a02 l=1 (|il\u27e9\u27e8il |)\u2297 l] ,\n(71)\nwhere we used the notation r for the representation of St that, for each permutation \u2208 St , permutes the t tensor factors according to :\nr( )|i1\u27e9 \u2297\u22ef \u2297 |it\u27e9 \u2236= |i \u22121(1)\u27e9 \u2297\u22ef \u2297 |i \u22121(t)\u27e9. (72)\nMoreover, we used the formula Psym,t ,D = 1t! \u2211 r( ). Notice that\nTr [ r( )\nk\n\u2a02 l=1 (|il\u27e9\u27e8il |)\u2297 l] = { 1 if \u2208 S 1 \u00d7 \u2026 \u00d7 S k 0 else.\n(73)\nHence, we nd\nE | \u27e9\u223c S\nk\n\u220f l=1\n|\u27e8 |il\u27e9|2 l = 1\nD\u22ef (D + t \u2212 1) |S 1 |\u22ef |S k |\n= \u220fkl=1 l!\nD\u22ef (D + t \u2212 1) .\n(74)\nIn the special case D = 2n we thus obtain the explicit formulas for the rst and second moment:\nE U\u223c U [PU (x)] = 1 2n , (75)\nE U\u223c U\n[PU (x)PU (y)] = 1\n2n(2n + 1) [1 + x,y] . (76)\nC.2 Restricted depth moments Next, we state bounds on the rst two moments over brickwork random quantum circuits of depth d :\nLemma 26: (Moments over circuits of restricted depth \u2013 adapted from [BCG21]) For C the measure over brickwork random quantum circuits on n qubits of depth d , it holds\nE U\u223c C [PU (x)] = 1 2n , (77)\nE U\u223c C [PU (x)PU (y)] \u2264 1 22n (1 + x,y) [1 + n ( 4 5)\nd\n] . (78)\nwhere the bound in Equation (78) holds for d \u2265 log nlog 5/4 .\nProof. We note that C is an exact 1-design at any depth d3. Hence, the rst moment is the same as in Equation (75) i.e.\nE U\u223c C [PU (x)] = E U\u223c U [PU (x)] = 1 2n . (79)\n3In fact, already a single layer of randomly drawn unitary gates forms an exact 1-design. This is because this layer contains as a subgroup the Pauli group which is known to form an exact 1-design. It follows from the invariance of the Haar measure under left multiplication that random unitary circuits form an exact 1-design also for d \u2265 1.\nTo obtain the second moment given in Equation (78), we adapt and modify a calculation presented in Section 6.3 of [BCG21]. Speci cally, using a mapping to a statistical mechanics model, the second moment with respect to the random circuit, E C [PU (x)PU (y)], can be expressed as a partition function. The value of this partition function can then be bounded by counting domain walls. In Section 6.3 of Ref. [BCG21], this technique was already used to obtain an upper bound on E C [PU (x)2], for random circuits of depth d \u2265 log n log 5/4 . More speci cally, Ref. [BCG21] has obtained the upper bound\nE U\u223c C [PU (x)2] \u2264 (1 + ( 4 5)\nd\n)\nn/2\nE U\u223c U [PU (x)2] , (80)\nwhich is given in terms of the Haar expectation value E U [PU (x)2], and indeed converges to this Haar value in the in nite circuit depth-limit d \u2192 \u221e. A similar analysis allows us to obtain the following bound on the expectation value of the cross terms PU (x)PU (y),\nE U\u223c C [PU (x)PU (y)] \u2264 (1 + ( 4 5)\nd\n)\nn/2\nE U\u223c U [PU (x)PU (y)] . (81)\nNote that this upper bound is also given in terms of the corresponding Haar valueE U [PU (x)PU (y)]. We use the second moment already calculated in Equation (76). Finally, we bound the prefactor: By Bernoulli\u2019s inequality, we have that (1+xd )n \u2264 enxd . For d \u2265 log nlog 5/4 and x < 1we can then use the convexity of the exponential function ey \u2264 (1\u2212y)e0+ye1 to obtain enxd \u2264 1\u2212nxd +enxd \u2264 1+2nxd . This allows us to show that\n(1 + ( 4 5)\nd\n)\nn/2\n\u2264 1 + n ( 4 5) d . (82)\nSubstituting Equations (76) and (82) into Equation (81) then yields Equation (78)."
        },
        {
            "heading": "D Random Cli ord unitaries",
            "text": "The Cli ord group forms a 3-design. Therefore, we can carry over the bounds on f obtained via Chebychev and hence second moments from the global Haar measure to the uniform measure over global Cli ord operations. The same analogy holds for local Haar random unitaries and local Cli ord unitaries. Thus, the bounds on f from the restricted depth moments from Appendix C.2 also hold for restricted depth Cli ord circuits. The key di erence between the case of Cli ord and Haar random unitaries lies thus in the far from uniform behavior. This is emphasized in the following lemma.\nLemma 27: The probability that the output distribution of a uniformly random global Cli ord circuit on n qubits is the uniform distribution is given by\nPr U\u223cCl(2n)\n(PU =  ) = 1\n\u220fni=1 (1 + 1 2i )\n. (83)"
        },
        {
            "heading": "In particular, it asymptotically approaches",
            "text": "Pr U\u223cCl(2n)\n(PU =  ) n\u2192\u221e \u2192 0.41942244... (84)\nfrom above and for any number of qubits n, the probability is larger than 0.41.\nThus, even though \u201cnon-trivial\u201d learning is hard for random Cli ord unitary output distributions as characterized by f, the trivial learning algorithm, which always returns  , will succeed with probability larger than 0.41 over the uniformly drawn U \u223c Cl(2n).\nProof. The result of drawing a uniformly random Cli ord unitary U \u223c Cl(2n) and applying to |0\u27e9\u2297n is a uniformly random stabilizer state.\nThe number of n-qubit stabilizer states is given by [AG04]\n|n| = 2n n\n\u220f i=i\n(2i + 1) = 2n+n(n+1)/2 n\n\u220f i=1\n(1 + 1 2i)\n(85)\nThe number of n-qubit stabilizer states giving rise to the uniform distribution is given by\n|nn | = 2n \u22c5 2n(n + 1)/2 = 2n+n(n+1)/2 (86)\nThis follows from [KG15]. In particular, Corollary 2 in Ref. [KG15] gives a formula for the number of stabilizer states with pre-described inner product with respect to a xed reference stabilizer state. For our purposes, it su ces to take as reference state the all-zero state |0n\u27e9 and nd the number of stabilizer states | \u27e9 such that | \u27e8 |0n\u27e9 | = 2\u2212n. Such states are precisely the n-qubit stabilizer states giving rise to the uniform distribution.\nHence,\nPr U\u223cCl(2n)\n(PU =  ) = 2n+n(n+1)/2\n2n+n(n+1)/2\u220fni=1 (1 + 1 2i )\n= 1\n\u220fni=1 (1 + 1 2i )\n(87)\nThe asymptotic behavior of this product for n \u2192 \u221e is found in [Ben21]."
        },
        {
            "heading": "E Deterministic algorithms",
            "text": "The aim of this appendix is to give a detailed proof of Lemma 1 which is restated as Theorem 30. We follow a similar strategy as Feldman in [Fel17] by proving the result for learning via a reduction to a suitably chosen decision problem.\nProblem 2: (Decide  versus Q) Let  be some distribution class and Q some xed reference distribution. The task decide  versus Q is de ned as, given access to an unknown P \u2208  \u222a {Q} to decide whether \u201cP = Q\u201d or \u201cP \u2208 \u201d.\nWe connect the query complexity of learning with the query complexity of deciding by the following lemma. .\nLemma 28: (Learning is as hard as deciding) Let  be a distribution class and let Q be such that dTV(P, Q) > + for all P \u2208 . Let 0 < \u2264 \u2264 1. Let be a deterministic algorithm that -learns from q many accurate statistical queries. Then there exists a deterministic algorithm that decides  versus Q from q + 1 many accurate statistical queries.\nProof. We run  on the unknown distribution P \u2208  \u222a {Q} and obtain either\n\u2022 a representation of some P \u2032 which is close to P if P \u2208 , or\n\u2022 anything if P = Q.\nIn case we do not receive a representation of any distribution return \u201cP = Q\u201d. Now, assume we receive a representation of some distribution P \u2032. Using this representation compute whether P \u2032 is close to any distribution in . While this step is computationally costly, it does not require any further queries to Stat(P ). If there does not exists such a distribution in  which is -close to P \u2032, return \u201cP = Q\u201d.\nNow assume there exists an H \u2208  such that dTV(P \u2032, H ) < . To assure, that  is not biased towards returning distributions close to  if it fails, compute the set S that maximizes the total variation distance between Q and P \u2032, |P \u2032(S) \u2212 Q(S)| = dTV(P \u2032, Q). Denote by = 1S the characteristic function on S and query v \u2190 Stat (P )[ ]. If ||Q[ ] \u2212 v || \u2264 return \u201cP = Q\u201d, else return \u201cP \u2208 \u201d.\nWe analyze the algorithm for each case separately. Common to both is that the algorithm makes, by de nition, at most q + 1 statistical queries.\nTo begin with assume P \u2208 . By the correctness of  we receive a representation of some P \u2032 that is at most far from P . By assumption, for any H \u2208  it holds dTV(H,Q) > + . Then, by the de nition of S using the reverse triangle inequality we nd\n||Q[ ] \u2212 v || \u2265 |||Q[ ] \u2212 P[ ]| \u2212 ||P[ ] \u2212 v |||| > | + \u2212 | = \u2265 . (88)\nHence, we correctly decide \u201cP \u2208 \u201d.\nFor the other case assume P = Q. If  does not return a valid representation or, if  returns a representation of some P \u2032 that is more than far away from any distribution in , we know, by the correctness of , that it must hold P = Q. It remains to show the last step. Assume there is an H \u2208  which is at most far from P \u2032. Then, by assumption, for every it must hold ||Q[ ] \u2212 v || \u2264 . Hence, we correctly decide \u201cP = Q\u201d.\nUsing Lemma 28 it will be su cient to bound the query complexity of deciding. This is achieved by the next lemma.\nLemma 29: (Hardness of deciding, deterministic version) Let be a deterministic algorithm that\ndecides versusQ from qmany -accurate statistical queries, then for anymeasure over it holds\nq \u2265 (max PrP\u223c [|P[ ] \u2212 Q[ ]| > ])\n\u22121\n. (89)\nProof. Assume we run  and answer every query by Q[ ]. Denote by 1,\u2026 , q the queries made. Assume for a contradiction, that for some P \u2208  there does not exist any distinguishing query. Then, the responses Q[ i] for i = 1,\u2026 , q would be valid responses for some Stat (P ) contradicting the assumption that  successfully decides whether P = Q. Thus, for any P \u2208  there must exist at least one i that distinguishes Q from P . In particular,\n1 = Pr P\u223c [\u2203i, |P[ i] \u2212 Q[ i]| > ] (90)\n\u2264 q\n\u2211 i=1 Pr P\u223c [|P[ i] \u2212 Q[ i]| > ] (91)\n\u2264 qmax Pr P\u223c [|P[ i] \u2212 Q[ i]| > ] , (92)\nwhich completes the proof.\nWe are now set to prove our bound for the deterministic average case query complexity. Note, that Lemma 28 holds for learners that learn all of . Thus, the core of the remaining proof will be to translate the implications on worst to average case learners.\nTheorem 30: (Lemma 1 restated) Suppose there is a deterministic algorithm  that -learns a fraction of  with respect to from q many -accurate statistical queries. Then for any Q it holds\nq + 1 \u2265 \u2212 PrP\u223c [dTV(P, Q) \u2264 + ] max PrP\u223c [|P[ ] \u2212 Q[ ]| > ] , (93)\nwhere again, the max is over all functions \u2236 X \u2192 [\u22121, 1].\nProof. Let \u2032 \u2286  with (\u2032) = be a set on which  is successful. De ne\nQ = { P \u2208 \u2032 \u2236 dTV(P, Q) > + }\nand let Q be the measure conditioned on Q , such that Q(P ) = (P \u2223 P \u2208 Q). Then, by the de nition of the conditional probability,\n\u2212 Pr P\u223c [dTV(P, Q) \u2264 + ] \u2264 (Q) . (94)\nTherefore, for any it holds\nPr P\u223c Q [|P[ ] \u2212 Q[ ]| > ] = Pr P\u223c [|P[ ] \u2212 Q[ ]| > \u2223 P \u2208 Q] \u2264 PrP\u223c [|P[ ] \u2212 Q[ ]| > ] \u2212 PrP\u223c [dTV(P, Q) \u2264 + ] (95)\nwhere we used the de nition of the conditional probability.\nThe claim then follows from the observation that the average learner  for  implies a learner for Q , the complexity of which can be bounded by the complexity of deciding Q vs Q via the reduction Lemma 32. We obtain a bound for the latter from Lemma 33 applying the measure Q .\nBefore we end this section we state a variant of this bound due to Feldman to discuss the connection to Theorem 30. We restate his proof adapted to our notation for the sake of completeness.\nLemma 31: (Variation of Lemma C.2 from [Fel17] for deterministic algorithms) Suppose there is a deterministic algorithm that -learns a fraction ofwith respect to from q many -accurate statistical queries. Then for any Q it holds\nq \u2265 \u2212 supD PrP\u223c [dTV(P, D) < ] max PrP\u223c [|P[ ] \u2212 Q[ ]| > ] , (96)\nwhere the max is over all functions \u2236 X \u2192 [\u22121, 1] and the sup is over all distributions D over the domain X .\nProof. Denote by \u2032 \u2286  the subset of size (\u2032) = on which  successfully -learns from q queries. We run and answer every query byQ[ ]. By assumptionmakes q queries 1,\u2026 , q to Q and, without loss of generality, we assume that the algorithm returns some distribution D. Now, let P be any distribution in \u2032 at least -far from D. Exactly as in the proof of Lemma 29, there must exists at least one query function i that distinguishes Q from P . In particular, it must hold\n\u2212 Pr P\u223c [dTV(P, D) < ] \u2264 Pr P\u223c [\u2203i \u2236 |P[ i] \u2212 Q[ i]| > ]\n\u2264 q\n\u2211 i=1 Pr P\u223c [|P i] \u2212 Q[ i] > |]\n\u2264 qmax Pr P\u223c [|P[ ] \u2212 Q[ ]| > ] .\n(97)\nNow assume that, after interacting with Q, the algorithm does not return any valid distribution. Then, again by \u2019s determinism, for any P \u2208 \u2032 there must exist a distinguishing query i that distinguishes Q from P . The claim then follows by taking the supremum over D \u2208 X to bound the -ball around the unknown D.\nNote 2: We want to highlight that Lemma 31 is tight with respect to : The trivial algorithm, which makes zero queries and always outputs that D which maximizes the open -ball will, with probability (B (D)) over P \u223c be correct.\nNote 3: As stated above, Lemma 31 gives the optimal lower bound with respect to . However, in some cases directly bounding the weight of all -balls may not be convenient. In Appendix G we give a general recipe for obtaining bounds for all -balls just from the two ingredients used in Theorem 30: The maximally distinguishable fraction and the mass of the ball around the reference distribution. While this strategy is straight forward, the bounds obtained are slightly worse than\nthose obtained by directly invoking Theorem 30, which is why we stick to the latter result in this work."
        },
        {
            "heading": "F Quantum and probabilistic algorithms",
            "text": "In this appendix we will detail the connection between statistical query learning via deterministic and random algorithms. Throughout the section we will refer by random algorithm to both classical probabilistic as well as quantum algorithms.\nThe randomized average case query complexity for -learning  with respect to the probability measure depends on the two parameters and , where\n\u2022 denotes the success probability with respect to the internal randomness of the learning algorithm and\n\u2022 denotes the fraction of distributions in  measured with respect to on which the learning algorithm must be successful.\nThe aim of this appendix is to bound the randomized average case query complexity for -learning  by (c.f. Theorem 34)\nq \u2265 2 \u22c5 ( \u2212 12) \u22c5 ( \u2212 PrP\u223c [dTV(P, Q) \u2264 + ])\nmax PrP\u223c [|P[ ] \u2212 Q[ ]| > ] . (98)\nThus, the randomized average case query complexity of -learning is bounded by the same bounds as the deterministic average case query complexity up to a prefactor 2( \u2212 1/2), which becomes trivial for \u2264 1/2.\nWe will follow the same strategy as in the deterministic case laid out in Appendix E. The main di erence is that we need a bound on the decision problem Problem 2 for random algorithms. The subtlety, why the arguments from Lemma 29 fail, is that a random algorithm does not need to make a distinguishing query to solve the problem. Rather, it may guess the correct solution using its internal randomness. The main technical ingredient of this Appendix is thus a result by Feldman which, rst bounding the probability of guessing correctly, bounds the statistical query complexity for Problem 2 which is stated as Lemma 33.\nTo begin with, we can follow the same strategy as before to reduce deciding to learning, also in the random setting.\nLemma 32: (Learning is as hard as deciding) Let  be a distribution class and let Q be such that dTV(P, Q) > + for all P \u2208 . Let 0 < \u2264 \u2264 1. Let  be an algorithm that -learns  from q many accurate statistical queries with probability over its internal randomness. Then there exists an algorithm that, with probability over its internal randomness, decides  versus Q from q + 1 many accurate statistical queries.\nProof. The proof is identical to that of Lemma 28. The only di erence is that the learner  only succeeds with probability , which leads to the decider only succeeding with probability . The\nreduction itself however is deterministic and, as such, does not change the statistics.\nWe now state the result by Feldman on the randomized statistical query complexity of Problem 2. For the sake of self consistency we provide the proof adapted to our notation.\nLemma 33: (Hardness of deciding. Taken from Theorem 3.9 from [Fel17]) Let  be a random algorithm that decides  versus Q with probability at least from q many accurate statistical queries. Then, for any measure over  it holds\nq \u2265 2 \u22c5 ( \u2212 12 )\nmax PrP\u223c [|P[ ] \u2212 Q[ ]| > ] . (99)\nProof. Let be an algorithm that decides vs. Q with probability over its internal randomness. We run  and, on every query return Q[ ] . Denote by 1,\u2026 , q the queries made. These queries then can be interpreted as random variables with respect to \u2019s randomness. Let P \u2208  and denote by\np(P ) = Pr  [\u2203i \u2236 |P[ i] \u2212 Q[ i]| > ] . (100)\nWe now show that p(P ) \u2265 2( \u2212 1/2): By the correctness of  the corresponding output will be \u201cP \u2208 \u201d with probability at most 1 \u2212 . For the sake of contradiction assume p(P ) < 2( \u2212 1/2). Thus, when run on P , for some valid answers  will still return \u201cP = Q\u201d with probability at least > 1 \u2212 p(P ) \u2212 (1 \u2212 ) > 1 \u2212 2 + 1 \u2212 1 + = 1 \u2212 . Since this probability is bounded by 1 \u2212 we nd a contradiction < . Thus p(P ) \u2265 2( \u2212 1/2).\nThe remainder now follows from the union bound as in Lemma 29:\n2( \u2212 1/2) \u2264 p(P ) = Pr ,P\u223c [\u2203i \u2236 |P[ i] \u2212 Q[ i]| > ]\n\u2264 q\n\u2211 i=1 Pr ,P\u223c [|P[ i] \u2212 Q[ i]| > ]\n\u2264 qmax Pr P\u223c [|P[ ] \u2212 Q[ ]| > ] .\n(101)\nThus, following Appendix E, we can state the main theorem of this appendix.\nTheorem 34: (Randomized average case query complexity of learning) Let  be a random algorithm for average case -learning  with respect to with parameters and from q many accurate statistical queries. Then for any Q it holds\nq + 1 \u2265 2 \u22c5 ( \u2212 12 ) \u22c5 ( \u2212 PrP\u223c [dTV(P, Q) \u2264 + ])\nmax PrP\u223c [|P[ ] \u2212 Q[ ]| > ] . (102)\nProof. The proof is identically to that of Theorem 30 using Lemma 32 and Lemma 33 instead of Lemma 28 and Lemma 29.\nFor the sake of context relating to the discussion in the end of Appendix E and [Fel17, Lemma C.2], we nish this appendix with two additional insights.\nNote 4: If we restrict the result to probabilistic algorithms we can follow the argument from [Fel17, Lemma C.2]: One can make the randomness explicit by writing the random algorithm  as an ensemble of deterministic algorithms {x} with x \u223c  the internal randomness. Then the randomized average case query complexity can be bounded by the deterministic average case query complexity replacing by \u22c5 . This yields\nq \u2265 \u22c5 \u2212 supD PrP\u223c [dTV(P, D) < ] max PrP\u223c [|P[ ] \u2212 Q[ ]| > ] , (103)\nwhere, for the sake of transparency, we used Feldmans bound stated as Lemma 31 for the deterministic reference bound.\nNote that Equation (103) is tight with respect to \u22c5 and the joint measure \u00d7. However, it has two disadvantages for our usecase. First, it only holds for classical probabilistic algorithms, but not for other random algorithms such as quantum algorithms. Second, we are interested in the average case hardness as in De nition 2. This means, we would like a statement that is tight with respect to with respect to only. Thus, to obtain the corresponding tight bound for quantum algorithms, we add the following lemma.\nLemma 35: Let  be a random algorithm for average case -learning  with respect to with parameters and from q many -accurate statistical queries. Then for any Q it holds\nq \u2265 2 \u22c5 ( \u2212 12 ) \u22c5 ( \u2212 supD PrP\u223c [dTV(P, D) < ])\nmax PrP\u223c [|P[ ] \u2212 Q[ ]| > ] , (104)\nwhere the sup is over all distributions with the same domain X and the max is over all functions \u2236 X \u2192 [\u22121, 1].\nProof. Assume  is a random algorithm that -learns  with respect to , and from q many -accurate statistical queries. We run  and answer each query by Q[ ]. Denote by 1,\u2026 , q the queries made and, without loss of generality, assume  returns the representation of some distribution D. Denote by \u2032 \u2286  the set on which, with probability at least , the algorithm is successful. Further, let p(P ) as in the proof of Lemma 33 and let\nD = { P \u2208 \u2032 \u2236 dTV(P, D) \u2265 } .\nSince p(P ) \u2265 2( \u2212 1/2) for any P \u2208 D \u2286 \u2032 we nd\n2( \u2212 1/2) \u2264 Pr P\u223c , [\u2203i , |P[ i] \u2212 Q[ i]| > \u2223 P \u2208 D]\n= PrP\u223c , [\u2203i , |P[ i] \u2212 Q[ i]| > ] (D) \u2264 PrP\u223c , [\u2203i , |P[ i] \u2212 Q[ i]| > ] \u2212 PrP\u223c [dTV(P, D) < ]\n\u2264 q\n\u2211 i=1 PrP\u223c , [|P[ i] \u2212 Q[ i]| > ] \u2212 PrP\u223c [dTV(P, D) < ] \u2264 q max PrP\u223c [|P[ ] \u2212 Q[ ]| > ] \u2212 PrP\u223c [dTV(P, D) < ] ,\n(105)\nwhere we used the de nition of the conditional probability, the bound on (D) similar to that on (Q) from the proof of Theorem 30 and the union bound. The claim then follows from taking the maximum over all distributions in order to estimate the unknown D.\nIt is easy to see that Lemma 35 is tight with respect to : The trivial algorithm that always returns D, where D is the distribution with the -ball of highest weight, will succeed with probability PrP\u223c [dTV(P, D) < ]. We conclude this appendix with a note similar to Note 3.\nNote 5: In general Lemma 35 gives the optimal lower bounds with respect to . However, in some cases directly bounding the weight of all -balls may not be convenient. The following appendix Appendix G gives a general recipe for obtaining such a bound just from the two ingredients used in Theorem 34: The maximally distinguishable fraction and the mass of the ball around the reference distribution. While this strategy is straight forward, the bounds obtained are slightly worse than those obtained by directly invoking Theorem 34, which is why we stick to the latter result in this work."
        },
        {
            "heading": "G Far from any xed distribution",
            "text": "In the main text, we obtained multiple \u201cfar-from-uniform\"-results for the output distributions of random circuits for di erent depth regimes. In this section, we show that random quantum circuits actually exhibit a more general property. Namely, their output distributions are with overwhelming probability far from any xed distribution. This result was stated in the main text as Informal Theorem 2. Here, we restate it formally and then go on to prove it.\nTheorem 36: (Formal version of Informal Theorem 2) Let C be the measure on U(2n) induced by local random quantum circuits of depth d . Then, there is a d \u2032 = O(n) such that at any depth d \u2265 d \u2032, for any \u2264 1/225, and for any distribution D over {0, 1}n it holds\nPr U\u223c C\n[dTV(PU , D) \u2265 ] \u2265 1 \u2212 c2\u2212n , (106)\nwhere c is a constant that can be bounded by c < 7 \u00d7 106 < 220.\nIn the following, letD denote the arbitrary but xed distribution as in the above theorem. We note\nthat to prove Theorem 36, we can distinguish two cases: Either D is itself close to uniform, then a far-from-uniform bound implies a far-from-D bound. In the other case, D is at least some distance away from the uniform distribution. As made explicit by the following lemma, the \u201cfar-fromany- xed-distribution\"-result for such D is implied by a bound on the maximal distinguishable fraction with respect to the uniform distribution, f = frac( , , ). In fact, the lemma holds not only for the uniform distribution but any choice of reference distribution Q.\nThus, a \u201cfar-from-any- xed-distribution\"-result follows from two ingredients: A \u201cfar-from-Q\" result and a bound on the maximally distinguishable fraction against Q, for any reference distribution Q. We happen to have calculated these bounds for the particular choice of Q =  already in the main text.\nLemma 37: Let , > 0, X be some domain and letQ \u2208 X be the reference distribution. Moreover, let D \u2208 X be such that\ndTV (Q, D) > + . (107)\nThen for any measure over X it holds\nPr P\u223c [dTV(P, D) < ] \u2264 frac( , Q, ) . (108)\nProof. Recall that by the variational characterization of the total variation distance it holds that dTV(Q, D) = maxT\u2282X |Q(T ) \u2212 D(T )|. Let S \u2286 X be such a set maximizing the total variation distance and denote by = 1S the characteristic function on S. This is, dTV(D, Q) = |D[ ] \u2212 Q[ ]|.\nBy the reverse triangle inequality for any P \u2032 \u2208 B (D) it then holds\n||P \u2032[ ] \u2212 Q[ ]|| \u2265 ||||P \u2032[ ] \u2212 D[ ]|| \u2212 |D[ ] \u2212 Q[ ]||| \u2265 ||dTV(Q, D) \u2212 dTV(P\n\u2032, D)|| > | + \u2212 | = ,\n(109)\nwhere we have used that |P \u2032[ ] \u2212 D[ ]| \u2264 dTV(P \u2032, D) < together with dTV(Q, D) > + > > dTV(P \u2032, D).\nHence, by Equation (109) it holds\nPr P\u223c [dTV(P, D) < ] \u2264 Pr P\u223c [|P[ ] \u2212 Q[ ]| > ] \u2264 frac( , Q, ) , (110)\nwhere the last inequality is due to the maximum over all functions in the de nition of frac( , Q, ) (De nition 3).\nApplying Lemma 37 to the choice of Q =  and using our bound for the maximally distinguishable fraction f against uniform from Lemma 7 in Section 4.1, we nd the following:\nCorollary 38: Let , > 0 and let D be any probability distribution over {0, 1}n satisfying\ndTV (D, ) > + (111)\nwhere  is the uniform distribution. Let C denote the measure over brickwork random quantum circuits as in De nition 5. Then, there is a d \u2032 = O(n) such that at any depth d \u2265 d \u2032 it holds\nPr U\u223c C\n[dTV(D, PU ) < ] \u2264 3\n2n 2 . (112)\nWe now complete the proof of Theorem 36 following the two-cases argument laid out above.\nProof of Theorem 36. Let D be an arbitrary distribution, let d large enough and let = 1/450 such that 3 = \u2032 = 1/150. We distinguish two cases:\n1. dTV(D, ) \u2264 2 ,\n2. dTV(D, ) > 2 .\nIn case 1, we have that\nPr U\u223c C [dTV(PU , D) < ] \u2264 Pr U\u223c C [dTV (PU , ) < \u2032] < 3200 \u22c5 2\u2212n \u2264 O (2\u2212n) (113)\nby our far-from-uniform result from the main text, namely Corollary 11.\nIn case 2, we have that dTV(D, ) > 2 . Setting = we can apply Corollary 38 to obtain\nPr U\u223c C [dTV(PU , D) < ] < 3 2n 2 = 607500 \u00d7 2\u2212n . (114)\nHence, for any distribution D, any \u2264 1/450 we nd\nPr U\u223c C\n[dTV(PU , D) < ] < 607500 \u00d7 2\u2212n = O (2\u2212n) . (115)\nFinally, we summarize the connection between the -ball with the largest weight and the maximally distinguishable fraction as advertised at the end of Appendix E as follows.\nLemma 39: Let , > 0, X be some domain, be a measure over X and Q \u2208 X an arbitrary distribution. Then for any D \u2208 X it holds\nPr P\u223c [dTV(P, D) < ] \u2264 max\n{ frac( , Q, ) , Pr\nP\u223c [dTV(P, Q) \u2264 2 + ]\n} . (116)\nProof. We consider two cases. In case dTV(D, Q) > + we obtain the contribution from frac( , Q, ) via Lemma 37. So consider the case dTV(D, Q) \u2264 + . In this case we can bound\nPr P\u223c [dTV(P, D) < ] \u2264 Pr P\u223c [dTV(P, Q) \u2264 2 + ] , (117)\nwhich yields the claim."
        }
    ],
    "title": "On the average-case complexity of learning output distributions of quantum circuits",
    "year": 2023
}