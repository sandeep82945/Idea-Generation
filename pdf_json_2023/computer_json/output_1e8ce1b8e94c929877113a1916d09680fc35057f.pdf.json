{
    "abstractText": "We consider contextual bandit problems with knapsacks [CBwK], a problem where at each round, a scalar reward is obtained and vector-valued costs are suffered. The learner aims to maximize the cumulative rewards while ensuring that the cumulative costs are lower than some predetermined cost constraints. We assume that contexts come from a continuous set, that costs can be signed, and that the expected reward and cost functions, while unknown, may be uniformly estimated\u2014 a typical assumption in the literature. In this setting, total cost constraints had so far to be at least of order T , where T is the number of rounds, and were even typically assumed to depend linearly on T . We are however motivated to use CBwK to impose a fairness constraint of equalized average costs between groups: the budget associated with the corresponding cost constraints should be as close as possible to the natural deviations, of order \u221a T . To that end, we introduce a dual strategy based on projected-gradient-descent updates, that is able to deal with total-cost constraints of the order of \u221a T up to poly-logarithmic terms. This strategy is more direct and simpler than existing strategies in the literature. It relies on a careful, adaptive, tuning of the step size. 1 Setting, literature review, and main contributions We consider contextual bandits with knapsacks [CBwK], a setting where at each round t \u2a7e 1, the learner, after observing some context xt \u2208 X , where X \u2286 R, picks an action at \u2208 A in a finite set A. We do not impose the existence of a null-cost action. Contexts are independently drawn according to a distribution \u03bd. The learner may pick at at random according to a probability distribution, denoted by \u03c0t(xt) = ( \u03c0t,a(xt) ) a\u2208A for consistency with the notion of policy defined later in Section 2. The action at played leads to some scalar reward rt \u2208 [0, 1] and some signed vector-valued cost ct \u2208 [\u22121, 1]. Actually, rt and ct are generated independently at random in a way such that the conditional expectations of rt and ct given the past, xt, and at, equal r(xt, at) and c(xt, at), respectively. We denoted here by r : X \u00d7 A \u2192 [0, 1] and c = (c1, . . . , cd) : X \u00d7 A \u2192 [\u22121, 1] the unknown expected-reward and expected-cost functions. The modeling and the estimation of r 37th Conference on Neural Information Processing Systems (NeurIPS 2023). ar X iv :2 30 5. 15 80 7v 2 [ st at .M L ] 2 6 O ct 2 02 3 BOX A: CONTEXTUAL BANDITS WITH KNAPSACKS [CBWK] Known parameters: finite action set A; context set X \u2286 R; number T of rounds; vector of average cost constraints B \u2208 [0, 1] Unknown parameters: context distribution \u03bd on X ; scalar expected-reward function r : X \u00d7 A \u2192 [0, 1] and vector-valued expected-cost function c : X \u00d7 A \u2192 [\u22121, 1], both enjoying some modeling allowing for estimation, see Section 2.2 For rounds t = 1, 2, 3, . . . , T : 1. Context xt \u223c \u03bd is drawn independently of the past; 2. Learner observes xt and picks an action at \u2208 A, possibly at random according to a distribution \u03c0t(xt) = ( \u03c0t,a(xt) ) a\u2208A; 3. Learner gets a reward rt \u2208 [0, 1] with conditional expectation r(xt, at) and suffers constraints ct with conditional expectations c(xt, at). Goal: Maximize \u2211 t\u2a7dT rt while ensuring \u2211",
    "authors": [
        {
            "affiliations": [],
            "name": "Evgenii Chzhen"
        },
        {
            "affiliations": [],
            "name": "Christophe Giraud"
        },
        {
            "affiliations": [],
            "name": "Zhen Li"
        }
    ],
    "id": "SP:7a11fe31a04415e082df89d79cb36d6afbe7ce6b",
    "references": [
        {
            "authors": [
                "Y. Abbasi-Yadkori",
                "D. P\u00e1l",
                "C. Szepesv\u00e1ri"
            ],
            "title": "Improved algorithms for linear stochastic bandits",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS\u201911),",
            "year": 2011
        },
        {
            "authors": [
                "S. Agrawal",
                "N. Devanur"
            ],
            "title": "Linear contextual bandits with knapsacks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS\u201916),",
            "year": 2016
        },
        {
            "authors": [
                "S. Agrawal",
                "N.R. Devanur",
                "L. Li"
            ],
            "title": "An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives",
            "venue": "In Proceedings of the 29th Annual Conference on Learning Theory (COLT\u201916),",
            "year": 2016
        },
        {
            "authors": [
                "R. Ai",
                "Z. Chen",
                "X. Deng",
                "Y. Pan",
                "C. Wang",
                "M. Yang"
            ],
            "title": "On the re-solving heuristic for (binary) contextual bandits with knapsacks, November 2022",
            "year": 2022
        },
        {
            "authors": [
                "A. Badanidiyuru",
                "R. Kleinberg",
                "A. Slivkins"
            ],
            "title": "Bandits with knapsacks",
            "venue": "In IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS\u201913),",
            "year": 2013
        },
        {
            "authors": [
                "A. Badanidiyuru",
                "J. Langford",
                "A. Slivkins"
            ],
            "title": "Resourceful contextual bandits",
            "venue": "In Proceedings of the 27th Conference on Learning Theory (COLT\u201914),",
            "year": 2014
        },
        {
            "authors": [
                "A. Badanidiyuru",
                "R. Kleinberg",
                "A. Slivkins"
            ],
            "title": "Bandits with global convex constraints and objective",
            "venue": "Journal of the ACM,",
            "year": 2018
        },
        {
            "authors": [
                "N. Cesa-Bianchi",
                "G. Lugosi",
                "G. Stoltz"
            ],
            "title": "Minimizing regret with label-efficient prediction",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2005
        },
        {
            "authors": [
                "A. Chohlas-Wood",
                "M. Coots",
                "H. Zhu",
                "E. Brunskill",
                "S. Goel"
            ],
            "title": "Learning to be fair: A consequentialist approach to equitable decision-making, 2021",
            "year": 2021
        },
        {
            "authors": [
                "E. Chzhen",
                "C. Giraud",
                "G. Stoltz"
            ],
            "title": "A unified approach to fair online learning via Blackwell approachability",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS\u201921),",
            "year": 2021
        },
        {
            "authors": [
                "L. Faury",
                "M. Abeille",
                "C. Calauzenes",
                "O. Fercoq"
            ],
            "title": "Improved optimistic algorithms for logistic bandits",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning (ICML\u201920),",
            "year": 2020
        },
        {
            "authors": [
                "S. Filippi",
                "O. Cappe",
                "A. Garivier",
                "C. Szepesv\u00e1ri"
            ],
            "title": "Parametric bandits: The generalized linear case",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS\u201910),",
            "year": 2010
        },
        {
            "authors": [
                "David A Freedman"
            ],
            "title": "On tail probabilities for martingales",
            "venue": "The Annals of Probability,",
            "year": 1975
        },
        {
            "authors": [
                "Y. Han",
                "J. Zeng",
                "Y. Wangy",
                "Y. Xiang",
                "J. Zhang"
            ],
            "title": "Optimal contextual bandits with knapsacks under realizability via regression oracles, October 2022",
            "year": 2022
        },
        {
            "authors": [
                "N. Immorlica",
                "K.A. Sankararaman",
                "R. Schapire",
                "A. Slivkins"
            ],
            "title": "Adversarial bandits with knapsacks",
            "venue": "IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS\u201919),",
            "year": 2019
        },
        {
            "authors": [
                "T. Lattimore",
                "C. Szepesv\u00e1ri"
            ],
            "title": "Bandit Algorithms",
            "year": 2020
        },
        {
            "authors": [
                "Z. Li",
                "G. Stoltz"
            ],
            "title": "Contextual bandits with knapsacks for a conversion model",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS\u201922),",
            "year": 2022
        },
        {
            "authors": [
                "D. Luenberger"
            ],
            "title": "Optimization by Vector Space Methods. Series in Decision and Control",
            "year": 1969
        },
        {
            "authors": [
                "A. Slivkins",
                "K.A. Sankararaman",
                "D.J. Foster"
            ],
            "title": "Contextual bandits with packing and covering constraints: A modular Lagrangian approach via regression, November 2022",
            "year": 2022
        },
        {
            "authors": [
                "A.B. Tsybakov"
            ],
            "title": "Introduction to Nonparametric Estimation",
            "year": 2008
        },
        {
            "authors": [
                "M. Zinkevich"
            ],
            "title": "Online convex programming and generalized infinitesimal gradient ascent",
            "venue": "In Proceedings of the 20th International Conference on Machine Learning",
            "year": 2003
        },
        {
            "authors": [
                "\u2a7e \u03b8"
            ],
            "title": "The dual function \u03c6 is defined as follows (Luenberger",
            "year": 1969
        },
        {
            "authors": [
                "Cesa-Bianchi"
            ],
            "title": "Corollary 16] involving the sum of the conditional variances (and not only",
            "year": 2005
        },
        {
            "authors": [],
            "title": "First, a classical lemma in CBwK (see, e.g., Agrawal and Devanur, 2016, Lemma 1) indicates that a sequence of adaptive cannot perform better than an optimal static policy",
            "year": 2016
        },
        {
            "authors": [
                "Li",
                "Stoltz"
            ],
            "title": "2022) consists in not directly dealing with KKT constraints (which, in addition, imposed the finiteness of X ) but in only relating optimization problems",
            "year": 2022
        },
        {
            "authors": [
                "D BOX"
            ],
            "title": "PRIMAL CBWK STRATEGY, GENERALIZED FROM LI AND STOLTZ",
            "year": 2022
        },
        {
            "authors": [
                "\u2208 R"
            ],
            "title": "The learner will estimate the reward function r by estimating \u03bc\u22c6. Reward estimation. We deal with a logistic model and follow the methodology described in Li and Stoltz [2022]; see Modeling 2 in Section 2.2. In particular, the parameters \u03bc\u22c6 are estimated, after each round t \u2a7e 1, thanks to the maximum likelihood estimator",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "\u221a T . To that end, we introduce\na dual strategy based on projected-gradient-descent updates, that is able to deal with total-cost constraints of the order of \u221a T up to poly-logarithmic terms. This strategy is more direct and simpler than existing strategies in the literature. It relies on a careful, adaptive, tuning of the step size."
        },
        {
            "heading": "1 Setting, literature review, and main contributions",
            "text": "We consider contextual bandits with knapsacks [CBwK], a setting where at each round t \u2a7e 1, the learner, after observing some context xt \u2208 X , where X \u2286 Rn, picks an action at \u2208 A in a finite set A. We do not impose the existence of a null-cost action. Contexts are independently drawn according to a distribution \u03bd. The learner may pick at at random according to a probability distribution, denoted by \u03c0t(xt) = ( \u03c0t,a(xt) ) a\u2208A for consistency with the notion of policy defined later in Section 2. The action at played leads to some scalar reward rt \u2208 [0, 1] and some signed vector-valued cost ct \u2208 [\u22121, 1]d. Actually, rt and ct are generated independently at random in a way such that the conditional expectations of rt and ct given the past, xt, and at, equal r(xt, at) and c(xt, at), respectively. We denoted here by r : X \u00d7 A \u2192 [0, 1] and c = (c1, . . . , cd) : X \u00d7 A \u2192 [\u22121, 1]d the unknown expected-reward and expected-cost functions. The modeling and the estimation of r\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 5.\n15 80\n7v 2\n[ st\nat .M\nBOX A: CONTEXTUAL BANDITS WITH KNAPSACKS [CBWK]\nKnown parameters: finite action set A; context set X \u2286 Rn; number T of rounds; vector of average cost constraints B \u2208 [0, 1]d\nUnknown parameters: context distribution \u03bd on X ; scalar expected-reward function r : X \u00d7 A \u2192 [0, 1] and vector-valued expected-cost function c : X \u00d7 A \u2192 [\u22121, 1]d, both enjoying some modeling allowing for estimation, see Section 2.2\nFor rounds t = 1, 2, 3, . . . , T : 1. Context xt \u223c \u03bd is drawn independently of the past; 2. Learner observes xt and picks an action at \u2208 A, possibly at random according to a\ndistribution \u03c0t(xt) = ( \u03c0t,a(xt) ) a\u2208A;\n3. Learner gets a reward rt \u2208 [0, 1] with conditional expectation r(xt, at) and suffers constraints ct with conditional expectations c(xt, at).\nGoal: Maximize \u2211 t\u2a7dT rt while ensuring \u2211 t\u2a7dT ct \u2a7d TB\nand c are discussed in Section 2.2. The aim of the learner is to maximize the sum of rewards (or equivalently, to minimize the regret defined in Section 2) while controlling cumulative costs. More precisely, we denote by B = (B1, . . . , Bd) \u2208 [0, 1]d the normalized (i.e., average per-instance) cost constraints, which may depend on the coordinates. The number T of rounds is known, and the learner must play in a way such that c1 + . . .+ cT \u2a7d TB. The example of Section 2.1 illustrates how this setting allows for controlling costs in absolute values. The setting described is summarized in Box A.\nOverview of the literature review. Contextual bandits with knapsacks [CBwK] is a setting that is a combination of the problems of contextual bandits (where only rewards are obtained, and no costs: see, among others, Lattimore and Szepesv\u00e1ri, 2020, Chapter 18 for a survey of this rich area) and of bandits with knapsacks (without contexts, as initially introduced by Badanidiyuru et al., 2013, 2018). The first approaches to CBwK (Badanidiyuru et al., 2014, Agrawal et al., 2016) relied on no specific modeling of the rewards and costs, and made the problem tractable by using as a benchmark a finite set of so-called static policies (but picking this set is uneasy, as noted by Agrawal and Devanur, 2016). Virtually all subsequent approaches to CBwK thus introduced structural assumptions in one way or the other. The simplest modelings are linear dependencies of expected rewards and costs on the contexts (Agrawal and Devanur, 2016) and a logistic conversion model (Li and Stoltz, 2022). The problem of CBwK attracted attention in recent past: we discuss and contrast the contributions by Chohlas-Wood et al. [2021], Slivkins et al. [2022], Han et al. [2022], Ai et al. [2022], Li and Stoltz [2022] after stating our main contributions.\nOverview of our main contributions. Each contribution calls or allows for the next one. 1. This article revisits the CBwK approach by Chohlas-Wood et al. [2021] to a problem of fair spending of resources between groups while maximizing some total utility. Fairness constraints require to deal with signed costs and with possibly small total-cost constraints (typically close to T 1/2), while having no null-cost action at disposal. 2. We therefore provide a new CBwK strategy dealing with these three issues, the most significant being handling cost constraint TB as small as T 1/2+\u03b5, breaking the state-of-the-art T 3/4 barrier for CBwK with continuous contexts, while preserving attractive computational efficiency. 3. This new strategy is a direct, simple, explicit, dual strategy, relying on projected-gradient-descent updates (instead of using existing general algorithms as subroutines). We may perform an ad hoc analysis. The latter leads to refined regret bounds, in terms of the norms of some optimal dual variables. We also discuss the optimality of the results achieved by offering a proof scheme for problem-dependent (not only minimax) regret lower bounds."
        },
        {
            "heading": "1.1 Detailed literature review",
            "text": "We now contrast our main contributions to the existing literature.\nCBwK and fairness: missing tools. In the setting of Chohlas-Wood et al. [2021], there is a budget constraint on the total spendings on top of the wish of sharing out these spendings among groups (possibly favoring to some maximal extent some groups that are more in need). However,\nChohlas-Wood et al. [2021] incorporated the fairness constraints in the reward function (through some Lagrangian penalty) instead of doing so in the vector-cost function, which does not seem the most natural way to proceed. One reason is that (see a discussion below) the total cost constraints TB were so far typically assumed to be linear, which is undesirable in fairness applications, where one wishes that TB is sublinear\u2014and actually, is sometimes as close as possible to the natural \u221a T deviations suffered even in a perfectly fair random allocation scheme. Also, there is no null-cost action and costs are signed\u2014two twists to the typical settings of CBwK that only Slivkins et al. [2022] and our approach deal with, to the best of our knowledge. We describe our alternative modeling of this fairness problem in Section 2.1.\nOrders of magnitude for total-cost constraints TB. The literature so far mostly considered linear total-cost constraints TB (see, e.g., Han et al., 2022 or Ai et al., 2022 among recent references), with two series of exceptions: (i) the primal strategy by Li and Stoltz [2022] handling total-cost contraints of the order of \u221a T up to poly-logarithmic terms but critically assuming the finiteness of the context set X ; and (ii) the two-stage dual strategies of Agrawal and Devanur [2016], Han et al. [2022] handling T 3/4 total-cost constraints. These two-stage strategies use \u221a T preliminary rounds to learn some key hyperparameter Z and then transform strategies that work with linear total-cost constraints into strategies that may handle total-cost constraints larger than T 3/4; see discussions after Lemma 2. In contrast, we provide a general theory of small cost constraints for continuous context sets, while obtaining similar regret bounds as the literature does: for some components j, we may have TBj \u226a T 3/4 (actually, any rate TBj \u226b T 1/2+\u03b5 is suitable for a sublinear regret), while for other components k, the total-cost constraints TBk may be linearly large.\nTypical CBwK strategies: primal approaches suffer from severe limitations. CBwK is typically solved through dual approaches, as the latter basically rely on learning a finite-dimensional parameter given by the optimal dual variables \u03bb\u22c6 (see the beginning of Section 3), while primal approaches rely on learning the distribution \u03bd of the contexts in L1\u2013distance. This may be achieved in some cases, like finiteness of the context set X (Li and Stoltz, 2022, Ai et al., 2022) or, at the cost of degraded regret bounds and under additional assumptions on \u03bd when resorting to L1\u2013density-estimation techniques (Ai et al., 2022). On top of these strong limitations, such primal approaches are also typically computationally inefficient as they require the computation of expectations over estimates of \u03bd.\nTypical CBwK strategies: dual approaches are modular and somewhat indirect. Typical dual approaches in CBwK take two general forms. One, illustrated in Slivkins et al. [2022] (extending the non-contextual approach by Immorlica et al., 2019), takes a game-theoretic approach by identifying the primal-dual formulation (4) as some minimax equilibrium, which may be learned by separate regret minimization of a learner picking actions at and an opponent picking dual variables \u03bbt. The second approach (Agrawal and Devanur, 2016, Han et al., 2022) learns more directly the \u03bbt via some online convex optimization algorithm fed with the suffered costs ct; this second approach however requires, at the stage of picking at, a suitable bound Z on the norms of the \u03bbt.\nThe dual strategies discussed above are modular and indirect as they all consist of using as building blocks some general-purpose strategies. We rather propose a more direct dual approach, tailored to our needs. (We also believe that it is simpler and more elegant.) Our strategy picks the arm at that maximizes the Lagrangian penalization of rewards by costs through the current \u03bbt\u22121 (see Step 2 in Box B), as also proposed by Agrawal and Devanur [2016] (while Han et al., 2022 add some randomization to this step), and then, performs some direct, explicit, projected-gradient-descent update on \u03bbt\u22121 to obtain \u03bbt, with step size \u03b3. We carefully and explicitly tune \u03b3 (in a sequential fashion, via regimes) to achieve our goals. Such fine-tuning is more difficult to perform with approaches relying on the black-box use of subroutines given by existing general-purpose strategies.\nRegret (upper and lower) bounds typically proposed in the CBwK literature. A final advantage of our more explicit strategy is that we master each piece of its analysis: while not needing a nullcost action, we provide refined regret bounds that go beyond the typical (OPT(r, c,B)/minB) \u221a T bound offered by the literature so far (see, among others, Agrawal and Devanur, 2016, Li and Stoltz, 2022, Han et al., 2022), where OPT(r, c,B) denotes the expected reward achieved by the optimal static policy (see Section 2 for definitions). Namely, our bounds are of the order of \u2225\u03bb\u22c6\u2225 \u221a T ,\nwhere \u03bb\u22c6 is the optimal dual variable; we relate the norm of the latter to quantities of the form( OPT(r, c,B)\u2212 OPT(r, c, \u03b1B) ) / ( (1\u2212 \u03b1)minB ) , for some \u03b1 < 1. Again, we may do so without the existence of a null-cost action, but when one such action is available, we may take \u03b1 = 0 in the interpretation of the bound. We also offer a proof scheme for lower bounds in Section 4 and\nexplain why \u2225\u03bb\u22c6\u2225 \u221a T appears as the correct target. We compare therein our problem-dependent lower bound-approach to the minimax ones of Han et al. [2022] and Slivkins et al. [2022]."
        },
        {
            "heading": "1.2 Outline",
            "text": "In Section 2, we further describe the problem of CBwK (by defining the regret and recalling how r and c may be estimated) and state our motivating example of fairness. Section 3 is devoted to our new dual strategy, which we analyze first with a fixed step size \u03b3, for which we move next to an adaptive version, and whose bounds we finally discuss. Section 4 offers a proof scheme for regret lower bounds and lists some limitations of our approach, mostly relative to optimality."
        },
        {
            "heading": "2 Further description of the setting, and statement of our motivating example",
            "text": "We define a static policy as a mapping \u03c0 : X \u2192 P(A), where P(A) denotes the set of probability distributions over A. We denote by \u03c0 = (\u03c0a)a\u2208A the components of \u03c0. We let EX\u223c\u03bd indicate that the expectation is taken over the random variable X with distribution \u03bd. In the sequel, the inequalities \u2a7d or \u2a7e between vectors will mean pointwise satisfaction of the corresponding inequalities. Assumption 1. The contextual bandit problem with knapsacks (r, c,B\u2032, \u03bd) is feasible if there exists a stationary policy \u03c0 such that\nEX\u223c\u03bd [\u2211 a\u2208A c(X, a)\u03c0a(X) ] \u2a7d B\u2032 .\nWe denote the average cost constraints by B\u2032 in the assumption above as in Section 3, we will actually require feasibility for average cost constraints B\u2032 < B, not just for B.\nIn a feasible problem (r, c,B\u2032, \u03bd), the optimal static policy \u03c0\u22c6 is defined as the policy \u03c0 : X \u2192 P(A) achieving\nOPT(r, c,B\u2032) = sup \u03c0\n{ EX\u223c\u03bd [\u2211 a\u2208A r(X, a)\u03c0a(X) ] : EX\u223c\u03bd [\u2211 a\u2208A c(X, a)\u03c0a(X) ] \u2a7d B\u2032 } . (1)\nMaximizing the sum of the rt amounts to minimizing the regret RT = T OPT(r, c,B\u2032)\u2212 T\u2211\nt=1\nrt ."
        },
        {
            "heading": "2.1 Motivating example: fairness\u2014equalized average costs between groups",
            "text": "This example is inspired from Chohlas-Wood et al. [2021] (who did not provide a strategy to minimize regret), and features a total budget constraint TBtotal together with fairness constraints on how the budget is spent among finitely many subgroups, whose set is denoted by G. We assume that the contexts xt include the group index gr(xt) \u2208 G, which means, in our setting, that the learner accesses this possibly sensitive attribute before making the predictions (the so-called awareness framework in the fairness literature). Each context\u2013action pair (x, a) \u2208 X \u00d7A, on top of leading to some expected reward r(x, a), also corresponds to some spendings cspd(x, a). We recall that we denoted that rt and ct (here, this is a scalar for now) the individual payoffs and costs obtained at round t; they have conditional expectations r(xt, at) and cspd(xt, at). We want to trade off the differences in average spendings among groups with the total reward achieved: larger differences lead to larger total rewards but generate feelings of inequity or of public money not optimally used. (Chohlas-Wood et al. [2021] consider a situation where unfavored groups should get slightly more spendings than favored groups.)\nFormally, we denote by gr(x) \u2208 G the group to which a given context x belongs and introduce a tolerance factor \u03c4 , which may depend on T . We issue a simplifying assumption: while the distribution \u03bd of the contexts is complex and is unknown, the respective proportions \u03b3g = \u03bd { x : gr(x) = g } of the sub-groups may be known.1 The total-budget and fairness constraints then read: T\u2211\nt=1\nct \u2a7d TBtotal and \u2200g \u2208 G,\n\u2223\u2223\u2223\u2223\u2223 1T\u03b3g T\u2211\nt=1\nct 1{gr(xt)=g} \u2212 1\nT T\u2211 t=1 ct \u2223\u2223\u2223\u2223\u2223 \u2a7d \u03c4 , 1This simplification is not unheard of in the fairness literature; see, for instance, Chzhen et al. [2021]. It amounts to having a reasonable knowledge of the breakdown of the population into subgroups. We use this assumption to have an easy rewriting of the fairness constraints in the setting of Box A. The knowledge of the \u03b3g is key for determining the average-constraint vector B.\nwhich corresponds, in the setting of Box A, to the following vector-valued expected-cost function c, with d = 1 + 2|G| components and with values in [\u22121, 1]1+2|G|: c(x, a) =(\ncspd(x, a), ( cspd(x, a)1{gr(x)=g} \u2212 \u03b3gcspd(x, a), \u03b3gcspd(x, a)\u2212 cspd(x, a)1{gr(x)=g} ) g\u2208G ) as well as to the average-constraint vector B = ( Btotal, ( \u03b3g \u03c4, \u03b3g \u03c4 ) g\u2208G ) .\nThe regimes we have in mind, and that correspond to the public-policy issues reported by ChohlasWood et al. [2021], are that the per-instance budget Btotal is larger than a positive constant, i.e., a constant fraction of the T individuals may benefit from some costly action(s), while the fairness threshold \u03c4 must be small, and even, as small as possible. Because of central-limit-theorem fluctuations, a minimal value of the order of 1/ \u221a T , or slightly larger (up to logarithmic terms, say), has to be considered for \u03c4 . The salient point is that the components of B may therefore be of different orders of magnitude, some of them being as small as 1/ \u221a T , up to logarithmic terms. More details on this example and numerical simulations about it are provided in Section 5 and Appendix G."
        },
        {
            "heading": "2.2 Modelings for, and estimation of, expected-reward and expected-cost functions",
            "text": "As is common in the literature (see Han et al., 2022 and Slivkins et al., 2022, who use the terminology of regression oracles), we sequentially estimate the functions r and c and assume that we may do so in some uniform way, e.g., because of the underlying structure assumed. Note that the estimation procedure of Assumption 2 below does not force any specific choice of actions, it is able to exploit actions at picked by the main strategy (this is what the \u201cotherwise\u201d is related to therein). We denote by r\u0302t : X \u00d7A \u2192 [0, 1] and c\u0302t : X \u00d7A \u2192 [\u22121, 1]d the estimations built based on (xs, as, rs, cs)s\u2a7dt, for t \u2a7e 1. We also assume that initial estimations r\u03020 and c\u03020, based on no data, are available. We denote by 1 the column-vector (1, . . . , 1). Assumption 2. There exists an estimation procedure such that for all individual sequences of contexts x1,x2, . . . and of actions a1, a2, . . . played otherwise, there exist known error functions \u03b5t : X \u00d7A\u00d7 (0, 1] \u2192 [0, 1], relying each on the pairs (xs, as)s\u2a7dt, where t = 0, 1, 2, . . ., ensuring that for all \u03b4 \u2208 (0, 1], with probability at least 1\u2212 \u03b4,\n\u22000 \u2a7d t \u2a7d T, \u2200x \u2208 X , \u2200a \u2208 A, \u2223\u2223r\u0302t(x, a)\u2212 r(x, a)\u2223\u2223 \u2a7d \u03b5t(x, a, \u03b4)\nand \u2223\u2223c\u0302t(x, a)\u2212 c(x, a)\u2223\u2223 \u2a7d \u03b5t(x, a, \u03b4)1 ,\nwhere we assume in addition that \u03b2T,\u03b4 def = T\u2211 t=1 \u03b5t\u22121(xt, at, \u03b4) = O (\u221a T ln T \u03b4 ) .\nThis assumption is satisfied at least for the two modelings discussed below, which are both of the form: for known transfer functions \u03c6, link functions \u03a6, and normalization function \u03b7, for some unknown finite-dimensional parameters \u00b5\u22c6, \u03b8\u22c6,1, . . . ,\u03b8\u22c6,d,\n\u2200x \u2208 X , \u2200a \u2208 A, r(x, a) = \u03b7r(a,x) \u03a6r ( \u03c6r(x, a) T \u00b5\u22c6 ) ,\nand for all 1 \u2a7d i \u2a7d d, ci(x, a) = \u03b7c,i(a,x) \u03a6c,i ( \u03c6c,i(x, a) T \u03b8\u22c6,i ) .\nSee also the exposition by Han et al. [2022, Section 3.3].\nBased on Assumption 2 and given the ranges [0, 1] for rewards and [\u22121, 1]d for costs, we may now define the following (clipped) upper- and lower-confidence bounds : given a confidence level 1\u2212 \u03b4 where \u03b4 \u2208 (0, 1], for all t \u2a7d T , for all x \u2208 X and a \u2208 A,\nr\u0302 ucb\u03b4,t (x, a) def = clip [ r\u0302t(x, a)+\u03b5t(x, a, \u03b4) ]1 0 and c\u0302 lcb\u03b4,t (x, a) def = clip [ c\u0302t(x, a)\u2212\u03b5t(x, a, \u03b4)1 ]1 \u22121 , (2)\nwhere we define clipping by clip[x]u\u2113 = min { max{x, \u2113}, u } for a scalar value x \u2208 R and lower and upper bounds \u2113 and u, and apply clipping component-wise to vectors. Under Assumption 2, we have that for all \u03b4 \u2208 (0, 1], with probability at least 1\u2212 \u03b4,\n\u22000 \u2a7d t \u2a7d T, \u2200x \u2208 X , \u2200a \u2208 A, r(x, a) \u2a7d r\u0302 ucb\u03b4,t (x, a) \u2a7d r(x, a) + 2\u03b5t(x, a, \u03b4) and c(x, a)\u2212 2\u03b5t(x, a, \u03b4)1 \u2a7d c\u0302 lcb\u03b4,t (x) \u2a7d c(x, a) . (3)\nDoing so, we have optimistic estimates of rewards (they are larger than the actual expected rewards) and of costs (they are smaller than the actual expected costs) for all actions a, while for actions at+1 played, and only these, we will also use the control from the other side, which will lead to manageable sums of \u03b5t(xt+1, at+1, \u03b4), given the control on \u03b2T,\u03b4 by Assumption 2.\nModeling 1: Linear modeling. Agrawal and Devanur [2016] (see also Li and Stoltz, 2022, Appendix E) consider the case of a linear modeling where \u03b7r = \u03b71 = . . . = \u03b7d \u2261 1 and \u03a6 is the identity (together with assumptions on the range of \u03c6 and on the norms of \u00b5\u22c6 and the \u03b8\u22c6,i). The LinUCB strategy by Abbasi-Yadkori et al. [2011] may be slightly adapted (to take care of the existence of a transfer function \u03c6 depending on the actions taken) to show that Assumption 2 holds; see Li and Stoltz [2022, Appendix E, Lemma 3].\nModeling 2: Conversion model based on logistic bandits. Li and Stoltz [2022, Sections 1-5] discuss the case where \u03a6(x) = 1/(1 + e\u2212x) for rewards and costs. The Logistic-UCB1 algorithm of Faury et al. [2020] (see also an earlier version by Filippi et al., 2010) may be slightly adapted (again, because of \u03c6) to show that Assumption 2 holds; see Li and Stoltz [2022, Lemma 1 in Appendix B, as well as Appendix C]."
        },
        {
            "heading": "3 New, direct, dual strategy for CBwK",
            "text": "Our strategy heavily relies on the following equalities between the primal and the dual values of the problem considered. The underlying justifications are typically omitted in the literature (see, e.g., Slivkins et al., 2022, end of Section 2); we detail them carefully in Appendix A. We introduce dual variables \u03bb \u2208 [0,+\u221e)d, denote by \u27e8 \u00b7 , \u00b7 \u27e9 the inner product in Rd, and we have, provided that the problem (r, c,B\u2032, \u03bd) is feasible for some B\u2032 < B:\nOPT(r, c,B) = sup \u03c0:X\u2192P(A) inf \u03bb\u2a7e0\nEX\u223c\u03bd [\u2211 a\u2208A r(X, a)\u03c0a(X) + \u2329 \u03bb, B \u2212 \u2211 a\u2208A c(X, a)\u03c0a(X) \u232a]\n= min \u03bb\u2a7e0 sup \u03c0:X\u2192P(A)\nEX\u223c\u03bd [\u2211 a\u2208A \u03c0a(X) ( r(X, a)\u2212 \u2329 c(X, a)\u2212B, \u03bb \u232a)]\n= min \u03bb\u2a7e0 EX\u223c\u03bd [ max a\u2208A { r(X, a)\u2212 \u2329 c(X, a)\u2212B, \u03bb \u232a}] . (4)\nWe denote by \u03bb\u22c6B a vector \u03bb \u2a7e 0 achieving the minimum in the final equality. Virtually all previous contributions to (contextual) bandits with knapsacks learned the dual optimal \u03bb\u22c6 in some way; see the discussion in Main contribution #3 in Section 1. Our strategy is stated in Box B. A high-level description is that it replaces (Step 2) the ideal dual problem above by an empirical version, with estimates of r and C, with the expectation over X \u223c \u03bd replaced by a point evaluation at xt, and that it sequentially chooses some \u03bbt based on projected-gradient-descent steps (Step 4 uses the subgradient of the function in \u03bb defined in Step 2). The gradient-descent steps are performed based on fixed step sizes \u03b3 in Section 3.1, and Section 3.2 will explain how to sequentially set the step sizes based on data. We also take some margin b1 on B.\nBOX B: PROJECTED GRADIENT DESCENT FOR CBWK WITH FIXED STEP SIZE\nInputs: step size \u03b3 > 0; number of rounds T ; confidence level 1 \u2212 \u03b4; margin b on the average constraints; estimation procedure and error functions \u03b5t of Assumption 2; optimistic estimates (2)\nInitialization: \u03bb0 = 0; initial estimates r\u0302 ucb\u03b4,0 and c\u0302 lcb \u03b4,0\nFor rounds t = 1, 2, 3, . . . , T : 1. Observe the context xt; 2. Pick an action at \u2208 A achieving\nmax a\u2208A\n{ r\u0302 ucb\u03b4,t\u22121(xt, a)\u2212 \u2329 c\u0302 lcb\u03b4,t\u22121(xt, a)\u2212 (B \u2212 b1), \u03bbt\u22121 \u232a} ;\n3. Observe the payoff rt and the costs ct; 4. Compute \u03bbt = ( \u03bbt\u22121 + \u03b3 ( c\u0302 lcb\u03b4,t\u22121(xt, at)\u2212 (B \u2212 b1) )) + ; 5. Compute the estimates r\u0302 ucb\u03b4,t and c\u0302 lcb \u03b4,t ."
        },
        {
            "heading": "3.1 Analysis for a fixed step size \u03b3",
            "text": "The strategy of Box B is analyzed through two lemmas. We let minv = min{v1, . . . , vd} > 0 denote the smallest component of a vector v \u2208 (0,+\u221e)d and take a margin b in (0,minB). We assume that the problem is feasible for some B\u2032 < B \u2212 b1, and denote by \u03bb\u22c6B\u2212b1 the vector achieving the minimum in (4) when the per-round cost constraint is B \u2212 b1. We use \u2225 \u00b7 \u2225 for the Euclidean norm. The bounds of Lemmas 1 and 2 critically rely on \u2225\u03bb\u22c6B\u2212b1\u2225, which we bound and interpret below in Section 3.3. We denote\n\u03a5T,\u03b4 = max { \u03b2T,\u03b4/4, 2 \u221a dT ln T 2\n\u03b4/4 ,\n\u221a 2T ln 2(d+ 1)T\n\u03b4/4\n} .\nLemma 1. Fix \u03b4 \u2208 (0, 1) and 0 < b < minB. If Assumption 2 holds and if the CBwK problem is feasible for some B\u2032 < B \u2212 b1, the Box B strategy run with \u03b4/4 ensures that the costs satisfy, with probability at least 1\u2212 \u03b4,\n\u2200 1 \u2a7d t \u2a7d T,\nwwwww ( t\u2211 \u03c4=1 c\u03c4 \u2212 t (B \u2212 b1) ) + wwwww \u2a7d 1 + 3\u2225\u03bb\u22c6B\u2212b1\u2225\u03b3 + 20\u221ad\u03a5T,\u03b4 . Lemma 2. Fix \u03b4 \u2208 (0, 1) and 0 < b < minB. Under the same assumptions as in Lemma 1 and on the same event with probability at least 1\u2212 \u03b4 as therein, the regret of the Box B strategy run with \u03b4/4 satisfies\n\u2200 1 \u2a7d t \u2a7d T, Rt \u2a7d \u2225\u03bb\u22c6B\u2212b1\u2225 ( t b \u221a d+ 6\u03a5T,\u03b4 ) + 36\u03b3 \u221a d ( \u03a5T,\u03b4 )2 + 8ln T 2\n\u03b4/4 .\nIdeal choice of \u03b3. The margin b will be taken proportional to the high-probability deviation provided by Lemma 1, divided by T . Now, the bounds of Lemmas 1 and 2 are respectively of the order\nTb \u223c ( 1 + \u2225\u03bb\u22c6B\u2212b1\u2225 ) /\u03b3 + \u221a T and \u2225\u03bb\u22c6B\u2212b1\u2225 ( Tb+ \u221a T ) + \u03b3T ,\nagain, up to logarithmic factors. To optimize each of these bounds, we therefore wish that b be of order 1/ \u221a T and that \u03b3 be of the order of \u2225\u03bb\u22c6B\u2212b1\u2225/ \u221a T . Of course, this wish stumbles across the same issues of ignoring a beforehand bound on \u2225\u03bb\u22c6B\u2212b1\u2225, exactly like, e.g., in Agrawal and Devanur [2016], Han et al. [2022] (where this bound is denoted by Z). We were however able to overcome this issue in a satisfactory way, i.e., by obtaining the ideal bounds implied above through adaptive choices of \u03b3, discussed in Section 3.2. Suboptimal choices of \u03b3. When \u03b3 is badly set, e.g., \u03b3 = 1/ \u221a T , the order of magnitude of the regret remains larger than \u2225\u03bb\u22c6B\u2212b1\u2225Tb and the cost deviations to T (B \u2212 b1) become of the larger order \u2225\u03bb\u22c6B\u2212b1\u2225 \u221a T , which dictates a larger choice for T b. Namely, if in addition \u2225\u03bb\u22c6B\u2212b1\u2225 is bounded in some worst-case way by 1/minB (see Section 3.3), the margin Tb is set of the order of\u221a T/minB. But of course, this margin must remain smaller than the total-cost constraints TB. This is the fundamental source of the typical minB \u2a7e T\u22121/4 constraints on the average cost constraints B observed in the literature: see, e.g., Agrawal and Devanur, 2016, Han et al., 2022, where the hyperparameter Z plays exactly the role of \u2225\u03bb\u22c6B\u2212b1\u2225.\nProof sketch. Lemmas 1 and 2 are proved in detail in Appendix B; the two proofs are similar and we sketch here the one for Lemma 1. The derivations below hold on the intersection of four events of probability at least 1\u2212 \u03b4/4 each: one for Assumption 2, one for the application of the HoeffdingAzuma inequality, and two for the application of a version of Bernstein\u2019s inequality. In this sketch of proof, \u2272 denotes inequalities holding up to small terms.\nBy the Hoeffding-Azuma inequality and by Assumption 2, the sum of the c\u03c4 \u2212 (B\u2212 b1) is not much larger than the sum of the \u2206c\u0302\u03c4 = c\u0302 lcb \u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1). We bound the latter by noting that by definition, \u03bb\u03c4 \u2a7e \u03bb\u03c4\u22121 + \u03b3\u2206c\u0302\u03c4 , so that after telescoping,wwwww ( t\u2211 \u03c4=1 c\u03c4 \u2212 t(B \u2212 b1) ) + wwwww \u2272 wwwww ( t\u2211 \u03c4=1 \u2206c\u0302\u03c4 ) +\nwwwww \u2a7d \u2225\u03bbt\u2225\u03b3 \u2a7d \u2225\u03bb\u22c6B\u2212b1\u2225\u03b3 + \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u2225\u03b3 . (5) We are thus left with bounding the \u2225\u03bb\u22c6B\u2212b1\u2212\u03bbt\u2225. The classical bound in (projected) gradient-descent analyses (see, e.g., Zinkevich, 2003) reads:\n\u2225\u03bb\u03c4 \u2212 \u03bb\u22c6B\u2212b1\u22252 \u2212 \u2225\u03bb\u03c4\u22121 \u2212 \u03bb \u22c6 B\u2212b1\u22252 \u2a7d 2\u03b3 \u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u2212 \u03bb\u22c6B\u2212b1 \u232a + \u03b32 \u2a7d4d\ufe37 \ufe38\ufe38 \ufe37ww\u2206c\u0302\u03c4ww2 . The estimates of Assumption 2, and the definition of at as the argument of some maximum, entail\u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121\u2212\u03bb\u22c6B\u2212b1 \u232a = \u2329 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212(B\u2212b1), \u03bb\u03c4\u22121\u2212\u03bb \u22c6 B\u2212b1 \u232a \u2272 g\u03c4 (\u03bb \u22c6 B\u2212b1)\u2212g\u03c4 (\u03bb\u03c4\u22121) ,\nwhere g\u03c4 (\u03bb) = max a\u2208A\n{ r(x\u03c4 , a)\u2212 \u2329 c(x\u03c4 , a)\u2212 (B \u2212 b1), \u03bb \u232a} .\nWe also introduce \u039bt = \u221a d max\n1\u2a7d\u03c4\u2a7dt \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u2225. We sum up the bounds above, use telescoping,\nand get\n\u2225\u03bbt \u2212 \u03bb\u22c6B\u2212b1\u22252 \u2272 \u2225\u03bb \u22c6 B\u2212b1\u22252 + 4d \u03b32t+ 2\u03b3 \u2211 \u03c4\u2a7dt\ng\u03c4 (\u03bb \u22c6 B\u2212b1)\u2212 g\u03c4 (\u03bb\u03c4\u22121)\ufe38 \ufe37\ufe37 \ufe38\n\u22720+3\u039bt \u221a t ln(T/\u03b4)\n, (6)\nwhere the \u2272 of the sum in the right-hand side comes from a version of Bernstein\u2019s inequality, the fact that E [ g\u03c4 (\u03bb \u22c6 B\u2212b1) ] = OPT(r, c,B \u2212 b1) is larger than the conditional expectation of g\u03c4 (\u03bb\u03c4\u22121). Since (6) holds for all t, we may then show by induction that for some constants \u25a1 and \u25b3,\n\u2225\u03bbt \u2212 \u03bb\u22c6B\u2212b1\u2225 \u2272 \u25a1\u2225\u03bb \u22c6 B\u2212b1\u2225+\u25b3\u03b3\u03a5T ,\nand we conclude by substituting this inequality into (5)."
        },
        {
            "heading": "3.2 Meta-stategy with adaptive step size \u03b3",
            "text": "As indicated after the statements of Lemmas 1 and 2, we want a (meta-)strategy adaptively setting \u03b3 to learn \u2225\u03bb\u22c6B\u2212b1\u2225. We do so via a doubling trick using the Box B strategy as a routine and step sizes of the form \u03b3k = 2k/ \u221a T in regimes k \u2a7e 0; the resulting meta-strategy is stated in Box C (a more detailed pseudo-code is provided in Appendix C). As the theorem below will reveal, the margin b therein can be picked based solely on T , \u03b4, and d.\nBOX C: PROJECTED GRADIENT DESCENT FOR CBWK WITH ADAPTIVE STEP SIZE\nInputs: number of rounds T ; confidence level 1\u2212 \u03b4; margin b on the average constraints; estimation procedure and error functions \u03b5t of Assumption 2; optimistic estimates (2) Initialization: T0 = 1; sequence \u03b3k = 2k/ \u221a T of step sizes;\nsequence MT,\u03b4,k = 4 \u221a T + 20 \u221a d\u03a5T,\u03b4/(k+2)2 of cost deviations\nFor regimes k = 0, 1, 2, . . .: \u2022 Take a fresh start of the Box B strategy with step size \u03b3k, risk level \u03b4/ ( 4(k + 2)2 ) ,\nand margin b;\n\u2022 Run it for t \u2a7e Tk until wwwww ( t\u2211 \u03c4=Tk c\u03c4 \u2212 (t\u2212 Tk + 1) (B \u2212 b1) ) + wwwww > MT,\u03b4,k ; \u2022 Stop regime k, and move to regime k + 1 for the next round, i.e., Tk+1 = t+ 1.\nBandit problems containing a null-cost action (which is a typical assumption in the CBwK literature) are feasible for all cost constraints B\u2032 \u2a7e 0; the assumption of (B \u2212 2bT1)\u2013feasibility in Theorem 1 below is therefore a mild assumption. Apart from this, the following theorem only imposes that the average cost constraints are larger than 1/ \u221a T , up to poly-logarithmic terms. We let O\u0303 denote orders of magnitude in T up to poly-logarithmic terms.\nFor x > 0, we denote by ilog x = \u2308log2 x\u2309 the upper integer part of the base-2 logarithm of x. Theorem 1. Fix \u03b4 \u2208 (0, 1) and let Assumption 2 hold. Consider the Box C meta-strategy, run with confidence level 1\u2212 \u03b4 and with margin bT = (1 + ilog T ) ( MT,\u03b4,ilog T + 2 \u221a d ) /T = O\u0303 ( 1/ \u221a T ) . If the average cost constraints B are such that B \u2a7e 2bT1 and if the problem (r, c,B \u2212 2bT1, \u03bd) is feasible, then this meta-strategy satisfies, with probability at least 1\u2212 \u03b4:\nRT \u2a7d O\u0303 (( 1 + \u2225\u03bb\u22c6B\u2212bT 1\u2225 )\u221a T )\nand \u2211 t\u2a7dT ct \u2a7d TB ,\nwhere a fully closed-form, non-asymptotic, regret bound is stated in (19) in Appendix C.\nA complete proof may be found in Appendix C. Its structure is the following; all statements hold with high probability. We denote by \u2113k the realized lengths of the regime. First, the number of regimes is bounded by noting that if regime K = ilog \u2225\u03bb\u22c6B\u2212bT 1\u2225 is achieved, then by Lemma 1 and the choice of MT,\u03b4,K , the stopping condition of regime K will never be met; thus, at most K + 1 regimes take place. We also prove that ilog \u2225\u03bb\u22c6B\u2212bT 1\u2225 \u2a7d KT = ilog T . Second, on each regime k, the difference of cumulated costs to \u2113k(B \u2212 bT1) is smaller than MT,\u03b4,k by design, so that the total cumulative costs are smaller than T (B \u2212 bT1) plus something of the order of (KT + 1)MT,\u03b4,KT , which is a quantity that only depends on T , \u03b4, d, and not on the unknown \u2225\u03bb\u22c6B\u2212bT 1\u2225, and that we take for T bT . Third, we similarly sum the regret bounds of Lemma 2 over the regimes: keeping in mind that bT = O\u0303 (\u221a T ) , we have sums of the form\n\u2225\u03bb\u22c6B\u2212bT 1\u2225 \u2211 k\u2a7dK ( bT \u2113k + \u221a T ) \u2a7d \u2225\u03bb\u22c6B\u2212bT 1\u2225 O\u0303 (\u221a T ) and \u2211 k\u2a7dK \u03b3kT \u2a7d 2K+1\u221a T \u2a7d 4\u2225\u03bb\u22c6B\u2212bT 1\u2225\u221a T ."
        },
        {
            "heading": "3.3 Discussion of the obtained regret bounds",
            "text": "The regret bound of Theorem 1 features a multiplicative factor \u2225\u03bb\u22c6B\u2212bT 1\u2225 instead of the typical factor OPT(r, c,B)/minB proposed by the literature (see, e.g., Agrawal and Devanur, 2016, Li and Stoltz, 2022, Han et al., 2022). In view of the lower bound proof scheme discussed in Section 4, this bound \u2225\u03bb\u22c6B\u2212bT 1\u2225 \u221a T seems the optimal bound. We thus now provide bounds on \u2225\u03bb\u22c6B\u2212bT 1\u2225 that have some natural interpretation. We recall that feasibility was defined in Assumption 1. The elementary proofs of Lemma 3 and Corollary 1 are based on (4) and may be found in Appendix D.\nLemma 3. Let b \u2208 [0,minB) and let 0 \u2a7d B\u0303 < B \u2212 b1. If the contextual bandit problem with knapsacks (r, c,B\u2032, \u03bd) is feasible for some B\u2032 < B\u0303, then\n\u2225\u03bb\u22c6B\u2212b1\u2225 \u2a7d OPT(r, c,B \u2212 b1)\u2212 OPT\n( r, c, B\u0303 ) min ( B \u2212 b1\u2212 B\u0303\n) . Corollary 1. When there exists a null-cost action, \u2225\u03bb\u22c6B\u2212b1\u2225 \u2a7d 2\nOPT(r, c,B)\u2212 OPT(r, c,0) minB\nfor all b \u2208 [0,minB/2].\nThe bounds provided by Lemma 3 and Corollary 1 must be discussed in each specific case. They are null (as expected) when the average cost constraints B are large enough so that costs do not constrain the choice of actions; this was not the case with the typical OPT(r, c,B)/minB bounds of the literature. Another typical example is the following. Example 1. Consider a problem featuring a baseline action anull with some positive expected rewards and no cost, and additional actions with larger rewards and scalar expected costs c(x, a) \u2a7e \u03b1 > 0. Denote by B the average cost constraint. Then actions a \u0338= anull may only be played at most B/\u03b1 times on average, and therefore, OPT(r, c, B)\u2212 OPT(r, c, 0) \u2a7d B/\u03b1. In particular, the bound stated in Corollary 1 may be further upper bounded by 1/(2\u03b1), which is fully independent of T and B; i.e., a \u221a T\u2013regret only is suffered in Theorem 1."
        },
        {
            "heading": "4 Optimality of the upper bounds, and limitations",
            "text": "The main limitations to our work are relative, in our eyes, to the optimality of the bounds achieved\u2014 up to one limitation, already discussed in Section 2.1: our fairness example is intrinsically limited to the awareness set-up, where the learner has access to the group index before making the predictions.\nA proof scheme for problem-dependent lower bounds. Results offering lower bounds on the regret for CBwK are scarce in the literature. They typically refer to some minimax regret, and either do so by indicating that lower bounds for CBwK must be larger in a minimax sense than the ones obtained for non-contextual bandits with knapsacks (see, e.g., Agrawal and Devanur, 2016, Li and Stoltz, 2022), or by leveraging a minimax regret lower bound for a subroutine of the strategy used (Slivkins et al., 2022). We rather focus on problem-dependent regret bounds but only offer a proof scheme, to be made more formal\u2014see Appendix E.1. Interestingly, this proof scheme follows closely the\nanalysis of a primal strategy performed in Appendix F. First, an argument based on the law of the iterated logarithm shows the necessity of a margin bT of order 1/ \u221a T to satisfy the total TB cost constraints. This imposes that only TOPT(r, c,B \u2212 bT1) is targeted, thus the regret is at least of order\nT ( OPT(r, c,B)\u2212 OPT(r, c,B \u2212 bT1) ) + \u221a T , i.e.,\n( 1 + \u2225\u03bb\u22c6B\u2225 )\u221a T .\nThis matches the bound of Theorem 1, but with \u03bb\u22c6B\u2212bT 1 replaced by \u03bb \u22c6 B .\nWe now turn to the list of limitations pertaining to the optimality of our bounds.\nLimitation 1: Large constants. First, we must mention that in the bounds of Lemmas 1 and 2, we did not try to optimize the constants but targeted readability. The resulting values in Theorem 1, namely, the recommended choice of bT and the closed-form bound (19) on the regret, therefore involve constants that are much larger than needed.\nLimitation 2: Not capturing possibly fast regret rates in some cases. Second, a careful inspection of our proof scheme for lower bounds shows that it only works in the absence of a null-cost action. When such a null-cost action exists and costs are non-negative, Example 1 shows that costly actions are played at most B/\u03b1 times on average. If the null-cost action has also a null reward, the costly actions are the only ones generating some stochasticity, thus, we expect that the \u221a T factors (coming, among\nothers, from a version of Bernstein\u2019s inequality) be replaced by \u221a B/\u03b1 factors. As a consequence,\nin the setting of Example 1, bounds growing much slower than \u221a T should be achievable, see the discussion in Appendix E.1. Limitation 3: \u221a T regret not captured in case of softer constraints. For the primal strategy discussed in Appendix F, we could prove a general \u221a T regret bound in the case where total costs\nsmaller than TB + O\u0303 (\u221a T ) are allowed. We were unable to do so for our dual strategy."
        },
        {
            "heading": "5 Brief overview of the numerical experiments performed",
            "text": "In Appendix G, we implement a specific version of the motivating example of Section 2.1, as proposed by Chohlas-Wood et al. [2021] in the public repository https://github.com/ stanford-policylab/learning-to-be-fair. Fairness costs together with spendings related to rideshare assistance or transportation vouchers are considered. We report below the performance of the strategies considered only in terms of average rewards and rideshare costs, and for the fairness threshold \u03c4 = 10\u22127. The Box B strategies with fixed step sizes may fail to control the budget when \u03b3 is too large: we observe this for \u03b3 = 0.01. We see overall a tradeoff between larger average rewards and larger average costs. The Box C strategy navigates between three regimes, \u03b3 = 0.01 to start (regime k = 0), then \u03b3 = 0.02 (regime k = 1), and finally \u03b3 = 0.04 (regime k = 2), which it sticks to. It overall adaptively finds a decent tuning of \u03b3.\n0 2000 4000 6000 8000 10000 T\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0.50\nAv er\nag e\nRe wa\nrd s\nOPT(r, c, B) OPT(r, c, B')\n0 2000 4000 6000 8000 100000.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10\nRi de\nsh ar\ne Co\nst s\nBudget Rideshare\nPGD Gamma 0.01 PGD Gamma 0.02 PGD Gamma 0.04 PGD Gamma 0.05 PGD Gamma 0.1 PGD Adaptive\nAcknowledgments and Disclosure of Funding\nEvgenii Chzhen, Christophe Giraud, Zhen Li, and Gilles Stoltz have no direct funding to acknowledge other than the salaries paid by their employers, CNRS, Universit\u00e9 Paris-Saclay, BNP Paribas, and HEC Paris. They have no competing interests to declare."
        },
        {
            "heading": "B Proofs of Lemmas 1 and 2",
            "text": "In both proofs, we will use the following deviation inequalities, holding on an event EH-Az \u2229 E\u03b2 of probability at least 1\u2212 \u03b4/2, where the event E\u03b2 is defined in Assumption 2 with a confidence level 1\u2212 \u03b4/4 and the event EH-Az is defined below: on EH-Az \u2229 E\u03b2 ,\n\u2200 1 \u2a7d t \u2a7d T, t\u2211\n\u03c4=1\nc\u03c4 \u2a7d ( \u03b1t,\u03b4/4 + 2\u03b2t,\u03b4/4 ) 1+ t\u2211 \u03c4=1 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 ) (7)\nand t\u2211\n\u03c4=1\nr\u03c4 \u2a7e \u2212 ( \u03b1t,\u03b4/4 + 2\u03b2t,\u03b4/4 ) + t\u2211 \u03c4=1 r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 ) , (8)\nwhere \u03b2t,\u03b4/4 is also defined in Assumption 2 and\n\u03b1t,\u03b4/4 =\n\u221a 2t ln 2(d+ 1)T\n\u03b4/4 .\nThese inequalities come, on the one hand, by the Hoeffding-Azuma inequality applied (d + 1)T times on the range [\u22121, 1]: it ensures that on an event EH-Az with probability at least 1\u2212 \u03b4/4, for all 1 \u2a7d t \u2a7d T ,wwwww t\u2211 \u03c4=1 c\u03c4 \u2212 t\u2211 \u03c4=1 c(x\u03c4 , a\u03c4 ) wwwww \u221e \u2a7d \u03b1t,\u03b4/4 1 and \u2223\u2223\u2223\u2223\u2223 t\u2211 \u03c4=1 r\u03c4 \u2212 t\u2211 \u03c4=1 r(x\u03c4 , a\u03c4 )\n\u2223\u2223\u2223\u2223\u2223 \u2a7d \u03b1t,\u03b4/4 (where we denoted by \u2225 \u00b7 \u2225\u221e the supremum norm). On the other hand, Assumption 2 and the clipping (3) entail, in particular, that on an event E\u03b2 of probability at least 1\u2212 \u03b4/4, for all 1 \u2a7d t \u2a7d T , t\u2211\n\u03c4=1\nc(x\u03c4 , a\u03c4 ) \u2a7d t\u2211\n\u03c4=1\n( c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 ) + 2\u03b5\u03c4\u22121(x\u03c4 , a\u03c4 , \u03b4/4)1 ) = 2\u03b2t,\u03b4/4 1+ t\u2211 \u03c4=1 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\nand (similarly) t\u2211\n\u03c4=1\nr(x\u03c4 , a\u03c4 ) \u2a7e \u22122\u03b2t,\u03b4/4 + t\u2211\n\u03c4=1\nr\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 ) .\nB.1 Proof of Lemma 1\nFor \u03c4 \u2a7e 1, by definition of \u03bb\u03c4 in Box B, \u03bb\u03c4 \u2212 \u03bb\u03c4\u22121 = ( \u03bb\u03c4\u22121 + \u03b3 ( c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1) )) + \u2212 \u03bb\u03c4\u22121\n\u2a7e \u03b3 ( c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1) ) .\nFor t \u2a7e 1, as \u03bb0 = 0 and \u03bbt \u2a7e 0, we get, after telescoping and taking non-negative parts,\n\u03bbt \u2a7e \u03b3\n( t\u2211\n\u03c4=1\n( c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1) )) + ,\nthus\nwwwww ( t\u2211 \u03c4=1 ( c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1) )) + wwwww \u2a7d \u2225\u03bbt\u2225\u03b3 . (9) Up to the deviation terms (7), \u2225\u03bbt\u2225/\u03b3 bounds how larger the cost constraints till round t are from t(B \u2212 b1). Most of the rest of the proof, namely, the Steps 1\u20134 below, thus focus on upper bounding the \u2225\u03bbt\u2225, while Step 5 will collect all bounds together and conclude.\nStep 1: Gradient-descent analysis. We introduce\n\u2206c\u0302\u03c4 = c\u0302 lcb \u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1) (10)\nand prove the following deterministic inequality: for all 1 \u2a7d t \u2a7d T ,\n\u2200\u03bb \u2a7e 0, \u2225\u03bbt \u2212 \u03bb\u22252 \u2a7d \u2225\u03bb\u22252 + 4d \u03b32t+ 2\u03b3 t\u2211\n\u03c4=1\n\u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u2212 \u03bb \u232a . (11)\nTo do so, we proceed as is classical in (projected) gradient-descent analyses; see, e.g., Zinkevich [2003]. Namely, for all 1 \u2a7d \u03c4 \u2a7d T ,\n2 \u2329 \u2212\u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u2212 \u03bb \u232a = 1\n\u03b3\n( \u2225\u03bb\u03c4\u22121 \u2212 \u03bb\u22252 + ww\u03b3\u2206c\u0302\u03c4ww2 \u2212ww\u03bb\u03c4\u22121 \u2212 \u03bb+ \u03b3\u2206c\u0302\u03c4ww2) = \u03b3\nww\u2206c\u0302\u03c4ww2 + 1 \u03b3 ( \u2225\u03bb\u03c4\u22121 \u2212 \u03bb\u22252 \u2212 ww\u03bb\u03c4\u22121 + \u03b3\u2206c\u0302\u03c4 \u2212 \u03bbww2) \u2a7d \u03b3\nww\u2206c\u0302\u03c4ww2 + 1 \u03b3 ( \u2225\u03bb\u03c4\u22121 \u2212 \u03bb\u22252 \u2212 \u2225\u03bb\u03c4 \u2212 \u03bb\u22252 ) ,\nwhere the inequality comes from the definition \u03bb\u03c4 = ( \u03bb\u03c4\u22121 + \u03b3\u2206c\u0302\u03c4 ) + and the fact that\n\u2200x \u2208 R, \u2200 y \u2a7e 0, \u2223\u2223(x)+ \u2212 y\u2223\u2223 \u2a7d |x\u2212 y|\n(which may be proved by distinguishing the cases x \u2a7d 0 and x \u2a7e 0).\nWe note that B \u2212 b1 \u2208 [0, 1]d and c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 ) \u2208 [\u22121, 1]d , so that ww\u2206c\u0302\u03c4ww2 \u2a7d 4d .\nCollecting all bounds above, we get, after summation and telescoping, t\u2211\n\u03c4=1\n\u2329 \u2212\u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u2212 \u03bb \u232a \u2a7d 2d \u03b3t+ 1\n2\u03b3\n( \u2225\u03bb0 \u2212 \u03bb\u22252 \u2212 \u2225\u03bbt \u2212 \u03bb\u22252 ) .\nRearranging and substituting \u03bb0 = 0 yields the claimed inequality (11).\nStep 2: Relating estimated costs (and rewards) to true conditional expectations. In this part, we upper bound the right-hand side of (11) by showing that on the event E\u03b2 ,\n\u2200\u03bb \u2a7e 0, \u2200 1 \u2a7d t \u2a7d T, t\u2211\n\u03c4=1\n\u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u2212 \u03bb \u232a \u2a7d 2 ( 1 + \u2225\u03bb\u22251 ) \u03b2t,\u03b4/4 + t\u2211 \u03c4=1 ( g\u03c4 (\u03bb)\u2212 g\u03c4 (\u03bb\u03c4\u22121) ) , (12)\nwhere g\u03c4 (\u03bb) = max a\u2208A\n{ r(x\u03c4 , a)\u2212 \u2329 c(x\u03c4 , a)\u2212 (B \u2212 b1), \u03bb \u232a} and where we recall that \u2225 \u00b7 \u22251 denotes the \u21131\u2013norm. Adding and subtracting r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 ), here, we deal with\nt\u2211 \u03c4=1 \u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u2212 \u03bb \u232a = t\u2211 \u03c4=1 ( r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 \u2329 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1), \u03bb \u232a) \u2212\nt\u2211 \u03c4=1 ( r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 \u2329 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1), \u03bb\u03c4\u22121 \u232a) .\nNow, for each \u03c4 , by (3) and the fact that \u03bb \u2a7e 0, r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 \u2329 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1), \u03bb \u232a \u2a7d r(x\u03c4 , a\u03c4 ) + 2\u03b5\u03c4\u22121(x\u03c4 , a\u03c4 , \u03b4/4)\u2212 \u2329 c(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1), \u03bb \u232a + 2\u03b5\u03c4\u22121(x\u03c4 , a\u03c4 , \u03b4/4) \u2225\u03bb\u22251\n\u2a7d g\u03c4 (\u03bb) + 2 ( 1 + \u2225\u03bb\u22251 ) \u03b5\u03c4\u22121(x\u03c4 , a\u03c4 , \u03b4/4) .\nOn the other hand, by definition of a\u03c4 for the equality, and then by the other inequalities in (3), for each \u03c4 , and the fact that \u03bb\u03c4\u22121 \u2a7e 0,\nr\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 \u2329 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1), \u03bb\u03c4\u22121 \u232a (13)\n= max a\u2208A\n{ r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a)\u2212 \u2329 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a)\u2212 (B \u2212 b1), \u03bb\u03c4\u22121 \u232a} \u2a7e max\na\u2208A\n{ r(x\u03c4 , a)\u2212 \u2329 c(x\u03c4 , a)\u2212 (B \u2212 b1), \u03bb\u03c4\u22121 \u232a} = g\u03c4 (\u03bb\u03c4\u22121) .\nCollecting the two series of bounds concludes this part.\nStep 3: Application of a Bernstein-Freedman inequality. We recall that we denoted by \u03bb\u22c6B\u2212b1 the optimal dual variable in (4) for OPT(r, c,B \u2212 b1); it exists because we assumed feasibility of a problem with average cost constraints B\u2032 < B \u2212 b1. We now upper bound the sum appearing in the right hand side of (12) at \u03bb = \u03bb\u22c6B\u2212b1 by showing that on an event EBern-c of probability at least 1\u2212 \u03b4/4, for all 1 \u2a7d t \u2a7d T ,\nt\u2211 \u03c4=1 ( g\u03c4 (\u03bb \u22c6 B\u2212b1)\u2212 g\u03c4 (\u03bb\u03c4\u22121) ) \u2a7d (1 + 2\u039bt)\n\u221a 2t ln T 2\n\u03b4/4 + 2Kt ln\nT 2 \u03b4/4 , (14)\nwhere \u039bt = max 1\u2a7d\u03c4\u2a7dt\n\u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u22251 and Kt = 4 ( 1 + \u2225\u03bb\u22c6B\u2212b1\u22251 + 2d\u03b3t ) .\nWe will do so by applying a version of the Bernstein-Freedman inequality for martingales stated in Cesa-Bianchi et al. [2005, Corollary 16] involving the sum of the conditional variances (and not only a deterministic bound thereon); it is obtained via peeling based on the classic version of Bernstein\u2019s inequality (Freedman, 1975). We restate it here for the convenience of the reader (after applying some simple boundings). Lemma 4 (a version of the Bernstein-Freedman inequality by Cesa-Bianchi et al., 2005). Let X1, X2, . . . be a martingale difference with respect to the filtration F = (Fs)s\u2a7e0 and with increments bounded in absolute values by K. For all t \u2a7e 1, let\nSt = t\u2211 \u03c4=1 E [ X2\u03c4 \u2223\u2223F\u03c4\u22121]\ndenote the sum of the conditional variances of the first t increments. Then, for all \u03b4 \u2208 (0, 1) and all t \u2a7e 1, with probability at least 1\u2212 \u03b4,\nt\u2211 \u03c4=1 X\u03c4 \u2a7d \u221a 2St ln t \u03b4 + 2K ln t \u03b4 .\nWe introduce, for all \u03bb \u2a7e 0, the common expectation of the g\u03c4 (\u03bb), namely,\nG(\u03bb) = E [ g\u03c4 (\u03bb) ] = EX\u223c\u03bd [ max a\u2208A { r(X, a)\u2212 \u2329 c(X, a)\u2212 (B \u2212 b1), \u03bb \u232a}] ,\nand consider the martingale increments X\u03c4 = ( g\u03c4 (\u03bb \u22c6 B\u2212b1)\u2212 g\u03c4 (\u03bb\u03c4\u22121) ) \u2212 ( G(\u03bb\u22c6B\u2212b1)\u2212G(\u03bb\u03c4\u22121) ) .\nAs B \u2212 b1 \u2208 [0, 1]d and c takes values in [\u22121, 1]d, for all x \u2208 X , all a \u2208 A, and all v \u2208 Rd, the quantities r(x, a)\u2212 \u2329 c(x, a)\u2212 (B \u2212 b1), v \u232a take absolute values smaller than 1 + 2\u2225v\u22251. Using that a difference of maxima is smaller than the maximum of the differences, we get, in particular,\u2223\u2223g\u03c4 (\u03bb\u22c6B\u2212b1)\u2212 g\u03c4 (\u03bb\u03c4\u22121)\u2223\u2223 \u2a7d 1 + 2\u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u22251 a.s. (15) and\n\u2223\u2223G(\u03bb\u22c6B\u2212b1)\u2212G(\u03bb\u03c4\u22121)\u2223\u2223 \u2a7d 1 + 2\u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u22251 . Now, given the update step in Step 3 of Box B, we have the deterministic bound \u2225\u03bbt\u22251 \u2a7d 2d\u03b3t for all 1 \u2a7d t \u2a7d T . Therefore, by a triangle inequality, the martingale increments are bounded in absolute values by Kt, as\n2 ( 1 + 2\u2225\u03bb\u22c6B\u2212b1\u22251 + 1 + 2max\n\u03c4\u2a7dt \u2225\u03bb\u03c4\u22121\u22251\n) \u2a7d 4 ( 1 + \u2225\u03bb\u22c6B\u2212b1\u22251 + 2d\u03b3t ) = Kt .\nThe conditional variance of X\u03c4 is smaller than the squared half-width of the conditional range (Popoviciu\u2019s inequality on variances); in particular, (15) thus entails\nSt \u2a7d t\u2211\n\u03c4=1\n( 1 + 2\u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u22251 )2 \u2a7d (1 + 2\u039bt) 2t .\nWe now get the claimed inequalities (14) first by noting that by (4), for all \u03c4 \u2a7d T ,\nG(\u03bb\u22c6B\u2212b1)\u2212G(\u03bb\u03c4\u22121) \u2a7d 0 , and second, by applying Lemma 4 for each 1 \u2a7d t \u2a7d T , using a confidence level \u03b4/(4T ). By a union bound, this indeed defines an event EBern-c of probability at least 1\u2212 \u03b4/4.\nStep 4: Induction to bound the \u039bt. In this step, we show by induction that, with high probability, the norms \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u2225 satisfy the bound (18) stated below. To do so, we combine the outcomes of Steps 1\u20133 and obtain that on the event E\u03b2 \u2229 EBern-c, for all 1 \u2a7d t \u2a7d T ,\n\u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u22252\n\u2a7d \u2225\u03bb\u22c6B\u2212b1\u22252 + 4d \u03b32t+ 2\u03b3 ( 2 ( 1 + \u2225\u03bb\u22c6B\u2212b1\u22251 ) \u03b2t,\u03b4/4 + t\u2211 \u03c4=1 ( g\u03c4 (\u03bb \u22c6 B\u2212b1)\u2212 g\u03c4 (\u03bb\u03c4\u22121) )) \u2a7d \u2225\u03bb\u22c6B\u2212b1\u22252 + 4d \u03b32t+ 4\u03b3 ( 1 + \u2225\u03bb\u22c6B\u2212b1\u22251 ) \u03b2t,\u03b4/4 + 2\u03b3(1 + 2\u039bt) \u221a 2t ln T 2\n\u03b4/4 + 4\u03b3Kt ln\nT 2 \u03b4/4 ,\nwhere we recall that norms not indexed by a subscript are Euclidean norms, and\n\u039bt = max 1\u2a7d\u03c4\u2a7dt\n\u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u22251 and Kt = 4 ( 1 + \u2225\u03bb\u22c6B\u2212b1\u22251 + 2d\u03b3t ) .\nWe upper bound \u2225\u03bb\u22c6B\u2212b1\u22251 and \u039bt in terms of Euclidean norms,\n\u2225\u03bb\u22c6B\u2212b1\u22251 \u2a7d \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225 and \u039bt \u2a7d \u221a d max\n1\u2a7d\u03c4\u2a7dt \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u2225 ,\nperform some crude boundings like \u221a t \u2a7d \u221a T and \u03b2t,\u03b4/4 \u2a7d \u03b2T,\u03b4/4, and obtain the following induction relationship: on the event E\u03b2 \u2229 EBern-c, for all 1 \u2a7d t \u2a7d T ,\n\u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u22252 \u2a7d A+B t+ C max 1\u2a7d\u03c4\u2a7dt \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u2225 , (16)\nwhere\nA = \u2225\u03bb\u22c6B\u2212b1\u22252 + \u03b3\n( 4 ( 1 + \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225 ) \u03b2T,\u03b4/4 + 2 \u221a 2T ln T 2\n\u03b4/4 + 16\n( 1 + \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225 ) ln T 2\n\u03b4/4\n) ,\nB = 4d\u03b32 + 4\u00d7 4\u00d7 2d\u03b32ln T 2\n\u03b4/4 =\n( 4 + 32 ln T 2\n\u03b4/4\n) d\u03b32 \u2a7d 36 d\u03b32 ln T 2\n\u03b4/4 ,\nC = 4\u03b3 \u221a 2dT ln T 2\n\u03b4/4 .\nWe now show that (16) implies that on E\u03b2 \u2229 EBern-c,\n\u2200 0 \u2a7d t \u2a7d T, \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u2225 \u2a7d M def =\nC 2 +\n\u221a A+BT + C2\n4 . (17)\nIndeed, for t = 0, given that \u03bb0 = 0, we have \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u2225 = \u2225\u03bb \u22c6 B\u2212b1\u2225 \u2a7d\n\u221a A. Now, if the\nbound (17) is satisfied for all 0 \u2a7d \u03c4 \u2a7d t, where 0 \u2a7d t \u2a7d T \u2212 1, then (16) implies that \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt+1\u2225 \u2a7d A+B (t+ 1) + CM \u2a7d A+B T + CM \u2a7d M2 ,\nwhere the final inequality follows from the fact that (by definition of M , and this explains how we picked M )\nM2 \u2212 CM = ( M \u2212 C\n2\n)2 + C2\n4 = A+BT .\nBelow, we will make repeated uses of \u221a x+ y \u2a7d \u221a x+ \u221a y, of xy \u2a7d 2(x2+ y2), and of \u221a x \u2a7d 1+x, for x, y \u2a7e 0. From (17), we conclude that on the event E\u03b2 \u2229 EBern-c, \u2200 0 \u2a7d t \u2a7d T, \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u2225 \u2a7d \u221a A+ \u221a BT + C\n= \u221a A+ 6\u03b3 \u221a dT ln T 2\n\u03b4/4 + 4\u03b3\n\u221a 2dT ln T 2\n\u03b4/4 \u2a7d\n\u221a A+ 6\u03b3\u03b2\u2032T,\u03b4/4 ,\nwhere we denoted\n\u03b2\u2032T,\u03b4/4 = max { \u03b2T,\u03b4/4, 2 \u221a dT ln T 2\n\u03b4/4\n} \u2a7e ln T 2\n\u03b4/4 .\nFor the sake of readability, we may further bound \u221a A as follows (in some crude way):\n\u221a A \u2a7d \u2225\u03bb\u22c6B\u2212b1\u2225+ \u221a \u03b3 \u221a\u221a\u221a\u221a4(1 +\u221ad \u2225\u03bb\u22c6B\u2212b1\u2225)\u03b2T,\u03b4/4 + 2 \u221a 2T ln T 2\n\u03b4/4 + 16\n( 1 + \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225 ) ln T 2\n\u03b4/4 \u2a7d \u2225\u03bb\u22c6B\u2212b1\u2225+ \u221a \u03b3 \u221a 22\u03b2\u2032T,\u03b4/4 + 20 \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225\u03b2\u2032T,\u03b4/4\n\u2a7d \u2225\u03bb\u22c6B\u2212b1\u2225+ 1 + 6\u03b3\u03b2\u2032T,\u03b4/4 + \u2225\u03bb \u22c6 B\u2212b1\u2225+ 5\u03b3 \u221a d \u03b2\u2032T,\u03b4/4 ,\nwhere we used the facts that \u221a 20xy = 2 \u221a 5xy \u2a7d x+ 5y and \u221a 22x = 2 \u221a 22x/4 \u2a7d 1 + 6x.\nAll in all, we proved that on the event E\u03b2 \u2229 EBern-c,\n\u2200 0 \u2a7d t \u2a7d T, \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u2225 \u2a7d 2\u2225\u03bb \u22c6 B\u2212b1\u2225+ 17\u03b3 \u221a d \u03b2\u2032T,\u03b4/4 + 1 . (18)\nStep 5: Conclusion. We combine the bound (18) with the bound (9) of Step 1 and the bound (7): on the intersection of events E\u03b2 \u2229 EBern-c \u2229 EH-Az, which has a probability at least 1 \u2212 3\u03b4/4, for all 1 \u2a7d t \u2a7d T ,wwwww ( t\u2211\n\u03c4=1\nc\u03c4 \u2212 t(B \u2212 b1) ) + wwwww \u2a7d \u221ad(\u03b1T,\u03b4/4 + 2\u03b2T,\u03b4/4)+ \u2225\u03bbt\u2225\u03b3 \u2a7d \u221a d ( \u03b1T,\u03b4/4 + 2\u03b2T,\u03b4/4 ) +\n\u2225\u03bb\u22c6B\u2212b1\u2225 \u03b3 + \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bbt\u2225 \u03b3\n\u2a7d 3\u2225\u03bb\u22c6B\u2212b1\u2225+ 1\n\u03b3 +\n\u221a d ( \u03b1T,\u03b4/4 + 19\u03b2 \u2032 T,\u03b4/4 ) .\nThis entails in particular the stated result, given the definition of \u03a5T,\u03b4 as max { \u03b2\u2032T,\u03b4/4, \u03b1T,\u03b4/4 } .\nB.2 Proof of Lemma 2\nThe proof is similar to (but much simpler and shorter than) the one of Lemma 1 and borrows some of its arguments. We use throughout this section the notation introduced therein; we also define a new event EBern-r of probability at least 1\u2212 \u03b4/4. We start from (8) and introduce the same \u2206c\u0302t quantity as in (10): on the event EH-Az \u2229 E\u03b2 , for all 1 \u2a7d t \u2a7d T ,\nt\u2211 \u03c4=1 r\u03c4 \u2a7e \u2212 ( \u03b1t,\u03b4/4 + 2\u03b2t,\u03b4/4 ) + t\u2211 \u03c4=1 r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\n\u2a7e \u2212 ( \u03b1t,\u03b4/4 + 2\u03b2t,\u03b4/4 ) + t\u2211 \u03c4=1 ( r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 \u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u232a) + t\u2211 \u03c4=1 \u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u232a .\nOn the one hand, the result (11) with \u03bb = 0 exactly states that t\u2211\n\u03c4=1\n\u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u232a \u2a7e \u2225\u03bbt\u22252\n2\u03b3 \u2212 2d \u03b3t \u2a7e \u22122d \u03b3t .\nOn the other hand, the result (13) states that on E\u03b2 , for all 1 \u2a7d \u03c4 \u2a7d T , r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 \u2329 \u2206c\u0302\u03c4 , \u03bb\u03c4\u22121 \u232a = r\u0302 ucb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 \u2329 c\u0302 lcb\u03b4/4,\u03c4\u22121(x\u03c4 , a\u03c4 )\u2212 (B \u2212 b1), \u03bb\u03c4\u22121 \u232a \u2a7e g\u03c4 (\u03bb\u03c4\u22121) .\nA similar application of Lemma 4 as in the proof of Lemma 1 shows that on a new event EBern-r of probability at least 1\u2212 \u03b4/4, for all 1 \u2a7d t \u2a7d T ,\nt\u2211 \u03c4=1 g\u03c4 (\u03bb\u03c4\u22121) \u2a7e t\u2211 \u03c4=1 G(\u03bb\u03c4\u22121)\u2212 \u221a\u221a\u221a\u221a2 t\u2211 \u03c4=1 ( 1 + 2\u2225\u03bb\u03c4\u22121\u22251 )2 ln T 2 \u03b4/4 \u2212 8(1 + 2\u03b3t) ln T 2 \u03b4/4 .\nWe relate \u21131\u2013norms to Euclidean norms, resort to a triangle inequality, and substitute (18) to get that on EBern-c, for all 1 \u2a7d t \u2a7d T ,\n\u2212 \u221a\u221a\u221a\u221a2 t\u2211 \u03c4=1 ( 1 + 2\u2225\u03bb\u03c4\u22121\u22251 )2 ln T 2 \u03b4/4 \u2a7e \u2212 ( 1 + 2 \u221a d max 0\u2a7d\u03c4\u2a7dt\u22121 \u2225\u03bb\u03c4\u22121\u2225 )\u221a 2t ln T 2 \u03b4/4\n\u2a7e \u2212 ( 1 + 2 \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225+ 2 \u221a d max 0\u2a7d\u03c4\u2a7dt\u22121 \u2225\u03bb\u22c6B\u2212b1 \u2212 \u03bb\u03c4\u22121\u2225 )\u221a 2t ln T 2 \u03b4/4\n\u2a7e \u2212 ( 8 \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225+ 34\u03b3d \u03b2\u2032T,\u03b4/4 + 2 \u221a d+ 1 )\u221a 2t ln T 2\n\u03b4/4 \u2a7e \u2212 ( 8 \u221a d \u2225\u03bb\u22c6B\u2212b1\u2225+ 34\u03b3d \u03b2\u2032T,\u03b4/4 + 4 \u221a d )\u221a 2t ln T 2\n\u03b4/4 \u2a7e \u22126\u2225\u03bb\u22c6B\u2212b1\u2225\u03b2\u2032T,\u03b4/4 \u2212 25\u03b3 \u221a d ( \u03b2\u2032T,\u03b4/4 )2 \u2212 2\u221a2\u03b2\u2032T,\u03b4/4 \u2a7e \u22126\u2225\u03bb\u22c6B\u2212b1\u2225\u03b2\u2032T,\u03b4/4 \u2212 28\u03b3 \u221a d ( \u03b2\u2032T,\u03b4/4 )2 ,\nwhere we performed some crude boundings using the definition of 1 \u2a7d \u03b2\u2032t,\u03b4/4 \u2a7d \u03b2 \u2032 T,\u03b4/4. We also note that\n8(1 + 2\u03b3t) ln T 2\n\u03b4/4 \u2a7d 8 ln\nT 2 \u03b4/4 + 4\u03b3 ( \u03b2\u2032T,\u03b4/4 )2 .\nBy (4), we have OPT(r, c,B \u2212 b1) = G(\u03bb\u22c6B\u2212b1) \u2a7d G(\u03bb), for all \u03bb \u2a7e 0. Also,\nG(\u03bb\u22c6B\u2212b1) + b \u2225\u03bb \u22c6 B\u2212b1\u22251 = EX\u223c\u03bd [ max a\u2208A { r(X, a)\u2212 \u2329 c(X, a)\u2212 (B \u2212 b1), \u03bb\u22c6B\u2212b1 \u232a}] + b \u2225\u03bb\u22c6B\u2212b1\u22251\n= EX\u223c\u03bd [ max a\u2208A { r(X, a)\u2212 \u2329 c(X, a)\u2212B, \u03bb\u22c6B\u2212b1 \u232a}] \u2a7e OPT(r, c,B) ,\nwhere we used again (4). In particular,\nt\u2211 \u03c4=1 G(\u03bb\u03c4\u22121) \u2a7e t OPT(r, c,B)\u2212 t b \u2225\u03bb\u22c6B\u2212b1\u22251 \u2a7e t OPT(r, c,B)\u2212 t b \u221a d \u2225\u03bb\u22c6B\u2212b1\u22251 .\nCollecting all bounds above and using the definition of \u03a5T,\u03b4 as max { \u03b2\u2032T,\u03b4/4, \u03b1T,\u03b4/4 } and the fact that dt \u2a7d (\u03a5T,\u03b4)2, we proved the following. On EH-Az \u2229E\u03b2 \u2229EBern-c \u2229EBern-r, which is indeed an event with probability at least 1\u2212 \u03b4, for all 1 \u2a7d t \u2a7d T , t\u2211\n\u03c4=1\nr\u03c4\n\u2a7e \u22123\u03a5T,\u03b4 \u2212 2d \u03b3t+ t\u2211\n\u03c4=1\nG(\u03bb\u03c4\u22121)\u2212 \u221a\u221a\u221a\u221a2 T\u2211 t=1 ( 1 + 2\u2225\u03bbt\u22121\u22251 )2 ln T 2 \u03b4/4 \u2212 8 ln T 2 \u03b4/4 \u2212 4\u03b3 ( \u03b2\u2032T,\u03b4/4\n)2 \u2a7e t OPT(r, c,B)\u2212 \u2225\u03bb\u22c6B\u2212b1\u2225 ( t b \u221a d+ 6\u03a5T,\u03b4 ) \u2212 36\u03b3 \u221a d ( \u03a5T,\u03b4 )2 \u2212 8 ln T 2 \u03b4/4 .\nThis entails in particular the stated result."
        },
        {
            "heading": "C Proof of Theorem 1",
            "text": "The proof is divided into three steps: on a favorable event Emeta of probability at least 1\u2212 \u03b4, (i) we bound by ilog T the index of the last regime achieved in Box C;\nAlgorithm 1: Pseudo-code for the Box C strategy Input: number of rounds T ; confidence level 1\u2212 \u03b4; margin b on the average constraints; estimation procedure and error functions \u03b5t of Assumption 2; optimistic estimates (2) Initialization: T0 = 1; sequence \u03b3k = 2k/ \u221a T of step sizes;\nsequence MT,\u03b4,k = 4 \u221a T + 20 \u221a d\u03a5T,\u03b4/(k+2)2 of cost deviations\n1 for k \u2a7e 0 do // Box C part 2 \u03bbTk\u22121 = 0; 3 for t \u2a7e Tk do // Box B part 4 if t = T then 5 Terminate algorithm; 6 end 7 Observe the context xt; 8 Pick an action\nat \u2208 argmax a\u2208A\n{ r\u0302 ucb\u03b4,t\u22121(xt, a)\u2212 \u2329 c\u0302 lcb\u03b4,t\u22121(xt, a)\u2212 (B \u2212 b1), \u03bbt\u22121 \u232a} ;\n9 Observe the payoff rt and the costs ct; 10 Compute \u03bbt = ( \u03bbt\u22121 + \u03b3k ( c\u0302 lcb\u03b4,t\u22121(xt, at)\u2212 (B \u2212 b1) )) + ; // make PGD update 11 Compute the estimates r\u0302 ucb\u03b4,t and c\u0302 lcb \u03b4,t ;\n12 if wwwww ( t\u2211 \u03c4=Tk c\u03c4 \u2212 (t\u2212 Tk + 1) (B \u2212 b1) ) + wwwww > MT,\u03b4,k then // Box C part 13 Break inner for loop; // finish phase k and move to phase k + 1 14 Tk+1 = t+ 1; // record beginning of phase k + 1 15 end 16 end 17 end\n(ii) we bound by (1+ilog T ) \u221a T the excess cumulative costs with respect to T (B\u2212bT1) and deduce that the cumulative costs are smaller than B; (iii) we provide a regret bound, by summing the bounds guaranteed by Lemma 1 over each regime.\nThe favorable event Emeta is defined as follows. By construction, and thanks to the assumption of feasibility for B\u2212 2bT1, the results of Lemmas 1 and 2 hold with probability at least 1\u2212 \u03b4/(k+2)2 at each round of each regime k \u2a7e 0 that is actually achieved. Given that\n\u2211 k\u2a7e0\n1 (k + 2)2 \u2a7d \u2211 k\u2a7e0\n1\n(k + 1)(k + 2) = 1 ,\nwe may define Emeta as the event indicating that the (uniform-in-time) bounds of Lemmas 1 and 2 are satisfied within each of the regimes achieved.\nC.1 Step 1: Bounding the number of regimes achieved\nWe show that, on Emeta, if regime K = ilog \u2225\u03bb\u22c6B\u2212bT 1\u2225 is achieved, then this regime does not stop. Hence, on Emeta, at most K + 1 \u2a7d 1 + ilog \u2225\u03bb\u22c6B\u2212bT 1\u2225 regimes take place.\nIndeed, in regime K = ilog \u2225\u03bb\u22c6B\u2212bT 1\u2225, if it is achieved, the meta-strategy resorts to the Box B strategy with\n\u03b3K = 2K\u221a T \u2a7e max\n{ \u2225\u03bb\u22c6B\u2212bT 1\u2225, 1 } \u221a T .\nTherefore, the bound of Lemma 1 entails that on Emeta, for all t \u2a7e TK ,wwwwww ( t\u2211 \u03c4=TK c\u03c4 \u2212 (t\u2212 Tk + 1) (B \u2212 bT1) ) + wwwwww \u2a7d 1 + 3\u2225\u03bb \u22c6 B\u2212bT 1\u2225 \u03b3K + 20 \u221a d\u03a5T,\u03b4/(K+2)2\n\u2a7d 4 \u221a T + 20 \u221a d\u03a5T,\u03b4/(K+2)2 = MT,\u03b4,K .\nThis is exactly the contrary of the stopping condition of regime K: the latter thus cannot be broken.\nIn some bounds, we will further bound ilog \u2225\u03bb\u22c6B\u2212bT 1\u2225 by ilog T : this holds because the assumption of (B \u2212 2bT1)\u2013feasibility entails, by Lemma 3 and as OPT is always smaller than 1, the crude bound\n\u2225\u03bb\u22c6B\u2212bT 1\u2225 \u2a7d OPT(r, c,B \u2212 bT1)\u2212 OPT\n( r, c,B \u2212 (3/2)bT1 ) bT /2 \u2a7d 2 bT \u2a7d \u221a T 7 \u2a7d T ,\nwhere we used that bT \u2a7e 14/ \u221a T given its definition. (Of course, sharper but more complex bounds could be obtained; however, they would only improve logarithmic terms in the bound, which we do not try to optimize anyway.)\nC.2 Step 2: Bounding the cumulative costs\nWe still denote by K the index of the last regime and recall that K \u2a7d ilog \u2225\u03bb\u22c6B\u2212bT 1\u2225 \u2a7d ilog(T ), and that for k \u2a7e 0, regime k starts at Tk and stops at Tk+1\u22121. By convention, T0 = 1 and TK+1 = T+1. By the very definition of the stopping condition of regime k \u2a7e 0,wwwwww Tk+1\u22122\u2211 t=Tk ct \u2212 (Tk+1 \u2212 Tk \u2212 1) (B \u2212 bT1)  +\nwwwwww \u2a7d MT,\u03b4,k . For rounds of the form t = Tk+1 \u2212 1, we bound the Euclidean norm of ct \u2212 (B \u2212 bT1) by 2 \u221a d. Therefore, by a triangle inequality, satisfied both by the non-negative part ( \u00b7 )+ and the norm \u2225 \u00b7 \u2225 functions, we havewwwww ( T\u2211\nt=1\nct \u2212 T (B \u2212 bT1) ) + wwwww \u2a7d\nK\u2211 k=0 wwwwww Tk+1\u22122\u2211\nt=Tk\nct \u2212 (Tk+1 \u2212 Tk \u2212 1) (B \u2212 bT1)  + wwwwww+ K\u2211 k=0\nwwcTk+1\u22121 \u2212 (B \u2212 bT1)ww \u2a7d (K + 1) ( MT,\u03b4,ilog T + 2 \u221a d ) \u2a7d T bT ,\nwhere we used the fact that MT,\u03b4,k increases with k, the bound K \u2a7d ilog T proved in Step 1 and holding on the event Emeta, as well as the definition of bT . Therefore, on Emeta, no component of(\nT\u2211 t=1 ct \u2212 T (B \u2212 bT1) ) +\ncan be larger than T bT , which yields the desired control T\u2211\nt=1\nct \u2a7d TB.\nC.3 Step 3: Computing the associated regret bound\nThe total regret is the sum of the regrets suffered over each regime:\nRT = K\u2211 k=0 (Tk+1 \u2212 Tk)OPT(r, c,B)\u2212 Tk+1\u22121\u2211 t=Tk rt  .\nOn the favorable event Emeta, the bound of Lemma 2 holds in particular at the end of each regime; i.e., given the parameters \u03b3k = 2k/ \u221a T and \u03b4/ ( 4(k + 2)2 ) to run the Box B strategy in regime k \u2208 {0, . . . ,K}, it holds on Emeta that\n(Tk+1 \u2212 Tk) OPT(r, c,B)\u2212 Tk+1\u22121\u2211 t=Tk rt \u2a7d \u2225\u03bb\u22c6B\u2212bT 1\u2225 ( (Tk+1 \u2212 Tk) bT \u221a d+ 6\u03a5T,\u03b4/(k+2)2 ) + 36 \u221a d ( \u03a5T,\u03b4/(k+2)2 )2 2k\u221a T + 8 ln T 2 \u03b4/ ( 4(k + 2)2\n) . We now sum the above bounds and use the (in)equalities \u03a5T,\u03b4/(k+2)2 \u2a7d \u03a5T,\u03b4/(K+2)2 ,\nK\u2211 k=0 (Tk+1 \u2212 Tk) bT = T bT , and K\u2211 k=0 2k \u2a7d 2K+1 \u2a7d 21+ilog \u2225\u03bb \u22c6 B\u2212bT 1 \u2225 \u2a7d 4 \u2225\u03bb\u22c6B\u2212bT 1\u2225\nto get\nRT \u2a7d \u2225\u03bb\u22c6B\u2212bT 1\u2225 ( T bT \u221a d+ 6K\u03a5T,\u03b4/(K+2)2 ) + 144 \u221a d \u2225\u03bb\u22c6B\u2212bT 1\u2225 ( \u03a5T,\u03b4/(K+2)2 )2 \u221a T\n+ 8K ln T 2 \u03b4/ ( 4(K + 2)2 ) . The final regret bound is achieved by substituting the inequality K \u2a7d ilog T proved in Step 1:\nRT \u2a7d \u2225\u03bb\u22c6B\u2212bT 1\u2225\n( 144 \u221a d ( \u03a5T,\u03b4/(2+ilog T )2 )2 \u221a T + T bT \u221a d+ 6\u03a5T,\u03b4/(2+ilog T )2 ilog T )\n+ 8 ln T 2 \u03b4/ ( 4(2 + ilog T )2 ) ilog T . (19) The order of magnitude is \u221a T , up to poly-logarithmic terms, for the quantity \u03a5T,\u03b4/(2+ilog T )2 , thus for MT,\u03b4,ilog T , thus for T bT , therefore, the order of magnitude in \u221a T of the above bound is, up to\npoly-logarithmic terms, ( 1 + \u2225\u03bb\u22c6B\u2212bT 1\u2225 )\u221a T , as claimed."
        },
        {
            "heading": "D Proofs of Lemma 3 and Corollary 1",
            "text": "Proof of Lemma 3. For \u03bb \u2a7e 0 and C \u2208 [0, 1]d, we denote L(\u03bb,C) = EX\u223c\u03bd [ max a\u2208A { r(X, a)\u2212 \u2329 c(X, a)\u2212C, \u03bb \u232a}] so that by (4) and the feasibility assumption, we have, at least for C = B\u0303 and C = B \u2212 b1:\nOPT(r, c,C) = min \u03bb\u2a7e0\nL(\u03bb,C) = L(\u03bb\u22c6C ,C) .\nThe function L is linear in C, so that OPT ( r, c, B\u0303 ) = L ( \u03bb\u22c6 B\u0303 , B\u0303 ) \u2a7d L ( \u03bb\u22c6B\u2212b1, B\u0303 ) = L ( \u03bb\u22c6B\u2212b1,B \u2212 b1 ) \u2212 \u2329 \u03bb\u22c6B\u2212b1, B \u2212 b1\u2212 B\u0303 \u232a = OPT ( r, c,B \u2212 b1 ) \u2212 \u2329 \u03bb\u22c6B\u2212b1, B \u2212 b1\u2212 B\u0303 \u232a . The result follows from substituting\u2329 \u03bb\u22c6B\u2212b1, B \u2212 b1\u2212 B\u0303 \u232a \u2a7e \u2225\u03bb\u22c6B\u2212b1\u22251 min ( B \u2212 b1\u2212 B\u0303 ) \u2a7e \u2225\u03bb\u22c6B\u2212b1\u2225 min ( B \u2212 b1\u2212 B\u0303\n) and from rearranging the inequality thus obtained.\nProof of Corollary 1. We apply Lemma 3 with B\u0303 = \u03b51 for some \u03b5 > 0 sufficiently small and obtain\n\u2225\u03bb\u22c6B\u2212b1\u2225 \u2a7d OPT(r, c,B \u2212 b1)\u2212 OPT\n( r, c, \u03b51 ) min ( B \u2212 (b+ \u03b5)1\n) . We conclude by substituting OPT(r, c,B \u2212 b1) \u2a7d OPT(r, c,B) and OPT ( r, c, \u03b51 ) \u2a7e OPT ( r, c,0\n) as well as min(B \u2212 b1) \u2a7e minB/2, and by letting \u03b5 \u2192 0."
        },
        {
            "heading": "E Additional (sketch of) results concerning optimality",
            "text": "We detail here two series of claims made in Section 4 .\nE.1 A proof scheme for problem-dependent lower bounds We provide the proof scheme for proving the problem-dependent lower bound of order ( 1+\u2225\u03bb\u22c6B\u2225 )\u221a T announced in Section 4.\nStep 0: Considering strict cost constraints. Our aim, as described in Box A, is to make sure that with high probability the cumulative costs are smaller than T B. If we considered softer constraints, of the form T B + O\u0303 (\u221a T ) , then O\u0303 (\u221a T )\nregret bounds would be possible (see Appendix F); i.e., the factor \u2225\u03bb\u22c6B\u2212bT 1\u2225 of Theorem 1 could be replaced by a constant. Thus, lower bounds are only interesting in the case of hard constraints stated in Box A.\nStep 1: Necessity of a margin bT of order 1/ \u221a T . First, a classical lemma in CBwK (see, e.g., Agrawal and Devanur, 2016, Lemma 1) indicates that a sequence of adaptive cannot perform better than an optimal static policy. Denote by \u03c0\u22c6B\u2032 a (quasi-)optimal policy for the average cost constraints B\u2032. Provided that costs are truly random (i.e., do not stem from Dirac distributions, which in particular, does not cover the cases where there is a null-cost action, see Limitation 2 in Section 4), then the law of iterated logarithm shows that when playing \u03c0\u22c6B\u2032 at each round, the cumulative costs must (almost-surely, as T \u2192 +\u221e) be larger than T B\u2032 plus a positive term of the order of \u221a T ln lnT . Therefore, to meet the hard constraints, one should pick B\u2032 of the form B \u2212 bT1, where bT is of order 1/ \u221a T up to logarithmic terms.\nStep 2: Consequences in terms of regret. Therefore, the largest average reward a strategy may target is OPT(r, c,B \u2212 bT1). Deviations of the order \u221a T are also bound to happen. Therefore, up to logarithmic terms, the regret lower bound is approximatively larger than something of the order\nT ( OPT(r, c,B)\u2212 OPT(r, c,B \u2212 bT1) ) + \u221a T .\nNow, an argument similar to the one used in the proof of Lemma 3 shows that\nOPT(r, c,B)\u2212 OPT(r, c,B \u2212 bT1) \u2a7e L(\u03bb\u22c6B,B)\u2212 L(\u03bb\u22c6B,B \u2212 bT1) = bT \u2225\u03bb\u22c6B\u2225 .\nAll in all, the regret lower bound is thus approximatively larger than something of the order of( 1 + \u2225\u03bb\u22c6B\u2225 )\u221a T .\nThis matches the form of the bound of Theorem 1, but the dual vector \u03bb\u22c6B\u2212bT 1 present in the upper bound is replaced by \u03bb\u22c6B in our lower bound.\nE.2 Faster rates may be achievable in some specific cases\nWe explain here why, in some specific cases, faster rates would be achievable\u2014with \u221a T in the regret bound of Theorem 1 replaced by \u221a TB for a problem with scalar costs and total-cost constraints B.\nIndeed, consider a problem similar to Example 1: with scalar costs, total-cost constraint B > 0, featuring a baseline action anull with null cost but also null reward, and additional actions with larger rewards and expected costs c(x, a) \u2a7e \u03b1 > 0. Let NT denotes the number of times non-null cost actions are played within the first T rounds. Deviation inequalities have it that\nT\u2211 t=1 ct \u2a7e \u03b1NT \u2212 O\u0303 (\u221a NT ) ,\nwhere we recall that O\u0303 is up to poly-logarithmic factors; as a consequence, the total-cost constraint enforces that\nNT \u2a7d TB\n\u03b1 + O\u0303\n(\u221a TB\n\u03b1\n) .\nIn particular, the margin bT only needs to be of order \u221a NT /T , i.e., \u221a B/(\u03b1T ) , instead of 1/ \u221a T .\nSimilarly, since at most NT non-null actions are played, the regret should be lower bounded by something of the order of\nT ( OPT(r, c, B)\u2212 OPT(r, c, B \u2212 bT ) ) + \u221a NT ,\nwhere, as in Step 2 above,( OPT(r, c, B)\u2212 OPT(r, c, B \u2212 bT ) ) \u2a7e bT |\u03bb\u22c6B | = O\u0303\n(\u221a B\n\u03b1T\n) |\u03bb\u22c6B |.\nThis suggests a lower bound on the regret of the order (up to poly-logarithmic factors) of( |\u03bb\u22c6B |+ 1 )\u221a TB/\u03b1 instead of ( 1 + |\u03bb\u22c6B | )\u221a T .\nThe difference between the two bounds is significant when B is small, i.e., B \u226a 1. While proving a matching upper bound with the Box B and Box C strategies looks a bit tricky, we feel that it must be possible to do so with the primal approach of Appendix F, at least when the context space X is finite. If our intuition holds, then it would be possible to get an upper bound\nRT \u2a7d O\u0303 ( TbT ( 1 + |\u03bb\u2217B\u2212bT | )) = O\u0303 (\u221a TB\n\u03b1\n( 1 +\nOPT(r, c, B)\u2212 OPT(r, c, 0) B\n)) = O\u0303 (\u221a TB/\u03b13 ) ,\nwhere the last bound follows from OPT(r, c, B)\u2212 OPT(r, c, 0) \u2a7d B/\u03b1, as explained in Example 1."
        },
        {
            "heading": "F Primal strategy",
            "text": "This section studies the primal strategy stated in Box D, which, at every round, solves an approximation of the primal optimization problem (1). The key issue in running such a primal approach is estimating \u03bd, see comments after Notation 1; this primal approach is essentially worth for the case of finite context sets X . The aim of this section is threefold:\n1. In Appendix F.1, we provide a theory of \u201csoft\u201d constraints, when total-cost deviations from TB of order \u221a T up to logarithmic terms are allowed; at least when X is a finite set, the\nregret bound then becomes proportional to \u221a T up to logarithmic terms.\n2. In Appendix F.2, we revisit and extend the results by Li and Stoltz [2022]. The extension consists of dealing with possibly signed constraints, and the revisited analysis (of the same strategy as in Li and Stoltz, 2022) consists in not directly dealing with KKT constraints (which, in addition, imposed the finiteness of X ) but in only relating optimization problems\u2014 defined with the true r and c or estimates thereof. We also offer a modular approach and separate the error terms coming from estimating \u03bd and from estimating r and c.\n3. In Appendix F.3, we generalize the results of Appendix F.2, which rely on the existence of a null-cost action, and get guarantees that correspond quite exactly to the combination of Theorem 1 and the interpretation thereof offered by Lemma 3, at least in the case of a finite X . Actually, in our research path, we had first obtained these primal results, before trying to obtain them in a more general case of a continuous X , by resorting to a dual strategy. The proof technique in Appendix F.3 also inspired our approach to proving problem-dependent lower bounds presented in Appendix E.1.\nThroughout this appendix, we will assume that the context distribution \u03bd can be estimated in some way. We provide examples and pointers below. Notation 1. Fix \u03b4 \u2208 (0, 1). We denote by \u03bd\u0302\u03b4,t a sequence of estimators of \u03bd, each constructed on the contexts x1, . . . ,xt, and by \u03bet,\u03b4 a sequence of estimation errors such that, with probability at least 1\u2212 \u03b4, for all bounded functions f : X \u2192 [\u22121, 1],\u2223\u2223\u2223EX\u223c\u03bd[f(X)]\u2212 EX\u223c\u03bd\u0302\u03b4,t[f(X)]\u2223\u2223\u2223 \u2a7d \u03bet,\u03b4 . We also denote \u039eT,\u03b4 =\nT\u2211 t=1 \u03bet\u22121,\u03b4 .\nWe consider the strategy described in Box D, whether X is finite (as in Li and Stoltz, 2022, where it was first considered) or not. Our simplified analyses do not rely on explicit KKT inequalities and therefore do not require anymore that X is finite. In Box D, by \u201cwhen the empirical cost constraints are feasible\u201d, we mean that there exists some policy \u03c0 such that\nEX\u223c\u03bd\u0302\u03b4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4,t\u22121(X, a)\u03c0a(X) ] \u2a7d B + bt1 .\nWe need to guarantee the existence of a policy achieving the constrained maximum of Step 2 in Box D. This is immediate when X is finite, as the problem then corresponds to a finite-dimensional linear program. In general, we may note that when (4) holds, we read therein the optimal policy, as a pointwise maximum involving the optimal dual variables. The proofs reveal that up to slightly augmenting the \u03bet\u22121,\u03b4 of Notation 1, we may assume that the strict feasibility sufficient for (4) indeed holds.\nWe also note that even if the argument of the maximum \u03c0t is guaranteed to exist, it may be difficult to compute: the linear program of Box D cannot be solved exactly if the \u03bd\u0302\u03b4,t are not finitely supported (which happens in general when X is not finite). We do not see this as a severe issues as the Box D strategy is an ideal strategy anyway, that we study for the sake of shedding lights on our results for the dual approach in the main body of the article.\nA final remark on the Box D strategy is that the margins on the average cost constraints B can now be signed, which is why they will be referred to as signed slacks.\nBOX D: PRIMAL CBWK STRATEGY, GENERALIZED FROM LI AND STOLTZ [2022]\nInputs: confidence level 1\u2212 \u03b4; estimation procedure and error functions \u03b5t of Assumption 2; optimistic estimates (2); estimation procedure for \u03bd as in Notation 1\nHyperparameter: signed slacks b1, b2, . . . , bT \u2208 R\nInitialization: initial estimates r\u0302 ucb\u03b4,0 (x, a) and c\u0302 lcb \u03b4,0(x, a), as well as \u03bd\u0302\u03b4,0\nFor rounds t = 1, 2, 3, . . . , T : 1. Compute a policy \u03c0t achieving\nmax \u03c0:X\u2192P(A) EX\u223c\u03bd\u0302\u03b4,t [\u2211 a\u2208A r\u0302 ucb\u03b4,t\u22121(X, a)\u03c0a(X) ]\nunder EX\u223c\u03bd\u0302\u03b4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4,t\u22121(X, a)\u03c0a(X) ] \u2a7d B + bt1\nwhen the empirical cost constraints are feasible, and pick an arbitrary policy \u03c0t otherwise;\n2. Observe xt and draw an action at \u223c \u03c0t(xt); 3. Compute the estimate r\u0302 ucb\u03b4,t (x, a) and c\u0302 lcb \u03b4,t (x, a), as well as \u03bd\u0302\u03b4,t.\nDiscussion of the estimation of \u03bd. The simplest case is when X is a finite set; in this case, we may take\n\u03bet,\u03b4 of order\n\u221a |X | ln(1/\u03b4)\nt ,\nwhere X denotes the cardinality of X ; see [Ai et al., 2022, Section 4.1], see also a less sharp bound based on Hoeffding\u2019s inequality in [Li and Stoltz, 2022, Section 5]. This leads to a total error term \u039eT,\u03b4 of order \u221a T up to poly-logarithmic term.\nWhen X is a continuous subset of Rn, some regularity conditions are put on \u03bd, which is typically assumed to have some smooth density with respect to the Lebesgue measure m. Estimates \u03bd\u0302\u03b4,t are obtained by estimating the density d\u03bd/dm: the criterion in Notation 1, which is proportional to the total-variation distance between \u03bd and \u03bd\u0302\u03b4,t, is then given by the L1(m) distance between the two\ndensities. To control the latter with uniform convergence rates, so as to obtain deviation terms \u03bet,\u03b4 only depending on t and \u03b4, heavy assumptions on the model to which \u03bd belongs are in order, e.g., some H\u00f6lderian regularity, and uniform estimation rates obtained degrade with the ambient dimension n; they are generally much slower than 1/ \u221a t. The total error terms \u039eT,\u03b4 then prevent the bounds stated below in Proposition 1 and Theorems 3 and 4 from sharing the same orders of magnitude than the bounds proved with our dual approach in Theorem 1 and interpreted in Section 3.3. On this topic, see also [Ai et al., 2022, Section 4.1] for the estimation rates as well as a similar description in Han et al. [2022, end of Section 1] of the limitation of the primal approach in CBwK due to the estimation of densities. General references on density estimations are the monographs by Devroye and Gy\u00f6rfi [1985] in L1 and Tsybakov [2008] in L2. Note that in the dual approach, the knowledge of (the possibly complex) \u03bd is replaced by the knowledge of the (finite-dimensional) optimal dual variables \u03bb\u22c6B \u2208 Rd, which is easier to learn. This explains the fundamental efficiency of the dual approach compared to the primal approach.\nF.1 Analysis with \u201csoft\u201d constraints\nWe first provide an analysis for a version of the Box D strategy that may possibly breach the total-cost constraints TB. More precisely, we allow deviations to TB of the order of \u221a T times poly-log factors: this is what we refer to as \u201csoft\u201d constraints. Our result is that the regret may then be bounded by a quantity of order \u221a T times poly-log factors, at least when X is finite.\nWe do so for two reasons: first, because we do not think that this is a well-known result, and second, for pedagogic reasons, as the proof for \u201chard\u201d constraints follows from adapting the proof scheme for soft constraints (see Appendix F.2). Proposition 1 (soft constraints). Fix \u03b4 \u2208 (0, 1). Under Assumption 2 and with Notation (1), the strategy of Box D, run with \u03b4/4 and positive slacks bt = \u03bet\u22121,\u03b4/4, ensures that with probability at least 1\u2212 \u03b4, T\u2211\nt=1\nct \u2a7d TB + ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + 2\u039eT,\u03b4/4 ) 1 and RT \u2a7d 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + 2\u039eT,\u03b4/4 ,\nwhere \u03b1T,\u03b4/4 = \u221a 2T ln ( (d+ 1)/(\u03b4/4) ) .\nIn particular, when X is finite, the deviation terms 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + 2\u039eT,\u03b4/4 are of order \u221a T up to poly-logarithmic terms, so that the bound of Proposition 1 reads: with high-probability, T\u2211\nt=1\nct \u2a7d TB + O\u0303 (\u221a T )\nand RT \u2a7d O\u0303 (\u221a T ) .\nPut differently, soft-constraint satisfaction allows for O\u0303 (\u221a T ) regret bounds when X is finite.\nProof. As in the proofs of Lemmas 1 and 2 in Appendix B, we consider four events, each of probability at least 1\u2212 \u03b4/4: two events EH-Az1 and EH-Az2, defined below, following from applications of the Hoeffding-Azuma inequality, the favorable event ETVD of Notation 1 with \u03b4/4, and the favorable event E\u03b2 of Assumption 2 with \u03b4/4. Namely, given the value for \u03b1T,\u03b4/4 proposed in the statement (note this value is slightly different from the one considered in Appendix B), we have, on the one hand, on EH-Az1,\nT\u2211 t=1 ct \u2a7d \u03b1T,\u03b4/41+ T\u2211 t=1 c(xt, at) and T\u2211 t=1 rt \u2a7e \u2212\u03b1T,\u03b4/4 + T\u2211 t=1 r(xt, at) , (20)\nand on the other hand, on EH-Az2, T\u2211\nt=1\nc\u0302 lcb\u03b4/4,t\u22121(xt, at) \u2a7d \u03b1T,\u03b4/41+ T\u2211\nt=1\nEX\u223c\u03bd [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ]\nand T\u2211\nt=1\nr\u0302 ucb\u03b4/4,t\u22121(xt, at) \u2a7e \u2212\u03b1T,\u03b4/4 + T\u2211\nt=1\nEX\u223c\u03bd [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] . (21)\nThe first two inequalities are obtained by considering conditional expectations with respect to the past, xt and at, while the second two inequalities follow from taking conditional expectations with respect to the past and xt; we crucially use that, by definition, xt \u223c \u03bd is independent from \u03c0t and the estimates r\u0302 ucb\u03b4/4,t\u22121 and c\u0302 lcb \u03b4/4,t\u22121.\nWe first note that by B\u2013feasibility of the problem, by (3), and by Notation (1), a policy \u03c0t satisfying the constraints stated in Box D exists at each round t \u2a7e 1 on the event E\u03b2 \u2229 ETVD. Indeed, denoting by \u03c0feas such a B\u2013feasible policy, this existence follows from the inequalities\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0 feas a (X) ] \u2a7d EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c(X, a)\u03c0feasa (X) ]\n\u2a7d \u03bet\u22121,\u03b4/4 + EX\u223c\u03bd [\u2211 a\u2208A c(X, a)\u03c0feasa (X) ] \ufe38 \ufe37\ufe37 \ufe38\n\u2a7dB\n(22)\nand from the choice bt = \u03bet\u22121,\u03b4/4.\nCost-wise, we then successively have, by (20), then (3) and Assumption 2, and finally (21) and Notation (1), on the event EH-Az1 \u2229 E\u03b2 \u2229 EH-Az2 \u2229 ETVD of probability at least 1\u2212 \u03b4,\nT\u2211 t=1 ct \u2a7d \u03b1T,\u03b4/41+ T\u2211 t=1 c(xt, at)\n\u2a7d ( \u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 ) 1+ T\u2211 t=1 c\u0302 lcb\u03b4/4,t\u22121(xt, at) \u2a7d ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 ) 1+\nT\u2211 t=1 EX\u223c\u03bd [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ]\n\u2a7d ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + \u039eT,\u03b4/4 ) 1+ T\u2211 t=1 EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] \ufe38 \ufe37\ufe37 \ufe38\n\u2a7dB+bt1\n,\nwhere the inequalities \u2a7d B + bt1 follow from the definition of \u03c0t in Box D. Substituting the choice bt = \u03bet\u22121,\u03b4/4, we thus proved that on EH-Az1 \u2229 E\u03b2 \u2229 EH-Az2 \u2229 ETVD,\nT\u2211 t=1 ct \u2a7d TB + ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + 2\u039eT,\u03b4/4 ) 1 ,\nas claimed.\nThe control for rewards mimics the steps above for costs (and then resorts to an additional argument). We obtain first that on EH-Az1 \u2229 E\u03b2 \u2229 EH-Az2 \u2229 ETVD,\nT\u2211 t=1 rt \u2a7e \u2212 ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + \u039eT,\u03b4/4 ) + T\u2211 t=1 EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] .\nWe denote by \u03c0\u22c6B an optimal static policy for the average cost constraints B\u2014when it exists, e.g., by (4), as soon as the problem is feasible for some B\u2032 < B; otherwise, we take a static policy achieving OPT(r, c,B) up to some small e > 0, which we let vanish in a final step of the proof. As in (22), we have, on E\u03b2 \u2229 ETVD,\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0 \u22c6 B,a(X) ] \u2a7d EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c(X, a)\u03c0\u22c6B,a(X) ]\n\u2a7d \u03bet\u22121,\u03b4/41+ EX\u223c\u03bd [\u2211 a\u2208A c(X, a)\u03c0\u22c6B,a(X) ] \ufe38 \ufe37\ufe37 \ufe38\n\u2a7dB\n,\nwhere the \u2a7d B inequality follows from the definition of \u03c0\u22c6B . Thanks to the choice bt = \u03bet\u22121,\u03b4/4, we have, by definition of \u03c0t as some optimal policy in Box D and on E\u03b2 \u2229 ETVD,\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] \u2a7e EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0 \u22c6 B,a(X) ] .\nAgain by (3) and Notation (1), we have, on E\u03b2 \u2229 ETVD,\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0 \u22c6 B,a(X) ] \u2a7e EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r(X, a)\u03c0\u22c6B,a(X) ]\n\u2a7e \u2212\u03bet\u22121,\u03b4/4 + EX\u223c\u03bd [\u2211 a\u2208A r(X, a)\u03c0\u22c6B,a(X) ] \ufe38 \ufe37\ufe37 \ufe38\n=OPT(r,c,B)\n.\nCollecting all bounds, we proved that on EH-Az1 \u2229 E\u03b2 \u2229 EH-Az2 \u2229 ETVD, T\u2211\nt=1\nrt \u2a7e TOPT(r, c,B)\u2212 ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + 2\u039eT,\u03b4/4 ) ,\nwhich corresponds to the claimed regret bound.\nF.2 Analysis with \u201chard\u201d constraints and a null-cost action\nWe now turn our attention to the main kind of result that we want to achieve: when constraints must be strictly satisfied\u2014which we refer to as \u201chard\u201d constraints. For the sake of simplicity, we do so for now in the presence of a null-cost action; Appendix F.3 explains how the analysis may be generalized to cases without such null-cost actions.\nThe following result corresponds to the combination of Theorem 1 with Corollary 1, and also, to Li and Stoltz [2022, main result: Theorem 2]. Theorem 3 (hard constraints). Fix \u03b4 \u2208 (0, 1). We consider the strategy of Box D, run with \u03b4/4 and negative slacks all equal to\nbt \u2261 \u2212\u2206T,\u03b4/4 , where \u2206T,\u03b4/4 def =\n2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + \u039eT,\u03b4/4\nT and \u03b1T,\u03b4/4 = \u221a 2T ln ( (d+ 1)/(\u03b4/4) ) . Assume that a null-cost action exists, that \u2206T,\u03b4/4 < minB, that Assumption 2 holds, and use Notation (1). Then, with probability at least 1\u2212 \u03b4, T\u2211\nt=1\nct \u2a7d TB and RT \u2a7d ( 2\u03b1T,\u03b4/4+\u03b2T,\u03b4/4+2\u039eT,\u03b4/4 )( 1 +\nOPT(r, c,B)\u2212 OPT(r, c,0) minB\n) .\nProof. We explain how the proof of Proposition 1 may be adapted. We first justify the existence at each round t \u2a7e 1 of a policy \u03c0t satisfying the cost constraints stated in Box D: for the null-cost action anull, we may impose that, for all x \u2208 X , all t \u2a7e 0, and all \u03b4 \u2208 (0, 1),\nc\u0302t(x, anull) = 0 and \u03b5t(x, anull, \u03b4) = 0 , so that c\u0302 lcb \u03b4,t (x, anull) = 0 a.s.;\nthis shows that at least the static policy \u03c0null always playing anull satisfying the defining cost-constraints for \u03c0t. (Alternatively, we may note that the policy \u03c0t defined below also satisfies the cost constraints stated in Box D, on the high-probability event considered.)\nWe then handle total-cost constraints similarly as in the proof of Proposition 1 and obtain that on the event EH-Az1 \u2229 E\u03b2 \u2229 EH-Az2 \u2229 ETVD of probability at least 1\u2212 \u03b4, T\u2211\nt=1\nct \u2a7d ( 2\u03b1T,\u03b4/4 +\u03b2T,\u03b4/4 +\u039eT,\u03b4/4 ) 1+ T\u2211 t=1 EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] \ufe38 \ufe37\ufe37 \ufe38\n\u2a7dB\u2212\u2206T,\u03b4/41\n\u2a7d TB ,\nwhere this time, the slacks bt = \u2212\u2206T,\u03b4/4 are negative and were set to cancel out the positive deviation terms. Similarly, on EH-Az1 \u2229 E\u03b2 \u2229 EH-Az2 \u2229 ETVD,\nT\u2211 t=1 rt \u2a7e \u2212T\u2206T,\u03b4/4 + T\u2211 t=1 EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] . (23)\nNow, the main modification to the proof of Proposition 1 is that (with its notation) we rather consider the policy\n\u03c0t = (1\u2212 wt)\u03c0\u22c6B + wt\u03c0null , where wt = min\n{ \u2206T,\u03b4/4 + \u03bet\u22121,\u03b4/4\nminB , 1\n} .\nAs in (22), we see that this policy satisfies, on E\u03b2 \u2229 ETVD:\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] \u2a7d EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c(X, a)\u03c0t,a(X) ]\n\u2a7d \u03bet\u22121,\u03b4/41+ EX\u223c\u03bd [\u2211 a\u2208A c(X, a)\u03c0t,a(X) ] \ufe38 \ufe37\ufe37 \ufe38\n\u2a7d(1\u2212wt)B\n.\nBy using B \u2a7e (minB)1 and since we assumed that \u2206T,\u03b4/4 < minB, we may continue the series of inequalities as follows:\n\u03bet\u22121,\u03b4/41+ (1\u2212 wt)B \u2a7d B + \u03bet\u22121,\u03b4/41\u2212 wt(minB)1 = B \u2212min { \u2206T,\u03b4/4, minB \u2212 \u03bet\u22121,\u03b4/4 } 1\n\u2a7d B \u2212min { \u2206T,\u03b4/4, \u2206T,\u03b4/4 \u2212 \u03bet\u22121,\u03b4/4 } 1 = B + bt1 ,\nmeaning that \u03c0t is a policy satisfying the constraints stated in Step 2 of Box D on round t \u2a7e 1. The consequence is that, first by definition of \u03c0t as a maximizer and then by the optimistic estimates (3) and Notation (1), on E\u03b2 \u2229 ETVD,\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ]\n\u2a7e EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ]\n\u2a7e \u2212\u03bet\u22121,\u03b4/4 + EX\u223c\u03bd [\u2211 a\u2208A r(X, a)\u03c0t,a(X) ] = \u2212\u03bet\u22121,\u03b4/4 + (1\u2212 wt)OPT(r, c,B) + wtOPT(r, c,0)\n= OPT(r, c,B)\u2212 \u03bet\u22121,\u03b4/4 \u2212min\n{ \u2206T,\u03b4/4 + \u03bet\u22121,\u03b4/4\nminB , 1\n}( OPT(r, c,B)\u2212 OPT(r, c,0) ) ,\nwhere OPT(r, c,0) refers to the expect reward achieved by the null-cost action anull. After combination with (23) and summation, we get that on EH-Az1 \u2229 E\u03b2 \u2229 EH-Az2 \u2229 ETVD:\nT\u2211 t=1 rt \u2212 T OPT(r, c,B)\n\u2a7e \u2212T\u2206T,\u03b4/4 \u2212 \u039eT,\u03b4/4 \u2212 T\u2211\nt=1\nmin\n{ \u2206T,\u03b4/4 + \u03bet\u22121,\u03b4/4\nminB , 1\n}( OPT(r, c,B)\u2212 OPT(r, c,0)\n) \u2a7e \u2212 ( T\u2206T,\u03b4/4 + \u039eT,\u03b4/4 )( 1 +\nOPT(r, c,B)\u2212 OPT(r, c,0) minB\n) ,\nwhich corresponds to the stated regret bound.\nF.3 General analysis with \u201chard\u201d constraints\nWe finally generalize Theorem 3 and get a result corresponding to Theorem 1 combined with Lemma 3.\nHere, we \u201cmix\u201d the slacks +\u03bet\u22121,\u03b4/4 and \u2212\u2206T,\u03b4/4 of Appendices F.1 and F.2, with a slight modification of \u2206T,\u03b4/4 to compensate for the \u03bet\u22121,\u03b4/4: we rather have a 2\u039eT,\u03b4/4 term in the numerator of \u2206 \u2032 T,\u03b4/4, compared to the \u039eT,\u03b4/4 term in the numerator of \u2206T,\u03b4/4. Theorem 4 (hard constraints). Fix \u03b4 \u2208 (0, 1). We consider the strategy of Box D, run with \u03b4/4 and signed slacks (depending on t \u2a7e 1)\nbt = \u2212\u2206 \u2032 T,\u03b4/4 + \u03bet\u22121,\u03b4/4 where \u2206 \u2032 T,\u03b4/4 = 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + 2\u039eT,\u03b4/4\nT and \u03b1T,\u03b4/4 = \u221a 2T ln ( (d+ 1)/(\u03b4/4) ) . Assume that the problem is (B \u2212 m)\u2013feasible for some m that does not need to be known by the strategy, with B \u2212 m \u2a7d B \u2212 \u2206\u2032T,\u03b4/41. Then, under Assumption 2 and with Notation (1), with probability at least 1\u2212 \u03b4,\nT\u2211 t=1 ct \u2a7d TB\nand RT \u2a7d ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + 2\u039eT,\u03b4/4 )( 1 +\nOPT(r, c,B)\u2212 OPT(r, c,B \u2212m) minm\n) .\nProof. We rather sketch the differences to the proofs of Theorem 3 and Proposition 1. We denote by Eall the event of probability at least 1 \u2212 \u03b4 obtained as the intersection of four convenient events of probability each at least 1\u2212 \u03b4/4. All inequalities below hold on Eall but for the sake of brevity, we will not highlight this fact each time.\nWe denote by \u03c0feas a (quasi-)optimal static policy among the ones achieving expected costs smaller than B \u2212m; it therefore ensures that its expected costs are smaller than B \u2212m and achieves an expected reward of OPT(r, c,B \u2212m)\u2212 e, possibly up to a small factor e \u2a7e 0 which we let vanish. We note that for each round t \u2a7e 1,\nEX\u223c\u03bd\u0302\u03b4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4,t\u22121(X, a)\u03c0 feas a (X) ] \u2a7d \u03bet\u22121,\u03b4/T1+ EX\u223c\u03bd [\u2211 a\u2208A c(X, a)\u03c0feasa (X) ] \u2a7d \u03bet\u22121,\u03b4/T1+B \u2212m1 \u2a7d B + bt1 ,\ngiven the definition bt = \u2212\u2206 \u2032 T,\u03b4/4 + \u03bet\u22121,\u03b4/4 and the assumption B \u2212m \u2a7d B \u2212\u2206 \u2032 T,\u03b4/41. Thus, on Eall, the strategy \u03c0t is indeed defined at each round t \u2a7e 1 by the optimization problem stated in Step 2 of Box D (and not in an arbitrary manner).\nCost-wise, we thus have T\u2211\nt=1\nct \u2a7d ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + \u039eT,\u03b4/4 ) 1+ T\u2211 t=1\n\u2a7dB+bt1\ufe37 \ufe38\ufe38 \ufe37 EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] \u2a7d TB + ( 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + \u039eT,\u03b4/4 ) 1\u2212 T\u2206\u2032T,\u03b4/4 + \u039eT,\u03b4/4 = TB ,\nwhere the final equality follows from the definition of \u2206 \u2032 T,\u03b4/4, which involves a 2\u039eT,\u03b4/4 term in its numerator.\nReward-wise, we introduce for each t \u2a7e 1,\n\u03c0t = (1\u2212 wt)\u03c0\u22c6B + wt\u03c0feas , where wt = min\n{ \u2206 \u2032 T,\u03b4/4\nminm , 1\n} ,\nwhose empirical expected cost at round t \u2a7e 1 is smaller than\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A c\u0302 lcb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ] \u2a7d \u03bet\u22121,\u03b4/41+ EX\u223c\u03bd [\u2211 a\u2208A c(X, a)\u03c0t,a(X) ] \u2a7d \u03bet\u22121,\u03b4/41+ (1\u2212 wt)B + wt(B \u2212m) = B + \u03bet\u22121,\u03b4/41\u2212 wtm .\nBy using m \u2a7e (minm)1 and thanks to the assumption B \u2212 m \u2a7d B \u2212 \u2206\u2032T,\u03b4/41, which can be equivalently formulated as minm \u2a7e \u2206 \u2032 T,\u03b4/4, we may continue this series of inequalities by\nB + \u03bet\u22121,\u03b4/41\u2212 wtm \u2a7d B \u2212min { \u2206 \u2032 T,\u03b4/4 \u2212 \u03bet\u22121,\u03b4/4, minm\u2212 \u03bet\u22121,\u03b4/4 } 1\n\u2a7d B \u2212min { \u2206 \u2032 T,\u03b4/4 \u2212 \u03bet\u22121,\u03b4/4, \u2206 \u2032 T,\u03b4/4 \u2212 \u03bet\u22121,\u03b4/4 } 1 = B + bt1 ,\nmeaning that \u03c0t is a policy satisfying the constraints stated in Step 2 of Box D on round t \u2a7e 1. In particular, by definition of \u03c0t as a maximizer,\nEX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ]\n\u2a7e EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ]\n\u2a7e \u2212\u03bet\u22121,\u03b4/4 + EX\u223c\u03bd [\u2211 a\u2208A r(X, a)\u03c0t,a(X) ] \u2a7e \u2212\u03bet\u22121,\u03b4/4 + ( (1\u2212 wt)OPT(r, cB) + wtOPT(r, cB \u2212m)\n) = OPT(r, cB)\u2212min { \u2206 \u2032 T,\u03b4/4\nminm , 1\n}( OPT(r, cB)\u2212 OPT(r, cB \u2212m) ) \u2212 \u03bet\u22121,\u03b4/4 . (24)\nFinally, by combining (23) and (24),\nT\u2211 t=1 rt \u2a7e \u2212 ( =T\u2206\u2032T,\u03b4/4\u2212\u039eT,\u03b4/4\ufe37 \ufe38\ufe38 \ufe37 2\u03b1T,\u03b4/4 + \u03b2T,\u03b4/4 + \u039eT,\u03b4/4 ) 1+ T\u2211 t=1 EX\u223c\u03bd\u0302\u03b4/4,t\u22121 [\u2211 a\u2208A r\u0302 ucb\u03b4/4,t\u22121(X, a)\u03c0t,a(X) ]\n\u2a7e T OPT(r, cB) + \u039eT,\u03b4/4 \u2212 T\u2206 \u2032 T,\u03b4/4 ( 1 +\nOPT(r, cB)\u2212 OPT(r, cB \u2212m) minm\n) \u2212 \u039eT,\u03b4/4,\nas claimed."
        },
        {
            "heading": "G Numerical experiments: full description",
            "text": "This appendix reports numerical simulations performed on simulated data with the motivating example described in Chohlas-Wood et al. [2021] and alluded at in Section 2.1. These simulations are for the sake of illustration only.\nA brief summary of the applicative background in AI for justice is the following. The learner wants to maximize the total number of appearances to court for people of concern. To achieve this goal, the learner is able to provide, or not, some transportation assistance: rideshare assistance (the highest level of help), or a transit voucher (a more modest level of help). There are of course budget limits on these assistance means, and the learner also wants to control how (un)fair the assistance policy is, in terms of subgroups of the population, while maximizing the total number of appearances. Some subgroups take a better advantage of assistance to appear in court, thus, without the fairness control, all assistance would go to these groups. The fairness costs described in Section 2.1 force the learner to perform some tradeoff between spending all assistance on most reactive subgroups and spending it equally among subgroups.\nOutline of this appendix. We first recall the experimental setting of Chohlas-Wood et al. [2021]\u2014 in particular, how contexts, rewards, and costs are generated in their simulations (we cannot replicate their study on real data, which is not accessible). We then specify the strategies we implemented and how we tuned them\u2014this includes describing the estimation procedure discussed in Section 2.2. We finally report the performance observed, in terms of (average) rewards and costs.\nG.1 The experimental setting of Chohlas-Wood et al. [2021]\nWe follow strictly the experimental setting of Chohlas-Wood et al. [2021] as provided in the public repository https://github.com/stanford-policylab/learning-to-be-fair. This experimental setting, as reverse-engineered from the code, deals with purely simulated data and seems actually inconsistent with the description made in Chohlas-Wood et al. [2021, Section 5.4 and Appendix E], which would anyway rely on proprietary data that could not be made public.\nContext generation. Each individual is described by four variables x: age, proximity, poverty, and group, which are to be read in x in its components, referred to as xage, xprox, xpov and gr(x) = xgroup. The first three variables are assumed to be normalized and are simulated independently, according to uniform distributions on [0, 1]. Groups are also simulated independently of these three variables: two groups are assumed, with respective probabilities of 1/2. This defines the context distribution \u03bd.\nAs we describe now, assistance has stronger impact on group 0 than in group 1.\nCost generation. We recall that three actions are available: offering some rideshare assistance (action aride), providing a transportation voucher (action avoucher), or providing no help (which is a control situation: action acontrol). The associated spending costs are deterministic and do not depend on x, and there are two separate budgets for rideshares and vouchers. More precisely, at round t, upon taking action at, the following deterministic spending costs are suffered:\ncride(xt, at) = 1{at=aride} and cvoucher(xt, at) = 1{at=avoucher} , where the corresponding average budgets are Bride = 0.05 and Bvoucher = 0.20.\nWe measure the extent of unfair allocation of spendings among groups with the following eight fairness costs: a first series of four fairness costs is given by\n21{at=aride}1{gr(xt)=0} \u2212 1{at=aride} , 21{at=aride}1{gr(xt)=1} \u2212 1{at=aride} , 21{at=avoucher}1{gr(xt)=0} \u2212 1{at=avoucher} , 21{at=avoucher}1{gr(xt)=1} \u2212 1{at=avoucher} ,\nand the second series is given by the opposite values of these costs. We denote by \u03c4 the corresponding average cost constraints \u03c4 , and set \u03c4 = 10\u22127 or \u03c4 = 0.025 in our experiments.\nAll in all, the global (spending and fairness) vector costs ct takes deterministic values in R10. The vector cost function c is fully known to the learner, and no estimation is needed.\nReward generation. The rewards are binary: rt = 1 if the t\u2013individual appeared in court, and rt = 0 otherwise. That is, the expected reward equals the probability of appearance. A logistic regression model is assumed: denoting by \u03a6(u) = 1/(1 + eu), we assume\nr(x, acontrol) = \u03a6(\u2212xage), r(x, avoucher) = \u03a6(\u2212xage + 2xprox)1{gr(x)=0} +\u03a6(\u2212xage + xprox)1{gr(x)=1} , r(x, aride) = \u03a6(\u2212xage + 4xpov)1{gr(x)=0} +\u03a6(\u2212xage + 2xpov)1{gr(x)=1} .\nWe may write these six equalities in a compact format as: r(x, a) = \u03a6 ( \u03c6(x, a)T \u00b5\u22c6 ) where \u03c6(x, a) =\n xage xprox 1{a=avoucher} xprox 1{a=avoucher} 1{gr(x)=0} xpov 1{a=aride} xpov 1{a=aride} 1{gr(x)=0}  and \u00b5\u22c6 =  \u22121 1 1 2 2  . The learner ignores the coefficients \u00b5\u22c6 of this structure and only knows that expected rewards are of the form \u03a6 ( \u03c6(x, a)T \u03b8 ) for some parameter \u03b8 \u2208 R5. The learner will estimate the reward function r by estimating \u00b5\u22c6.\nReward estimation. We deal with a logistic model and follow the methodology described in Li and Stoltz [2022]; see Modeling 2 in Section 2.2. In particular, the parameters \u00b5\u22c6 are estimated, after each round t \u2a7e 1, thanks to the maximum likelihood estimator\n\u00b5\u0302t \u2208 argmax \u00b5\u2208R5 t\u2211 s=1 rs\u03a6 ( \u03c6(as,xs) T \u00b5 ) + (1\u2212 rs) ln ( 1\u2212 \u03a6 ( \u03c6(as,xs) T \u00b5 )) \u2212 \u03bb logistic 2 \u2225\u00b5\u22252 ,\nwhere \u03bblogistic \u2a7e 0 is a regularization factor. We define as r\u0302t(x, a) = \u03a6 ( \u03c6(x, a)T \u00b5\u0302t ) the estimated expected rewards, and the associated uniform estimation errors (see Assumption 2) are of the form\n\u03b5t(x, a, \u03b4) = C\u03b4 ( 1 + ln t )\u221a \u03c6(a,x)T V \u22121t \u03c6(a,x)\nwhere Vt = t\u2211\ns=1\n\u03c6(as,xs)\u03c6(as,xs) T + \u03bblogisticI5 ,\nwhere I5 is the 5\u00d7 5 identity matrix. In our simulations, we tested a range of values and picked (in hindsight) the well-performing values C\u03b4 = 0.025 and \u03bblogistic = 0.\nG.2 Strategies implemented in this numerical study\nWe run all these strategies not with the total-cost constraints\nB = (0.05, 0.2, \u03c4, \u03c4, \u03c4, \u03c4) , where \u03c4 \u2208 {10\u22127, 0.025} , but take a margin b = 0.005 and use B\u2032 = B \u2212 (b, b, 0, 0, 0, 0) instead. This is a slightly different way of taking some margin on the average cost constraint: we do so because we are not aiming for a strict respect of the fairness constraints but rather want to report the level of violation on it. Again, we tried a range of values for b (between 0.001 to 0.01) and this value of 0.005 led to a good balance between (lack of) total-cost constraint violations and rewards.\nPerformance of optimal static policies. We use OPT(r, c,B) as the benchmark in the definition of regret; our methodology also reveals that OPT(r, c,B\u2032) is another benchmark, see the discussion in Section 4. We report both values on our graphs. To compute them, we proceed as follows, e.g., for B. As computing directly the minimum stated in (4) is difficult, even when fully knowing the distribution \u03bd, we compute 100 estimates\nOPT(j)(r, c,B) , which we average out into O\u0302PT(r, c,B) = 1\n100 100\u2211 j=1 OPT(j)(r, c,B) .\nFor each j, we sample S = 10,000 contexts from the distribution \u03bd, and denote by \u03bd\u0302(j)S the associated empirical distribution; we then solve numerically the problem (4) with \u03bd replaced by this empirical distribution:\nOPT(j)(r, c,B) = min \u03bb\u2a7e0 E X\u223c\u03bd\u0302(j)S [ max a\u2208A { r(X, a)\u2212 \u2329 c(X, a)\u2212B, \u03bb \u232a}] .\nMixed policy knowing \u03bb\u22c6B\u2032 but estimating r. For the sake of illustration, we report the performance of a policy that would have oracle knowledge of \u03bb\u22c6B\u2032 , which is a finite-dimensional parameter that summarizes \u03bd, but would ignore r, i.e., the underlying logistic model. That is, this mixed policy would pick, at each round,\nmax a\u2208A\n{ r\u0302 ucb\u03b4,t\u22121(xt, a)\u2212 \u2329 c\u0302 lcb\u03b4,t\u22121(xt, a)\u2212B \u2032, \u03bb\u22c6B\u2032 \u232a} ,\n(and would omit the \u03bb update in Box B).\nTo compute (an approximation of) \u03bb\u22c6B\u2032 , we proceed 100 times as described below to compute estimates\n\u03bb \u22c6,(j) B\u2032 , which we average out into \u03bb\u0302\n\u22c6 B\u2032 = 1\n100 100\u2211 j=1 \u03bb \u22c6,(j) B\u2032 ,\nwhere we noted that the numerical values obtained for the \u03bb\u22c6,(j)B\u2032 are rather similar. With the notation above, for each j, we sample S = 10,000 contexts from the distribution \u03bd, and solve\n\u03bb \u22c6,(j) B\u2032 \u2208 argmin \u03bb\u2a7e0 E X\u223c\u03bd\u0302(j)S [ max a\u2208A { r(X, a)\u2212 \u2329 c(X, a)\u2212B, \u03bb \u232a}] .\n(These estimations are independent from the estimations used to compute the OPT values, i.e., we use different seeds.)\nPGD \u03b3. We refer to the Box B strategy as PGD \u03b3, for which we report the performance for values \u03b3 \u2208 {0.01, 0.02, 0.04, 0.05, 0.1}. We also implemented the Box C strategy, with an alternative value of MT,\u03b4,k, given that the one exhibited based on the theoretical analysis was too conservative, so that no regime break occurred. We resort to a value of the same order of magnitude:\nM \u2032T,\u03b4,k = c d \u221a T ln ( T (k + 2) ) ,\nfor a numerical constant c that we set to 0.01 in our simulations. We call this strategy PGD Adaptive in Figure 1 and Table 1.\nG.3 Outcomes of the simulations\nWe take T = 10,000 individuals (instead of T = 1,000 as in the code by Chohlas-Wood et al., 2021) and set initial 50 rounds as a warm start for strategies (mostly because of the logistic estimation). We were limited by the computational power (see Appendix G.4) and could only perform N = 100 simulations for each (instance of each) strategy. We report averages (strong lines in the graphs) as well as \u00b12 times standard errors (shaded areas in the graphs or values in parentheses in the table).\nGraphs. In the first line of graphs in Figure 1, we report the average rewards (over the N runs) of the strategy under scrutiny as a function of the sample size. More precisely, with obvious notation, we plot\nt 7\u2192 1 N N\u2211 n=1\n( 1\nt t\u2211 s=1 r(n)s\n) .\nWe report the values of OPT(r, c,B) and OPT(r, c,B\u2032) as dashed horizontal lines.\nIn the second and third lines of graphs in Figure 1, we report the average costs suffered; again, with obvious notation, we plot\nt 7\u2192 1 N N\u2211 n=1\n( 1\nt t\u2211 s=1 1{a(n)s =aride}\n) and t 7\u2192 1\nN N\u2211 n=1\n( 1\nt t\u2211 s=1 1{a(n)s =avoucher}\n) .\nWe include the average budget constraints Bride = 0.05 and Bvoucher = 0.20 as dashed horizontal lines.\nFinally, the fourth line of the graphs in Figure 1 reports the fairness costs; we average their absolute values and draw, again with obvious notation,\nt 7\u2192 1 N N\u2211 n=1 1 4 \u2211 a\u2208{aride,avoucher} \u2211 g\u2208{0,1} \u2223\u2223\u2223\u2223\u22231t t\u2211 s=1 ( 21{a(n)s =a} 1{gr(x(n)s )=g} \u2212 1{a(n)s =a} )\u2223\u2223\u2223\u2223\u2223  .\nWe include the fairness tolerance \u03c4 as a dashed horizontal line.\nTable. We also report the performance of the strategies at the final round T in following table, with \u00b12 times standard errors in parentheses. We note that the performance of the mixed policy is poor, in particular in terms of fairness costs, which is why we omitted it on the graphs, to keep them readable.\nComments. When \u03b3 is well set, i.e., large enough (see the bound of Lemma 1), the PGD strategies control spending costs thanks to targeting B\u2032 instead of B. We observe that for \u03b3 = 0.01, the PGD strategy does not control the rideshare costs, but for all larger values of \u03b3, the associated strategies control the three costs considered. The average rewards achieved are coherent with the average spendings: the smaller the average spendings, the smaller the average rewards. There is some lag: the strategies tuned with \u03b3 parameters in {0.04, 0.05, 0.1} could use costly actions more. We also observe that fairness costs remain under the target limits.\nThe mixed policy does not success in controlling the costs, in particular, the fairness costs. While the optimal dual variables \u03bb\u22c6B\u2032 summarize the distribution \u03bd, it seems that \u03bb \u22c6 B\u2032 has to be used with care: only with the exact values r and c, and not with estimates. On the contrary, the PGD strategies of Box B are more stable, as the dual variables are learned based also on the estimated reward and cost functions.\nFairness Tolerance = 1e-7\nFairness Tolerance = 0.025\nPGD \u03b3 = 0.01 0.4651 (0.0002) 0.0519 (<0.0001) 0.1984 (<0.0001) 0.0006 (<0.0001) PGD \u03b3 = 0.02 0.4613 (0.0002) 0.0492 (<0.0001) 0.1967 (0.0004) 0.0004 (<0.0001) PGD \u03b3 = 0.04 0.4571 (0.0002) 0.0479 (<0.0001) 0.1962 (0.0002) 0.0004 (<0.0001) PGD \u03b3 = 0.05 0.4554 (0.0002) 0.0476 (<0.0001) 0.1961 (0.0002) 0.0003 (<0.0001) PGD \u03b3 = 0.1 0.4502 (0.0002) 0.0471 (<0.0001) 0.196 (0.0002) 0.0003 (<0.0001) PGD Adaptive 0.4581 (0.0002) 0.0498 (0.0002) 0.1971 (0.0002) 0.0005 (<0.0001) Mixed Policy 0.4402 (0.0056) 0.0499 (0.0058) 0.1056 (0.017) 0.0411 (0.0052)\nFairness tolerance \u03c4 = 0.025\nOPT(r, c,B) 0.4731 (0.0002) OPT(r, c,B\u2032) 0.4691 (0.0002)\nPGD \u03b3 = 0.01 0.4698 (0.0002) 0.0518 (0.0002) 0.1983 (<0.0001) 0.0246 (0.0002) PGD \u03b3 = 0.02 0.4663 (0.0002) 0.0492 (<0.0001) 0.1966 (0.0006) 0.0242 (0.0002) PGD \u03b3 = 0.04 0.4621 (0.0004) 0.0478 (<0.0001) 0.1958 (0.001) 0.0223 (0.0004) PGD \u03b3 = 0.05 0.4604 (0.0004) 0.0476 (<0.0001) 0.1955 (0.0014) 0.0208 (0.0004) PGD \u03b3 = 0.1 0.4538 (0.0002) 0.0471 (<0.0001) 0.1958 (0.0004) 0.0128 (0.0004) PGD Adaptive 0.4634 (0.0002) 0.0499 (0.0002) 0.1972 (0.0002) 0.0228 (0.0002) Mixed Policy 0.4466 (0.0054) 0.0566 (0.0052) 0.1053 (0.0164) 0.0473 (0.0054)\nFinally, we note that in our experiments, the regimes in PGD Adaptive strategy typically covered range from k = 0 (corresponding to \u03b30 = 1/ \u221a T = 0.01) to k = 2 (corresponding to \u03b32 = 22/ \u221a T = 0.04). The PGD Adaptive strategy performs well and is (only) outperformed by the PDG strategy with a fixed \u03b3 = 0.02 (of course difficult to pick in advance). In particular, the PGD Adaptive strategy controls costs, and does so by switching to larger step sizes when needed.\nG.4 Computation time and environment\nAs requested by the NeurIPS checklist, we provide details on the computation time and environment. Our experiments were ran on the following hardware environment: no GPU was required, CPU is 3.3 GHz 8 Cores with total of 16 threads, and RAM is 32 GB 4800 MHz DDR5. We ran 100 simulations with 10 different seeds on parallel each time. In the setting and for the data described above, the average time spend on each algorithm for a single run was inferior to 10 minutes."
        }
    ],
    "title": "Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness",
    "year": 2023
}