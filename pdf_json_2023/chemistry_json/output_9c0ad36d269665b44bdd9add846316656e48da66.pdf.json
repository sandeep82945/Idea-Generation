{
    "abstractText": "NOTICE OF COPYRIGHT: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).",
    "authors": [
        {
            "affiliations": [],
            "name": "Van-Quan Vuong"
        },
        {
            "affiliations": [],
            "name": "Caterina Cevallos"
        },
        {
            "affiliations": [],
            "name": "Ben Hourahine"
        },
        {
            "affiliations": [],
            "name": "B\u00e1lint Aradi"
        },
        {
            "affiliations": [],
            "name": "Jacek Jakowski"
        },
        {
            "affiliations": [],
            "name": "Stephan Irle"
        },
        {
            "affiliations": [],
            "name": "Cristopher Camacho"
        }
    ],
    "id": "SP:5cd7cc4e6c86b648d6bb3865efd82171ff4aff22",
    "references": [],
    "sections": [
        {
            "text": "NOTICE OF COPYRIGHT: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan)."
        },
        {
            "heading": "Accelerating the Density-Functional Tight-Binding Method Using Graphical Processing Units",
            "text": "Van-Quan Vuong \u22a5,1 Caterina Cevallos \u22a5,2 Ben Hourahine,3 B\u00e1lint Aradi,4 Jacek Jakowski,5, a) Stephan Irle,5, b) and Cristopher Camacho2, c) 1)Bredesen Center for Interdisciplinary Research and Graduate Education, University of Tennessee, Knoxville, TN, United States 2)School of Chemistry, University of Costa Rica, San Jos\u00e9, 11501-2060, Costa Rica 3)SUPA, Department of Physics, The John Anderson building, 107 Rottenrow East, Glasgow G4 0NG, United Kingdom 4)Bremen Center for Computational Materials Science, Universit\u00e4t Bremen, Bremen, Germany 5)Computational Sciences & Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN, United States\n(Dated: 30 January 2023)\nAcceleration of the density-functional tight-binding (DFTB) method on single and multiple graphical processing units (GPUs) was accomplished using the MAGMA linear algebra library. Two major computational bottlenecks of DFTB ground-state calculations were addressed in our implementation: the Hamiltonian matrix diagonalization and the density matrix construction. The code was implemented and benchmarked on two different computer systems: (1) the SUMMIT IBM Power9 supercomputer at the Oak Ridge National Laboratory Leadership Computing Facility (OLCF) with 1 to 6 NVIDIA Volta V100 GPUs per computer node, and (2) an in-house Intel Xeon computer with 1 to 2 NVIDIA Tesla P100 GPUs. The performance and parallel scalability were measured for three molecular models of 1-, 2- and 3-dimensional chemical systems, represented by carbon nanotubes, covalent organic frameworks, and water clusters.\n\u22a5 These authors contributed equally to this work a)corresponding author, e-mail: jjakowski@ornl.gov b)corresponding author, e-mail: irles@ornl.gov c)corresponding author, e-mail: cristopher.camacho@ucr.ac.cr\n1\nThis is a peer-reviewed, accepted author manuscript of the following article: Vuong, V-Q., Cevallos, C., Hourahine, B., Aradi, B., Jakowski, J., Irle, S., & Camacho, C. (2023). Accelerating the density-functional tight-binding method using graphical processing units. Journal of Chemical Physics. https://doi.org/10.1063/5.0130797\nI. INTRODUCTION\nComputational simulation of chemical systems and materials has enriched our understanding of matter from the atomic to the microscopic scales1\u20133. For the simulation of complex chemical processes, quantum mechanics (QM)-based methods like ab initio wave function theory (WFT) or Density Functional Theory (DFT) are currently the most commonly used methods because of their capability to explicitly describe electronic structures, chemical bond breaking and making, including electron transfer occurring on electrodes4,5 among other chemical reactions on the surface of heterogeneous catalysts6,7 or in biosystems8\u201310. However, the applications of these methods are typically limited to systems containing a few hundred atoms focusing on static minimum energy reaction mechanism pathways or only short-time molecular dynamics (MD) simulations, due to the tremendous computational cost required to compute large numbers of integrals over electronic degrees of freedom11,12. Even though DFT calculations comprising of more than 106 atoms are possible, these must be considered heroic calculations and are not feasible for their routinely use in science13,14.\nTo overcome the challenge in computational cost, semi-empirical WFT-based (MNDO, AM1, PMx, OMx, etc.) and approximate DFT-based methods (DFTB, xTB) have long been developed1,15\u201318 and have not lost their relevance3,19. Among them, the density-functional tight-binding (DFTB) method is particularly attractive due to its hierarchical way of approaching the DFT level of theory at a considerably lower computational cost, greatly reducing the time-to-solution ratio by several orders of magnitude16,20\u201323. A key feature of DFTB originates from is the construction of the Slater-Koster tight-binding approximation24 to the Kohn-Sham Hamiltonian25,26 based on a two-center approximation which enables the use of tabulated, distance-dependent electronic integrals and repulsive potentials. Explicit computation of the highly resource-demanding integrals27,28 can thereby be avoided and the construction of the corresponding Hamiltonian matrices is very efficient and scales quadratically with the number of atoms29,30. For example, on a single central processing unit (CPU) of an ordinary laptop computer, the DFTB+ code allows for geometry optimizations of molecular and bulk solid systems containing thousands of atoms within a few hours, and can perform MD simulations for systems containing hundreds of atoms on the nanosecond time-scale with a similar time-to-solution20,31.\nUnfortunately, the wall-clock time of DFTB calculations increases cubically with the size of the simulated systems, which imposes a heavy toll on large-scale applications (systems containing more than several thousand atoms). The cubic scaling of DFTB is due to BLAS3 type dense matrix-matrix operations and is primarily driven by its most time-consuming step, namely a generalized eigenvalue problem used for solving the time-independent Schrodinger equation32\u201335. Several reduced-scaling DFTB approaches have been suggested to address this problem, such as the divide-and-conquer (DC)-DFTB method33,36,37, modified DC (mDC)-DFTB38, the fragment molecular orbital (FMO)-DFTB method32,39\u201342, a graph-based, sparse linear algebra approach to formulating the eigenvalue equation34, and various methods for Fermi-function expansion via matrix polynomials or expansion of it\u2019s poles3,43. In addition, for quasi-2D and 3D bulk systems, O(N3/2) and O(N2) scaling can be achieved, respectively, for large systems by means of the PEXSI method44,45. While these approaches allow for DFTB-based simulations of systems containing up to 100 million atoms by taking advantage of conventional homogeneous parallel computational architectures46, the procedures cannot be easily applied to chemically reactive systems or metallic systems32. However, general purpose methods with no worse than quadratic scaling can be used in these cases43. Moreover, several of the techniques mentioned above sacrifice the accuracy of the energy and the gradient in an uncontrollable way33. The accuracy of the method can be particularly affected in MD simulations due to error accumulation, where a small error would lead to qualitatively different MD trajectories.\nWhile DFTB calculations can in principle be accelerated in homogeneous parallel computer architec-\n2\ntures using open multi-processing (OpenMP) or message passing interface (MPI) techniques, the resulting speedup factor is usually low for multiple-node calculations due to the limited parallel scalability of matrix diagonalization and slow speed of inter-node communication. Alternatively, the calculations can be accelerated by taking advantage of modern computer graphics processing units (GPUs) in a similar fashion as other QM-based methods47\u201355. In addition, modern supercomputer architectures are increasingly employing heterogeneous CPU architectures56. In the past, several attempts to accelerate the DFTB calculations on heterogeneous CPU+GPU computational architectures by porting only the Hamiltonian matrix diagonalization onto the GPUs and leaving the remainder of the DFTB calculations on the CPU have been reported57\u201359. The latest implementation by Allec et al. showed promising results, for instance, a speedup factor of \u223c6x (when 4 NVIDIA P100 GPUs and 24 threads of an Intel Xeon E5-2680v3 CPU was compared to 24 threads of the same CPU) on the diagonalization of the Hamiltonian matrix for a water cluster containing 16,000 atoms was reported59. However, only a modest \u223c1.5x speedup factor was achieved for a single-point energy calculation of the same water cluster under the same conditions. Furthermore, no parallel scalability with the number of GPUs has been reported. In this work, we report a new implementation in which the two most time-consuming steps for ground state calculations, (1) diagonalization of the Hamiltonian matrix and (2) the construction of density matrix, are ported to the GPU (being the only cubic scaling parts of the calculation). As in the ELPA2 library55, the MAGMA GPU-accelerated library60 is employed for the linear algebra manipulations on GPUs. The general performance and parallel scalability of the new implementation was benchmarked with various numbers of GPUs employed in parallel for a wide range of system dimensionalities (linear 1D, planar 2D, and cluster-type 3D) and system sizes. The benchmark was performed on two different CPU+GPU architectures, namely one on the Summit supercomputer at Oak Ridge National Laboratory featuring IBM Power9 CPU architectures and up to six NVIDIA Volta GPUs per node, and another, workstation-type computer using ordinary Intel Xeon CPUs with up to 2 NVIDIA Tesla GPUs. Finally, we analyzed and report in detail the benefits of porting the construction of the density matrix onto the GPU."
        },
        {
            "heading": "II. METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A. Density-Functional Tight-Binding (DFTB)",
            "text": "The electron density of a system can be approximated using a Taylor series around a reference density61. Truncating the expansion at the first, second, or third orders gives rise to a hierarchical family of DFTB \u201cflavors\u201d: beginning from the simplest formulation, the non-self-consistent charge (non-SCC)-DFTB (or DFTB1)29,30; secondly, the self-consistent charge (SCC)-DFTB (or DFTB2)62,63, and finishing with the most involved third-order DFTB3 flavor16,64,65. The DFTB methods have already been the subjects of many comprehensive reviews16,22,63,66\u201369; hence, they will not be discussed in detail here. In this section, as a reminder, a brief review of the SCC-DFTB (DFTB2) method62 is given. In this method, the total energy is defined as\nE = occ.\u2211 i ni \u3008\u03a8i| H\u03020 |\u03a8i\u3009+ 1 2 atoms\u2211 AB \u03b3AB\u2206qA\u2206qB + atoms\u2211 A>B ErepAB , (1)\nwhere H\u03020 is the initial Hamiltonian constructed from the superposition of neutral atomic densities in a two-center approximation29,30; |\u03a8i\u3009 are molecular orbitals (MOs); \u2206qA are the Mulliken charges70, and \u03b3AB\u2206qA\u2206qB represents the electrostatic interaction energy between the two Mulliken charges on atom A\n3\nand atom B62. The molecular orbitals |\u03a8i\u3009 are expanded as a linear combination of atomic orbitals |\u03c6\u00b5\u3009,\n|\u03a8i\u3009 = \u2211 \u00b5 ci\u00b5 |\u03c6\u00b5\u3009 . (2)\nAs with other QM-based methods, the molecular orbitals |\u03a8i\u3009 as well as the total energy are determined by applying the variational principle. By recasting the problem of solving the coefficient ci\u00b5 into a secular matrix equation, the task becomes solving the generalized eigenvalue problem\u2211\n\u03bd H\u00b5\u03bdci\u03bd = i \u2211 \u03bd S\u00b5\u03bdci\u03bd , (3)\nwhere H and S are the Hamiltonian and overlap matrices, respectively. The SCC-DFTB Hamiltonian is then given by\nH\u00b5\u03bd = \u3008\u03c6\u00b5| H\u03020 |\u03c6\u03bd\u3009+ 1\n2 S\u00b5\u03bd \u2211 C (\u03b3AC + \u03b3BC)\u2206qC , with \u00b5 \u2208 A, \u03bd \u2208 B. (4)\nIn the framework of the two-center approximation, the initial Hamiltonian integrals \u3008\u03c6\u00b5| H\u03020 |\u03c6\u03bd\u3009 and the overlap integrals \u3008\u03c6\u00b5|\u03c6\u03bd\u3009 are pre-tabulated for each atomic pair. The Mulliken charge on atom A is defined as\n\u2206qA = occ\u2211 i ni \u2211 \u00b5\u2208A \u2211 \u03bd ci\u00b5ci\u03bdS\u00b5\u03bd \u2212 q0A = \u2211 \u00b5\u2208A \u2211 \u03bd P\u00b5\u03bdS\u00b5\u03bd \u2212 q0A, (5)\nwhere P\u00b5\u03bd = \u2211occ i nici\u00b5ci\u03bd are the single particle density matrix elements. Since the gross atomic charges, \u2206qA, depend on the molecular orbitals |\u03a8i\u3009, the eigenvalue problem must be solved self-consistently. This entails solving equation 3 iteratively until convergence of the eigenvalues is reached, as depicted in Figure 1. Due to the pre-tabulated integrals in the formalism of DFTB, the most time-consuming step in traditional DFT is avoided. In DFTB calculations for systems containing more than tens of atoms, the performancecritical routines correspond to the diagonalization of the Hamiltonian matrix, taking on the order of 90\u221295% of the total running time, and the construction of the density matrix accounts approximately for 5\u2212 10%.\n4\nB. Implementation\nIn this work, we utilize the MAGMA GPU-based eigensolver to diagonalize the Hamiltonian matrix and replaced the BLAS routines required to construct the density matrix with the corresponding GPU-based routines. In our implementation, the number of GPUs of the \u201cmagma\u201d solver can be specified by the user via the environment variable \u201cMAGMA_NUM_GPUS\u201d. The code detects all available GPUs and ensure that the requested number does not exceed the number of physically available units. Otherwise, only one GPU is used by default. Furthermore, a Fortran wrapper, namely device_info, based exclusively on MAGMA Device Management routines, was created to guarantee future portability with GPUs manufactured by companies other than NVIDIA. Such an interface facilitates the acquisition of information regarding the available devices by enabling seamless communication between the GPUs and the DFTB+ program. Subsequently, the MAGMA routines are called from the eigensolver module; an appropriate subroutine is selected from the gpu_gvd interface, depending on the type of chemical system, periodic or molecular.\nThe second most time-consuming routine in DFTB calculations is the construction of the density matrix, P , from the matrix of eigenvectors, C. The matrix construction is carried out after obtaining the eigenvectors and the occupation numbers ni, as explained in Figure 1. For dense matrices, the computational cost typically scales as O(N3), but it can be reduced to O(N2) by exploiting the sparsity of P 20. The building of this matrix is done by calling the BLAS level-3 routines performing the Hermitian rank-k (HERK ) operations, P \u2190 \u03b1CCH + \u03b2P . We replaced these CPU-based HERK routines with their single-GPU-based equivalents, as provided by the MAGMA library. A patch for the 19.1 DFTB+ release71 can be found at https://doi.org/10.5281/zenodo.7566762. The transfer of the eigenvectors, density matrix between the host and devices, and the memory allocation are carefully managed by handling the GPU pointers directly."
        },
        {
            "heading": "C. Computational Details",
            "text": "As mentioned above, the performance of our implementations was examined on two different computer architectures: (1) the SUMMIT supercomputer and (2) an in-house workstation-type Intel-based Linux computer which we call KOFUN. On the SUMMIT computer, two \u201cIBM Power9 CPUs\u201d at 4.00GHz in conjunction with 6 \u201cNVIDIA Volta V100\u201d GPUs were used. Each GPU is paired with 16 GB of High Bandwidth Memory (HBM) with a bandwidth of 900 GB/s. Bidirectional GPUs-HBMs interconnect at 50 GB/s. Communication between CPUs and GPUs-HBMs units rates at 50 GB/s. CPU to RAM possesses a bandwidth of 170 GB/s. Each \u201cNVIDIA Volta V100\u201d GPU has a peak performance of 7.8 teraFLOPS. The two CPUs in each node interconnect at a bandwidth of 64 GB/s. On the KOFUN computer, two \u201cIntel Xeon CPU E5-2630 v4\u201d at 2.20GHz paired with 2 \u201cNVIDIA Tesla P100\u201d GPUs were used. Each Xeon CPU is capable of 704 GFLOPS. The system possesses a max memory bandwidth of 25.6 GB/s. GPUs interconnect with a bandwidth of 16 GB/s. Each \u201cNVIDIA Tesla P100\u201d GPU has a peak performance of 4.7 teraFLOPS. Firstly, computational performance and parallel scalability of only the Hamiltonian matrix diagonalization ported onto the GPUs were benchmarked on SUMMIT for various carbon nanotubes (CNTs, 1D), covalent organic frameworks (COFs, 2D), and water clusters (3D). For the test of parallel scalability, the number of GPUs varied from 1 to 6. Secondly, an equivalent test was carried out on KOFUN with the constraint to using only up to 2 GPUs. Finally, the time savings of building the density matrix on the GPU was investigated on both SUMMIT and KOFUN computers. For all tests, the wall-clock time of SCC-DFTB single-point energy calculations was used to evaluate the computational cost. The wall-clock time was measured using the Unix \u201ctime\u201d command. In these benchmarks, we only discuss the single-point energy calculation because the force calculation often takes on the order of only 1-3% of the total running time, even when the Hamiltonian matrix diagonalization is carried out on the GPUs, as shown in Table\n5\nS1 in the Supplementary Material. All DFTB single-point energy calculations were performed with the number of SCC iteration cycles set equal to 10 to ensure a consistent comparison, except in comparison with energy+gradient calculations where the default options were used. The constant number of SCC iteration cycles was achieved with the following options: \u201cSCCTolerance = 1e-12\u201d and \u201cMaxSCCIterations = 10\u201d. DFTB calculations were performed using a development version of the DFTB+ program20,72 in combination with the MAGMA (2.5.3) library60. The math kernel library (MKL) was used to maximize the performance of its \u201cIntel CPU,\u201d similarly to the engineering scientific subroutine library (ESSL) used in the SUMMIT computer."
        },
        {
            "heading": "III. RESULTS AND DISCUSSION",
            "text": ""
        },
        {
            "heading": "A. Effect of the Hamiltonian matrix diagonalization on the GPUs: performance and parallel scalability",
            "text": "To assess the effects, we measured the wall-clock time of DFTB single-point energy calculations using various computer configurations (with and without GPUs) for a set of water clusters, varying from 3 to 11,944 water molecules. The calculations with CPU+GPUs were carried out using 21 or 42 Power9 CPU threads combined with 1, 2, 3, or 6 V100 GPUs; the calculations with CPU-only were carried out using the corresponding 21 or 42 CPU threads. We emphasize that, in this test, only the Hamiltonian matrix diagonalization utilized the GPUs in the calculations with CPU+GPUs.\nFigure 2 shows how the wall-clock time increases with the system size, and Table S2 shows actual values of the wall-clock time. In the case of the CPU-only calculations, the time increases drastically for systems having more than 10,000 basis functions. It is important to note that the wall-clock time increases with the system size cubically in all cases, with and without GPUs. However, the rate of change (slope) is much lower in the cases of GPU-accelerated calculations compared to CPU-only calculations.\nWhile doubling the number of CPU threads from 21 to 42 only sped up the calculation in a minimal way,\n6\nusing GPUs to diagonalize the Hamiltonian matrix significantly sped up the calculations in general. When the GPUs were employed, the wall-clock time was greatly reduced for moderate- and large-size systems with more than 10,000 basis functions. For instance, while the CPU-only calculation took 10,403 seconds for the (H2O)5280 cluster, the calculation took only 1,285 seconds with the aid of 3 GPUs. Nevertheless, the acceleration is less substantial for smaller systems: The wall-clock time only reduces from 97 seconds to 43 seconds when the 3 GPUs were used for the (H2O)1046 cluster. The calculation can be even slower with GPUs for the smallest system, the (H2O)3 cluster. The optimization of the algorithms and kernels implemented in the MAGMA library for different matrix sizes has been the subject of several studies, and we refer the reader to Refs.73,74\nFor a more comprehensive benchmark, we extended the test to include three representative types of systems: (1) carbon nanotubes (CNTs) for 1D materials, (2) covalent organic frameworks (COFs) for 2D materials, and (3) water clusters for 3D materials. The wall-clock time is listed in Table S2 for water clusters, Table S4 and Table S3 in the Supplementary Material for CNTs and COFs, respectively. Figure 3 compares the speedup factor as a function of the number of GPUs and the number of basis functions for these systems.\nGenerally, the advantages of using GPUs, with a speedup factor larger than one, were observed for\n7\nsystems having more than 4,000 basis functions. The parallel scalability increases with the system size and only shows a good scaling factor with the number of GPUs for large systems. For systems with less than 36,000 basis functions, using 6 GPUs slows down the calculation, as data must be transferred through two different sockets in the computer. We note that in the SUMMIT supercomputer all three GPUs in a single socket are interconnected via NVLink with a bi-directional bandwidth of 50 GB/s, while data transferred between sockets and thus between GPUs in different sockets share a single bandwidth of 64 GB/s. Although using 42 threads is marginally faster than 21 threads, the combination of 42 threads + 6 GPUs is significantly faster than 21 threads + 6 GPUs. Interestingly, for a similar size, the larger speedup factor was observed with 2D systems (COFs) compared to the 3D materials (water clusters), and the largest speedup factor was observed with 1D materials (CNTs). The trend might be attributed to the sparsity of the Hamiltonian matrix of these systems.\nB. Xeon CPU + P100 GPU versus Power CPU + V100 GPU\nTo further evaluate the effects of different hardware configurations on the performance of our implementation, we carried out the same test for CNTs, COFs, and water clusters on our in-house KOFUN computer. The wall-clock time of DFTB single-point energy calculations for CNTs, COFs, and water clusters is listed in Table I.\nFigure 4 compares the wall-clock time and the speedup factor of DFTB single-point energy calculations for water clusters on SUMMIT and KOFUN computers. For water clusters, the calculation is faster on KOFUN than on SUMMIT when they are carried out using CPU-only, suggesting that the Intel Xeon CPU/MKL combination is faster than the IBM Power CPU/ESSL for this specific case of study. Nevertheless, the V100\n8\nGPU is faster than the P100 GPU, making the GPU-accelerated DFTB calculations faster on SUMMIT than on KOFUN. As a result, remarkably more significant speedup factors were observed on SUMMIT. For example, for water clusters of (H2O)5280 computed using 2 GPUs, the speedup factor of 7.6x and 4.6x was observed on SUMMIT and KOFUN, respectively.\nWe noticed that in the case of CNTs, the homogeneous computation is drastically slower on KOFUN than on SUMMIT, as shown in Table I. As a result, an unreasonable high speedup factor of 32x was observed. The issue can most likely be attributed to the implementation of the MKL library. Similar behavior was observed for COFs but in a less severe manner."
        },
        {
            "heading": "C. Effect of building the density matrix on GPU",
            "text": "For DFTB calculations using CPU-only, the Hamiltonian matrix diagonalization is the bottleneck, taking 90 \u2212 95% of the total wall-clock time, and the density matrix construction generally takes in the order of 5\u2212 10% of the total wall-clock time. When the GPUs are used to accelerate the diagonalization by a factor of 10 \u2212 15x, the density matrix construction becomes a relatively more time-consuming step. Therefore, it is natural to question whether porting the density matrix construction to the GPU is worthwhile. This section assesses the effects of constructing the density matrix on the GPU. The differences in wall-clock time of DFTB single-point energy calculations with and without the density matrix constructed on the GPU are compared for water clusters; the wall-clock time values are listed in Table II. The corresponding wall-clock time values for COFs and CNTs are listed in Tables S5-S6 and Tables S7-S8 in the Supplementary Material, respectively.\nFigure 5 shows the comparison of running times and speedup factors obtained on the SUMMIT computer. Similar to the case of porting only the Hamiltonian matrix diagonalization to the GPUs, the effects of porting the density matrix construction to the GPU increase with the system size: The larger the system, the higher the speedup factor is observed. While the trend is similar, having both the Hamiltonian matrix diagonalization and the density matrix construction on the GPUs speeds up the calculation more significantly. For instance, in the case of (H2O)5280 with 3 GPUs, the speedup factor increases from 8x to 13x. Figure 5 shows that including the density matrix construction on a single GPU can make the calculations as fast as having only the Hamiltonian matrix diagonalization on 2 or 3 GPUs.\nAnalogous to the results observed on the SUMMIT supercomputer, a similar effect was measured on KOFUN by performing the density matrix construction on a GPU, shown in Table II. Nonetheless, the overall speedup is less pronounced as the P100 GPU on KOFUN is less powerful than the V100 GPU on SUMMIT. For example, compared to the GPU-accelerated only-diagonalization implementation in the case of (H2O)5280 with 2 GPUs, having the density matrix construction on GPU speeds up the calculations by 1.51x on SUMMIT, and the speedup factor is only 1.26x on KOFUN.\n10\nTo further analyze the effects of the Hamiltonian matrix H diagonalization and the density matrix P construction on the GPUs, we decomposed the total running time of DFTB single-point energy calculation for (H2O)5280 into three components: (1) time for the matrix H diagonalization, (2) time for the matrix P construction, and (3) the rest. The percentage of these components with respect to the total wall-clock time was examined. Figure 6 shows how the percentage of these components varies with the number of GPUs on SUMMIT. When only the Hamiltonian matrix diagonalization is ported to the GPUs, the matrix H diagonalization percentage reduces drastically from 95.5% with CPU-only to 56.3% with CPU+3GPUs. On the other hand, the matrix P construction percentage increases from 5.4% with CPU-only to 43.1% with CPU+3GPUs. However, having the Hamiltonian matrix diagonalization as well as the density matrix construction on the GPUs shows a modest change in these percentages, varying from 94/5% to 91.1% in the case of the matrix H diagonalization and from 5.4% to 7.9% in the case of the matrix P construction, with CPU-only and CPU+3GPUs respectively. Thus, besides the Hamiltonian matrix diagonalization, porting the density matrix construction to the GPU is worth doing and necessary.\n11"
        },
        {
            "heading": "IV. SUMMARY AND OUTLOOK",
            "text": "We presented the acceleration of the density-functional tight-binding (DFTB) method on single or multiple graphical processing units (GPUs) using the MAGMA linear algebra library. In the new implementation, both the diagonalization of the Hamiltonian matrix and the construction of the density matrix were ported to the GPUs. The implementation was carefully benchmarked on two different computer systems with various numbers of GPUs for three molecular models of 1-, 2- and 3-dimensional chemical systems.\nThe GPUs can speed up DFTB calculations remarkably up to 15x for large-size systems having more than 70,000 basis functions (when 6 NVIDIA V100 GPUs and 42 threads of IBM Power9 CPUs was compared to 42 threads of the same CPUs on the SUMMIT supercomputer) but are less notable for the small ones. The difference in speedup for systems with different dimensionalities is rather small, as it varies only between 14x and 15x.\nLikewise, the parallel scalability with number of GPUs increases with system size as well. Our benchmark results suggest that employing only 1 to 2 GPUs can be an efficient option for routine applications, as a tradeoff between available computational resources and largest-possible acceleration for small- and moderate-size systems. On the other hand, using multiple GPUs on a supercomputer like SUMMIT can be worthwhile for simulations of large-size systems when the Hamiltonian matrix exceeds the size of the available memory.\nBesides the Hamiltonian matrix diagonalization, constructing the density matrix on a GPU makes utilizing GPUs more effective, especially when the GPU speedup for diagonalization is significant. It can make DFTB calculations using one GPU as fast as when using two GPUs where only matrix diagonalization is performed on the GPUs.\nThe current implementation offers the possibility of efficiently accelerating the time\u2013to\u2013solution for systems in which the calculations can be parallelized as individually independent calculations, such as MD ensembles and anharmonic vibrational frequency calculations along normal modes. These type of calculations can be spanned over several nodes in a cluster, as most commonly available in computational chemistry groups.\nA limit of our current implementation is the lack of control in memory allocation provided by MAGMA\u2018s\n12\ndiagonalization routines. If eigenvectors obtained from the Hamiltonian matrix diagonalization procedure could be left on the GPUs to construct the density matrix, we could further improve the performance and endow the software with superior speedup.\n13"
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We appreciate valuable comments and suggestions from Jack Wells. We gratefully acknowledge support for this work on OLCF Summit through the Directors Discretionary Allocation CHM152 (2018-2020). Cristopher Camacho and Caterina Cevallos acknowledge financial support from the Rectory and the Vice-rectory for research of the University of Costa Rica through grant 115-B9-461. Van-Quan Vuong acknowledges support by an Energy Science and Engineering Fellowship of the Bredesen Center for Interdisciplinary Research and Graduate Education at the University of Tennessee, Knoxville. S.I. acknowledges support from the Fluid Interface Reactions, Structures, and Transport (FIRST) Center, an Energy Frontier Research Center funded by the U.S. Department of Energy (DOE), Office of Science, Office of Basic Energy Sciences. This research used resources of the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725."
        },
        {
            "heading": "SUPPLEMENTARY MATERIAL",
            "text": "See the Supplementary Material for the wall-clock time of DFTB calculations for carbon nanotubes (CNTs), covalent organic frameworks (COFs), and water clusters."
        },
        {
            "heading": "AUTHOR DECLARATIONS",
            "text": ""
        },
        {
            "heading": "Conflict of Interest",
            "text": "The authors have no conflicts to disclose."
        },
        {
            "heading": "Author Contributions",
            "text": "Van-Quan Vuong: Conceptualization (equal); Data curation (equal); Formal analysis (equal); Investigation (equal); Methodology (equal); Resources (equal); Software (equal); Supervision (equal); Validation (equal); Visualization (equal); Writing - original draft (equal); Writing - review & editing (equal). Caterina Cevallos: Data curation (equal); Methodology (equal); Resources (equal); Software (equal); Validation (equal); Writing - original draft (equal). Ben Hourahine: Methodology (equal); Software (equal); Writing - review & editing (equal). B\u00e1lint Aradi: Methodology (equal); Software (equal); Writing - review & editing (equal). Jacek Jakowski: Conceptualization (lead); Formal analysis (equal); Investigation (equal); Methodology (equal); Resources (equal); Software (equal); Writing - review & editing (supporting). Stephan Irle: Funding acquisition (equal); Project administration (equal); Supervision (equal); Writing - review & editing (equal). Cristopher Camacho: Funding acquisition (equal); Conceptualization (lead); Investigation (equal); Methodology (equal); Project administration (equal); Software (equal); Supervision (equal); Validation (equal); Writing - review & editing (equal)."
        },
        {
            "heading": "DATA AVAILABILITY",
            "text": "The data that supports the findings of this study are available within the article and its supplementary material.\n14"
        }
    ],
    "title": "Accelerating the Density-Functional Tight-Binding Method Using Graphical Processing Units",
    "year": 2023
}