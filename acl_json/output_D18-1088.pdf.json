{
    "abstractText": "Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes directly from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingxing Zhang"
        },
        {
            "affiliations": [],
            "name": "Mirella Lapata"
        },
        {
            "affiliations": [],
            "name": "Furu Wei"
        },
        {
            "affiliations": [],
            "name": "Ming Zhou"
        }
    ],
    "id": "SP:4081b2e04794a38ed2b33b697900755ea9ecfbe8",
    "references": [
        {
            "authors": [
                "Yejin Choi"
            ],
            "title": "Deep communicating agents",
            "year": 2018
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Tomas Mikolov."
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa-",
            "year": 2017
        },
        {
            "authors": [
                "Yoon Kim",
                "Yacine Jernite",
                "David Sontag",
                "Alexander M Rush."
            ],
            "title": "Character-aware neural language models",
            "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence, pages 2741\u20132749, Phoenix, Arizona.",
            "year": 2016
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Julian Kupiec",
                "Jan Pedersen",
                "Francine Chen."
            ],
            "title": "A trainable document summarizer",
            "venue": "Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 68\u201373. ACM.",
            "year": 1995
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\u201381, Barcelona, Spain.",
            "year": 2004
        },
        {
            "authors": [
                "Hui Lin",
                "Jeff Bilmes."
            ],
            "title": "Multi-document summarization via budgeted maximization of submodular functions",
            "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2010
        },
        {
            "authors": [
                "Inderjeet Mani."
            ],
            "title": "Automatic Summarization",
            "venue": "John Benjamins Pub Co.",
            "year": 2001
        },
        {
            "authors": [
                "Rada Mihalcea."
            ],
            "title": "Language independent extractive summarization",
            "venue": "Proceedings of the ACL Interactive poster and demonstration sessions, pages 49\u201352, Ann Arbor, Michigan.",
            "year": 2005
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Feifei Zhai",
                "Bowen Zhou."
            ],
            "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "In Proceedings of the 31st AAAI Conference on Artificial Intelligence, pages 3075\u20133091,",
            "year": 2017
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Caglar Gulcehre",
                "Bing Xiang"
            ],
            "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
            "venue": "arXiv preprint arXiv:1602.06023",
            "year": 2016
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Ranking sentences for extractive summarization with reinforcement learning",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Ani Nenkova",
                "Kathleen McKeown."
            ],
            "title": "Automatic summarization",
            "venue": "Foundations and Trends in Information Retrieval, 5(2\u20133):103\u2013233.",
            "year": 2011
        },
        {
            "authors": [
                "Razvan Pascanu",
                "Tomas Mikolov",
                "Yoshua Bengio."
            ],
            "title": "On the difficulty of training recurrent neural networks",
            "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 1310\u20131318, Atlanta, Georgia.",
            "year": 2013
        },
        {
            "authors": [
                "Romain Paulus",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "A deep reinforced model for abstractive summarization",
            "venue": "arXiv preprint arXiv:1705.04304.",
            "year": 2017
        },
        {
            "authors": [
                "MarcAurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba."
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "Proceedings of the 4th International Conference on Learning Representations, San Juan, Puerto Rico.",
            "year": 2016
        },
        {
            "authors": [
                "Alexander M. Rush",
                "Sumit Chopra",
                "Jason Weston."
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389, Lisbon, Portugal.",
            "year": 2015
        },
        {
            "authors": [
                "Mike Schuster",
                "Kuldip K Paliwal."
            ],
            "title": "Bidirectional recurrent neural networks",
            "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681.",
            "year": 1997
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey E Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "Journal of machine learning research, 15(1):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112. urran Associates, Inc.",
            "year": 2014
        },
        {
            "authors": [
                "Ronald J Williams."
            ],
            "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
            "venue": "Machine learning, 8(3-4):229\u2013256.",
            "year": 1992
        },
        {
            "authors": [
                "Kristian Woodsend",
                "Mirella Lapata."
            ],
            "title": "Automatic generation of story highlights",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565\u2013574, Uppsala, Sweden.",
            "year": 2010
        },
        {
            "authors": [
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Oriol Vinyals."
            ],
            "title": "Recurrent neural network regularization",
            "venue": "arXiv preprint arXiv:1409.2329.",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779\u2013784 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n779"
        },
        {
            "heading": "1 Introduction",
            "text": "Document summarization aims to automatically rewrite a document into a shorter version while retaining its most important content. Of the many summarization paradigms that have been identified over the years (see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document.\nA great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries). Early attempts mostly leverage human-engineered features (Filatova and Hatzivassiloglou, 2004) coupled with binary classifiers (Kupiec et al., 1995), hidden Markov models (Conroy and O\u2019leary, 2001), graph based methods\n(Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010).\nThe successful application of neural network models to a variety of NLP tasks and the availability of large scale summarization datasets (Hermann et al., 2015; Nallapati et al., 2016) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions).\nAlthough seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods (Cheng and Lapata, 2016) or by maximizing the ROUGE score (Lin, 2004) between a subset of sentences and the human written summaries (Nallapati et al., 2017). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal.\nIn this paper we propose a latent variable extractive model and view labels of sentences in a document as binary latent variables (i.e., zeros and ones). Instead of maximizing the likelihood of \u201cgold\u201d standard labels, the latent model directly maximizes the likelihood of human summaries given selected sentences. Experiments on the CNN/Dailymail dataset (Hermann et al., 2015) show that our latent extractive model improves upon a strong extractive baseline trained on rulebased labels and also performs competitively to several recent models."
        },
        {
            "heading": "2 Model",
            "text": "We first introduce the neural extractive summarization model upon which our latent model is based on. We then describe a sentence compression model which is used in our latent model and finally move on to present the latent model itself."
        },
        {
            "heading": "2.1 Neural Extractive Summarization",
            "text": "In extractive summarization, a subset of sentences in a document is selected as its summary. We model this problem as an instance of sequence labeling. Specifically, a document is viewed as a sequence of sentences and the model is expected to predict a True or False label for each sentence, where True indicates that the sentence should be included in the summary. It is assumed that during training sentences and their labels in each docu-\nment are given (methods for obtaining these labels are discussed in Section 3).\nAs shown in the lower part of Figure 1, our extractive model has three parts: a sentence encoder to convert each sentence into a vector, a document encoder to learn sentence representations given surrounding sentences as context, and a document decoder to predict sentence labels based on representations learned by the document encoder. Let D = (S1, S2, . . . , S|D|) denote a document and Si = (wi1, w i 2, . . . , w i |Si|) a sentence in D (where wij is a word in Si). Let Y = (y1, . . . , y|D|) denote sentence labels. The sentence encoder first transforms Si into a list of hidden states (hi1,h i 2, . . . ,h i |Si|) using a Bidirectional Long Short-Term Memory Network (BiLSTM; Hochreiter and Schmidhuber 1997; Schuster and Paliwal 1997). Then, the sentence encoder yields vi, the representation of Si, by averaging these hidden states (also see Figure 1):\nvi = 1 |Si| \u2211 j hij (1)\nIn analogy to the sentence encoder, the document encoder is another Bi-LSTM but applies on the sentence level. After running the BiLSTM on a sequence of sentence representations (v1,v2, . . . ,v|D|), we obtain context dependent sentence representations (hE1 ,h E 2 , . . . ,h E |D|).\nThe document decoder is also an LSTM which predicts sentence labels. At each time step, it takes the context dependent sentence representation of Si produced by the document encoder as well as the prediction in the previous time step:\nhDi = LSTM(h D i\u22121,\n[ We e(yi\u22121)\nhEi\n] ) (2)\nwhere We \u2208 Rd\u00d72 is the label embedding matrix (d is the hidden dimension for the document decoder LSTM) and yi\u22121 is the prediction at time step i\u22121; the predicted label distribution for yi is:\np(yi|y1:i\u22121,hDi\u22121) = softmax(Wo hDi ) (3)\nwhere Wo \u2208 R2\u00d7d. The model described above is usually trained by minimizing the negative log-likelihood of sentence labels in training documents; it is almost identical to Cheng and Lapata (2016) except that\nwe use a word-level long short-term memory network coupled with mean pooling to learn sentence representations, while they use convolutional neural network coupled with max pooling (Kim et al., 2016)."
        },
        {
            "heading": "2.2 Sentence Compression",
            "text": "We train a sentence compression model to map a sentence selected by the extractive model to a sentence in the summary. The model can be used to evaluate the quality of a selected sentence with respect to the summary (i.e., the degree to which it is similar) or rewrite an extracted sentence according to the style of the summary.\nFor our compression model we adopt a standard attention-based sequence-to-sequence architecture (Bahdanau et al., 2015; Rush et al., 2015). The training set for this model is generated from the same summarization dataset used to train the exractive model. Let D = (S1, S2, . . . , S|D|) denote a document and H = (H1, H2, . . . ,H|H|) its summary. We view each sentence Hi in the summary as a target sentence and assume that its corresponding source is a sentence in D most similar to it. We measure the similarity between source sentences and candidate targets using ROUGE, i.e., Sj = argmaxSj ROUGE(Sj , Hi) and \u3008Sj , Hi\u3009 is a training instance for the compression model. The probability of a sentence H\u0302i being the compression of S\u0302j (i.e., ps2s(H\u0302i|S\u0302j)) can be estimated with a trained compression model."
        },
        {
            "heading": "2.3 Latent Extractive Summarization",
            "text": "Training the extractive model described in Section 2.1 requires sentence-level labels which are obtained heuristically (Cheng and Lapata, 2016; Nallapati et al., 2017). Our latent variable model views sentences in a document as binary variables (i.e., zeros and ones) and uses sentences with activated latent variables (i.e., ones) to infer gold summaries. The latent variables are predicted with an extractive model and the loss during training comes from gold summaries directly.\nLet D = (S1, S2, . . . , S|D|) denote a document and H = (H1, H2, . . . ,H|H|) its human summary (Hk is a sentence inH). We assume that there is a latent variable zi \u2208 {0, 1} for each sentence Si indicating whether Si should be selected, and zi = 1 entails it should. We use the extractive model from Section 2.1 to produce probability distributions for latent variables (see Equation (3)) and obtain them by sampling zi \u223c p(zi|z1:i\u22121,hDi\u22121) (see\nFigure 1). C = {Si|zi = 1}, the set of sentences whose latent variables equal to one, are our current extractive summaries. Without loss of generality, we denote C = (C1, . . . , C|C|). Then, we estimate how likely it is to infer the human summary H from C. We estimate the likelihood of summary sentence Hl given document sentence Ck with the compression model introduced in Section 2.2 and calculate the normalized1 probability skl:\nskl = exp\n( 1\n|Hl| log ps2s(Hl|Ck)\n) (4)\nThe score Rp measures the extent to which H can be inferred from C:\nRp(C,H) = 1\n|C| |C|\u2211 k=1 |H| max l=1 skl (5)\nFor simplicity, we assume one document sentence can only find one summary sentence to explain it. Therefore, for all Hl, we only retain the most evident skl. Rp(C,H) can be viewed as the \u201cprecision\u201d of document sentences with regard to summary sentences. Analogously, we also define Rr, which indicates the extent to whichH can be covered by C:\nRr(C,H) = 1\n|H| |H|\u2211 l=1 |C| max k=1 skl (6)\nRr(C,H) can be viewed as the \u201crecall\u201d of document sentences with regard to summary sentences. The final scoreR(C,H) is the weighted sum of the two:\nR(C,H) = \u03b1Rp(C,H) + (1\u2212 \u03b1)Rr(C,H) (7)\nOur use of the terms \u201cprecision\u201d and \u201crecall\u201d is reminiscent of relevance and coverage in other summarization work (Carbonell and Goldstein, 1998; Lin and Bilmes, 2010; See et al., 2017).\nWe train the model by minimizing the negative expected R(C,H):\nL(\u03b8) = \u2212E(z1,...,z|D|)\u223cp(\u00b7|D)[R(C,H)] (8)\nwhere p(\u00b7|D) is the distribution produced by the neural extractive model (see Equation (3)). Unfortunately, computing the expectation term is prohibitive, since the possible latent variable combinations are exponential. In practice, we approximate this expectation with a single sample from\n1We also experimented with unnormalized probabilities (i.e., excluding the exp in Equation (4)), however we obtained inferior results.\nthe distribution of p(\u00b7|D). We use the REINFORCE algorithm (Williams, 1992) to approximate the gradient of L(\u03b8):\n\u2207L(\u03b8) \u2248\u2211|D| i=1\u2207 log p(zi|z1:i\u22121,hDi\u22121)[R(C,H)\u2212 bi]\nNote that the model described above can be viewed as a reinforcement learning model, where R(C,H) is the reward. To reduce the variance of gradients, we also introduce a baseline linear regression2 model bi (Ranzato et al., 2016) to estimate the expected value of R(C,H). To avoid random label sequences during sampling, we use a pre-trained extractive model to initialize our latent model."
        },
        {
            "heading": "3 Experiments",
            "text": "Dataset and Evaluation We conducted experiments on the CNN/Dailymail dataset (Hermann et al., 2015; See et al., 2017). We followed the same pre-processing steps as in See et al. (2017). The resulting dataset contains 287,226 document-summary pairs for training, 13,368 for validation and 11,490 for test. To create sentence level labels, we used a strategy similar to Nallapati et al. (2017). We label the subset of sentences in a document that maximizes ROUGE (against the human summary) as True and all other sentences as False. Using the method described in Section 2.2, we created a compression dataset with 1,045,492 sentence pairs for training, 53,434 for validation and 43,382 for testing. We evaluated our models using full length F1 ROUGE (Lin, 2004) and the official ROUGE-1.5.5.pl script. We report ROUGE-1, ROUGE-2, and ROUGE-L.\nImplementation We trained our extractive model on an Nvidia K80 GPU card with a batch size of 32. Model parameters were uniformly initialized to [\u2212 1\u221a\nc , 1\u221a c ] (c is the number of columns\nin a weight matrix). We used Adam (Kingma and Ba, 2014) to optimize our models with a learning rate of 0.001, \u03b21 = 0.9, and \u03b22 = 0.999. We trained our extractive model for 10 epochs and selected the model with the highest ROUGE on the validation set. We rescaled the gradient when its norm exceeded 5 (Pascanu et al., 2013) and\n2The linear regression model bt is trained by minimizing the mean squared error between the prediction of bt and R(C,H).\nregularized all LSTMs with a dropout rate of 0.3 (Srivastava et al., 2014; Zaremba et al., 2014). We also applied word dropout (Iyyer et al., 2015) at rate 0.2. We set the hidden unit size d = 300 for both word-level and sentence-level LSTMs and all LSTMs had one layer. We used 300 dimensional pre-trained FastText vectors (Joulin et al., 2017) to initialize our word embeddings. The latent model was initialized from the extractive model (thus both models have the same size) and we set the weight in Equation (7) to \u03b1 = 0.5. The latent model was trained with SGD, with learning rate 0.01 for 5 epochs. During inference, for both extractive and latent models, we rank sentences with p(yi = True|y1:i\u22121,D) and select the top three as summary (see also Equation (3)).\nComparison Systems We compared our model against LEAD3, which selects the first three leading sentences in a document as the summary and a variety of abstractive and extractive models. Abstractive models include a sequence-tosequence architecture (Nallapati et al., 2016); abstract), its pointer generator variant (See et al. 2017; pointer+coverage), and two reinforcement learning-based models (Paulus et al. 2017; abstract-RL and abstract-ML+RL). We also compared our approach against an extractive model based on hierarchical recurrent neural networks (Nallapati et al. 2017; SummaRuNNer), the model described in Section 2.1 (EXTRACT) which encodes sentences using LSTMs, a variant which employs CNNs instead (Cheng and Lapata 2016; EXTRACT-CNN), as well as a similar system based on reinforcement learning (Narayan et al. 2018; REFRESH).\nResults As shown in Table 1, EXTRACT, our extractive model outperforms LEAD3 by a wide margin. EXTRACT also outperforms previously published extractive models (i.e., SummaRuNNer, EXTRACT-CNN, and REFRESH). However, note that SummaRuNNer generates anonymized summaries (Nallapati et al., 2017) while our models generate non-anonymized ones, and therefore the results of EXTRACT and SummaRuNNer are not strictly comparable (also note that LEAD3 results are different in Table 1). Nevertheless, EXTRACT exceeds LEAD3 by +0.75 ROUGE-2 points and +0.57 in terms of ROUGE-L, while SummaRuNNer exceeds LEAD3 by +0.50 ROUGE2 points and is worse by \u22120.20 points in terms of ROUGE-L. We thus conclude that EXTRACT is better when evaluated with ROUGE-2 and ROUGE-L. EXTRACT outperforms all abstractive models except for abstract-RL. ROUGE-2 is lower for abstract-RL which is more competitive when evaluated against ROUGE-1 and ROUGE-L.\nOur latent variable model (LATENT; Section 2.3) outperforms EXTRACT, despite being a strong baseline, which indicates that training with a loss directly based on gold summaries is useful. Differences among LEAD3, EXTRACT, and LATENT are all significant with a 0.95 confidence interval (estimated with the ROUGE script). Interestingly, when applying the compression model from Section 2.2 to the output of our latent model ( LATENT+COMPRESS ), performance drops considerably. This may be because the compression model is a sentence level model and it removes phrases that are important for creating the document-level summaries."
        },
        {
            "heading": "4 Conclusions",
            "text": "We proposed a latent variable extractive summarization model which leverages human summaries directly with the help of a sentence compression model. Experimental results show that the proposed model can indeed improve over a strong extractive model while application of the compression model to the output of our extractive system leads to inferior output. In the future, we plan to explore ways to train compression models tailored to our summarization task."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the EMNLP reviewers for their valuable feedback and Qingyu Zhou for preprocessing the\nCNN/Dailymail dataset. We gratefully acknowledge the financial support of the European Research Council (award number 681760; Lapata)."
        }
    ],
    "title": "Neural Latent Extractive Document Summarization",
    "year": 2018
}